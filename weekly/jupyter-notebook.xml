<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Jupyter Notebook Weekly Trending</title>
    <description>Weekly Trending of Jupyter Notebook in GitHub</description>
    <pubDate>Wed, 13 Aug 2025 01:44:15 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>rasbt/LLMs-from-scratch</title>
      <link>https://github.com/rasbt/LLMs-from-scratch</link>
      <description>&lt;p&gt;Implement a ChatGPT-like LLM in PyTorch from scratch, step by step&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Build a Large Language Model (From Scratch)&lt;/h1&gt; 
&lt;p&gt;This repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book &lt;a href=&quot;https://amzn.to/4fqvn0D&quot;&gt;Build a Large Language Model (From Scratch)&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href=&quot;https://amzn.to/4fqvn0D&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123&quot; width=&quot;250px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;In &lt;a href=&quot;http://mng.bz/orYv&quot;&gt;&lt;em&gt;Build a Large Language Model (From Scratch)&lt;/em&gt;&lt;/a&gt;, you&#39;ll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I&#39;ll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.&lt;/p&gt; 
&lt;p&gt;The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Link to the official &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;source code repository&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://mng.bz/orYv&quot;&gt;Link to the book at Manning (the publisher&#39;s website)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/gp/product/1633437167&quot;&gt;Link to the book page on Amazon.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ISBN 9781633437166&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;http://mng.bz/orYv#reviews&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com//images/LLMs-from-scratch-images/other/reviews.png&quot; width=&quot;220px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;To download a copy of this repository, click on the &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/archive/refs/heads/main.zip&quot;&gt;Download ZIP&lt;/a&gt; button or execute the following command in your terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone --depth 1 https://github.com/rasbt/LLMs-from-scratch.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;p&gt;(If you downloaded the code bundle from the Manning website, please consider visiting the official code repository on GitHub at &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;https://github.com/rasbt/LLMs-from-scratch&lt;/a&gt; for the latest updates.)&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h1&gt;Table of Contents&lt;/h1&gt; 
&lt;p&gt;Please note that this &lt;code&gt;README.md&lt;/code&gt; file is a Markdown (&lt;code&gt;.md&lt;/code&gt;) file. If you have downloaded this code bundle from the Manning website and are viewing it on your local computer, I recommend using a Markdown editor or previewer for proper viewing. If you haven&#39;t installed a Markdown editor yet, &lt;a href=&quot;https://ghostwriter.kde.org&quot;&gt;Ghostwriter&lt;/a&gt; is a good free option.&lt;/p&gt; 
&lt;p&gt;You can alternatively view this and other files on GitHub at &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;https://github.com/rasbt/LLMs-from-scratch&lt;/a&gt; in your browser, which renders Markdown automatically.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; If you&#39;re seeking guidance on installing Python and Python packages and setting up your code environment, I suggest reading the &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/README.md&quot;&gt;README.md&lt;/a&gt; file located in the &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup&quot;&gt;setup&lt;/a&gt; directory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests Linux&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests Windows&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests macOS&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Chapter Title&lt;/th&gt; 
   &lt;th&gt;Main Code (for Quick Access)&lt;/th&gt; 
   &lt;th&gt;All Code + Supplementary&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup&quot;&gt;Setup recommendations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 1: Understanding Large Language Models&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 2: Working with Text Data&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/ch02.ipynb&quot;&gt;ch02.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/dataloader.ipynb&quot;&gt;dataloader.ipynb&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02&quot;&gt;./ch02&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 3: Coding Attention Mechanisms&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/ch03.ipynb&quot;&gt;ch03.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/multihead-attention.ipynb&quot;&gt;multihead-attention.ipynb&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03&quot;&gt;./ch03&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 4: Implementing a GPT Model from Scratch&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/ch04.ipynb&quot;&gt;ch04.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/gpt.py&quot;&gt;gpt.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04&quot;&gt;./ch04&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 5: Pretraining on Unlabeled Data&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/ch05.ipynb&quot;&gt;ch05.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_train.py&quot;&gt;gpt_train.py&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_generate.py&quot;&gt;gpt_generate.py&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05&quot;&gt;./ch05&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 6: Finetuning for Text Classification&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/ch06.ipynb&quot;&gt;ch06.ipynb&lt;/a&gt; &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/gpt_class_finetune.py&quot;&gt;gpt_class_finetune.py&lt;/a&gt; &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06&quot;&gt;./ch06&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 7: Finetuning to Follow Instructions&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/ch07.ipynb&quot;&gt;ch07.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py&quot;&gt;gpt_instruction_finetuning.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/ollama_evaluate.py&quot;&gt;ollama_evaluate.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07&quot;&gt;./ch07&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix A: Introduction to PyTorch&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/code-part1.ipynb&quot;&gt;code-part1.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/code-part2.ipynb&quot;&gt;code-part2.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/DDP-script.py&quot;&gt;DDP-script.py&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A&quot;&gt;./appendix-A&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix B: References and Further Reading&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix C: Exercise Solutions&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix D: Adding Bells and Whistles to the Training Loop&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-D/01_main-chapter-code/appendix-D.ipynb&quot;&gt;appendix-D.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-D&quot;&gt;./appendix-D&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix E: Parameter-efficient Finetuning with LoRA&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-E/01_main-chapter-code/appendix-E.ipynb&quot;&gt;appendix-E.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-E&quot;&gt;./appendix-E&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;p&gt;The mental model below summarizes the contents covered in this book.&lt;/p&gt; 
&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/mental-model.jpg&quot; width=&quot;650px&quot; /&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;The most important prerequisite is a strong foundation in Python programming. With this knowledge, you will be well prepared to explore the fascinating world of LLMs and understand the concepts and code examples presented in this book.&lt;/p&gt; 
&lt;p&gt;If you have some experience with deep neural networks, you may find certain concepts more familiar, as LLMs are built upon these architectures.&lt;/p&gt; 
&lt;p&gt;This book uses PyTorch to implement the code from scratch without using any external LLM libraries. While proficiency in PyTorch is not a prerequisite, familiarity with PyTorch basics is certainly useful. If you are new to PyTorch, Appendix A provides a concise introduction to PyTorch. Alternatively, you may find my book, &lt;a href=&quot;https://sebastianraschka.com/teaching/pytorch-1h/&quot;&gt;PyTorch in One Hour: From Tensors to Training Neural Networks on Multiple GPUs&lt;/a&gt;, helpful for learning about the essentials.&lt;/p&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;p&gt;The code in the main chapters of this book is designed to run on conventional laptops within a reasonable timeframe and does not require specialized hardware. This approach ensures that a wide audience can engage with the material. Additionally, the code automatically utilizes GPUs if they are available. (Please see the &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/raw/main/setup/README.md&quot;&gt;setup&lt;/a&gt; doc for additional recommendations.)&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Video Course&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/livevideo/master-and-build-large-language-models&quot;&gt;A 17-hour and 15-minute companion video course&lt;/a&gt; where I code through each chapter of the book. The course is organized into chapters and sections that mirror the book&#39;s structure so that it can be used as a standalone alternative to the book or complementary code-along resource.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/livevideo/master-and-build-large-language-models&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/video-screenshot.webp?123&quot; width=&quot;350px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Exercises&lt;/h2&gt; 
&lt;p&gt;Each chapter of the book includes several exercises. The solutions are summarized in Appendix C, and the corresponding code notebooks are available in the main chapter folders of this repository (for example, &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;./ch02/01_main-chapter-code/exercise-solutions.ipynb&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In addition to the code exercises, you can download a free 170-page PDF titled &lt;a href=&quot;https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch&quot;&gt;Test Yourself On Build a Large Language Model (From Scratch)&lt;/a&gt; from the Manning website. It contains approximately 30 quiz questions and solutions per chapter to help you test your understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/test-yourself-cover.jpg?123&quot; width=&quot;150px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Bonus Material&lt;/h2&gt; 
&lt;p&gt;Several folders contain optional materials as a bonus for interested readers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Setup&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/01_optional-python-setup-preferences&quot;&gt;Python Setup Tips&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/02_installing-python-libraries&quot;&gt;Installing Python Packages and Libraries Used In This Book&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/03_optional-docker-environment&quot;&gt;Docker Environment Setup Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 2: Working with text data&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb&quot;&gt;Byte Pair Encoding (BPE) Tokenizer From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/02_bonus_bytepair-encoder&quot;&gt;Comparing Various Byte Pair Encoding (BPE) Implementations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/03_bonus_embedding-vs-matmul&quot;&gt;Understanding the Difference Between Embedding Layers and Linear Layers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/04_bonus_dataloader-intuition&quot;&gt;Dataloader Intuition with Simple Numbers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 3: Coding attention mechanisms&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/02_bonus_efficient-multihead-attention/mha-implementations.ipynb&quot;&gt;Comparing Efficient Multi-Head Attention Implementations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/03_understanding-buffers/understanding-buffers.ipynb&quot;&gt;Understanding PyTorch Buffers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 4: Implementing a GPT model from scratch&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/02_performance-analysis/flops-analysis.ipynb&quot;&gt;FLOPS Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/03_kv-cache&quot;&gt;KV Cache&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 5: Pretraining on unlabeled data:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/02_alternative_weight_loading/&quot;&gt;Alternative Weight Loading Methods&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/03_bonus_pretraining_on_gutenberg&quot;&gt;Pretraining GPT on the Project Gutenberg Dataset&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/04_learning_rate_schedulers&quot;&gt;Adding Bells and Whistles to the Training Loop&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/05_bonus_hparam_tuning&quot;&gt;Optimizing Hyperparameters for Pretraining&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/06_user_interface&quot;&gt;Building a User Interface to Interact With the Pretrained LLM&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/07_gpt_to_llama&quot;&gt;Converting GPT to Llama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/07_gpt_to_llama/standalone-llama32.ipynb&quot;&gt;Llama 3.2 From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/11_qwen3/&quot;&gt;Qwen3 Dense and Mixture-of-Experts (MoE) From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb&quot;&gt;Memory-efficient Model Weight Loading&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/09_extending-tokenizers/extend-tiktoken.ipynb&quot;&gt;Extending the Tiktoken BPE Tokenizer with New Tokens&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/10_llm-training-speed&quot;&gt;PyTorch Performance Tips for Faster LLM Training&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 6: Finetuning for classification&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/02_bonus_additional-experiments&quot;&gt;Additional experiments finetuning different layers and using larger models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/03_bonus_imdb-classification&quot;&gt;Finetuning different models on 50k IMDB movie review dataset&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/04_user_interface&quot;&gt;Building a User Interface to Interact With the GPT-based Spam Classifier&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 7: Finetuning to follow instructions&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/02_dataset-utilities&quot;&gt;Dataset Utilities for Finding Near Duplicates and Creating Passive Voice Entries&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/03_model-evaluation&quot;&gt;Evaluating Instruction Responses Using the OpenAI API and Ollama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/05_dataset-generation/llama3-ollama.ipynb&quot;&gt;Generating a Dataset for Instruction Finetuning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/05_dataset-generation/reflection-gpt4.ipynb&quot;&gt;Improving a Dataset for Instruction Finetuning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/04_preference-tuning-with-dpo/create-preference-data-ollama.ipynb&quot;&gt;Generating a Preference Dataset with Llama 3.1 70B and Ollama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb&quot;&gt;Direct Preference Optimization (DPO) for LLM Alignment&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/06_user_interface&quot;&gt;Building a User Interface to Interact With the Instruction Finetuned GPT Model&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Questions, Feedback, and Contributing to This Repository&lt;/h2&gt; 
&lt;p&gt;I welcome all sorts of feedback, best shared via the &lt;a href=&quot;https://livebook.manning.com/forum?product=raschka&amp;amp;page=1&quot;&gt;Manning Forum&lt;/a&gt; or &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/discussions&quot;&gt;GitHub Discussions&lt;/a&gt;. Likewise, if you have any questions or just want to bounce ideas off others, please don&#39;t hesitate to post these in the forum as well.&lt;/p&gt; 
&lt;p&gt;Please note that since this repository contains the code corresponding to a print book, I currently cannot accept contributions that would extend the contents of the main chapter code, as it would introduce deviations from the physical book. Keeping it consistent helps ensure a smooth experience for everyone.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this book or code useful for your research, please consider citing it.&lt;/p&gt; 
&lt;p&gt;Chicago-style citation:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Raschka, Sebastian. &lt;em&gt;Build A Large Language Model (From Scratch)&lt;/em&gt;. Manning, 2024. ISBN: 978-1633437166.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;BibTeX entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@book{build-llms-from-scratch-book,
  author       = {Sebastian Raschka},
  title        = {Build A Large Language Model (From Scratch)},
  publisher    = {Manning},
  year         = {2024},
  isbn         = {978-1633437166},
  url          = {https://www.manning.com/books/build-a-large-language-model-from-scratch},
  github       = {https://github.com/rasbt/LLMs-from-scratch}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>patchy631/ai-engineering-hub</title>
      <link>https://github.com/patchy631/ai-engineering-hub</link>
      <description>&lt;p&gt;In-depth tutorials on LLMs, RAGs and real-world AI agent applications.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/12800&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/assets/TRENDING-BADGE.png&quot; alt=&quot;Trending Badge&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/assets/ai-eng-hub.gif&quot; alt=&quot;AI Engineering Hub Banner&quot; /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;AI Engineering Hub ðŸš€&lt;/h1&gt; 
&lt;p&gt;Welcome to the &lt;strong&gt;AI Engineering Hub&lt;/strong&gt;!&lt;/p&gt; 
&lt;h2&gt;ðŸŒŸ Why This Repo?&lt;/h2&gt; 
&lt;p&gt;AI Engineering is advancing rapidly, and staying at the forefront requires both deep understanding and hands-on experience. Here, you will find:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In-depth tutorials on &lt;strong&gt;LLMs and RAGs&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Real-world &lt;strong&gt;AI agent&lt;/strong&gt; applications&lt;/li&gt; 
 &lt;li&gt;Examples to implement, adapt, and scale in your projects&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Whether youâ€™re a beginner, practitioner, or researcher, this repo provides resources for all skill levels to experiment and succeed in AI engineering.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ“¬ Stay Updated with Our Newsletter!&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Get a FREE Data Science eBook&lt;/strong&gt; ðŸ“– with 150+ essential lessons in Data Science when you subscribe to our newsletter! Stay in the loop with the latest tutorials, insights, and exclusive resources. &lt;a href=&quot;https://join.dailydoseofds.com&quot;&gt;Subscribe now!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://join.dailydoseofds.com&quot;&gt;&lt;img src=&quot;https://github.com/patchy631/ai-engineering/raw/main/resources/join_ddods.png&quot; alt=&quot;Daily Dose of Data Science Newsletter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ“¢ Contribute to the AI Engineering Hub!&lt;/h2&gt; 
&lt;p&gt;We welcome contributors! Whether you want to add new tutorials, improve existing code, or report issues, your contributions make this community thrive. Hereâ€™s how to get involved:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Fork&lt;/strong&gt; the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch for your contribution.&lt;/li&gt; 
 &lt;li&gt;Submit a &lt;strong&gt;Pull Request&lt;/strong&gt; and describe the improvements.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;ðŸ“œ License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the MIT License - see the &lt;a href=&quot;https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;ðŸ’¬ Connect&lt;/h2&gt; 
&lt;p&gt;For discussions, suggestions, and more, feel free to &lt;a href=&quot;https://github.com/patchy631/ai-engineering/issues&quot;&gt;create an issue&lt;/a&gt; or reach out directly!&lt;/p&gt; 
&lt;p&gt;Happy Coding! ðŸŽ‰&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>udlbook/udlbook</title>
      <link>https://github.com/udlbook/udlbook</link>
      <description>&lt;p&gt;Understanding Deep Learning - Simon J.D. Prince&lt;/p&gt;&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>chiphuyen/aie-book</title>
      <link>https://github.com/chiphuyen/aie-book</link>
      <description>&lt;p&gt;[WIP] Resources for AI engineers. Also contains supporting materials for the book AI Engineering (Chip Huyen, 2025)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Engineering book and other resources&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;This repo will be updated with more resources in the next few weeks.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/#about-the-book&quot;&gt;About the book AI Engineering&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/ToC.md&quot;&gt;Table of contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/chapter-summaries.md&quot;&gt;Chapter summaries&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/study-notes.md&quot;&gt;Study notes&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/resources.md&quot;&gt;AI engineering resources&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/prompt-examples.md&quot;&gt;Prompt examples&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/case-studies.md&quot;&gt;Case studies&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/misalignment.md&quot;&gt;Misalignment AI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/appendix.md&quot;&gt;Appendix&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Fun tools:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/scripts/ai-heatmap.ipynb&quot;&gt;ChatGPT and Claude conversation heatmap generator&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;And more ...&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About the book&lt;/h2&gt; 
&lt;p&gt;The availability of foundation models has transformed AI from a specialized discipline into a powerful development tool everyone can use. This book covers the end-to-end process of adapting foundation models to solve real-world problems, encompassing tried-and-true techniques from other engineering fields and techniques emerging with foundation models.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://amzn.to/49j1cGS&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/assets/aie-cover.png&quot; width=&quot;250&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;https://amzn.to/49j1cGS&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/assets/aie-cover-back.png&quot; width=&quot;250&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The book is available on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://amzn.to/49j1cGS&quot;&gt;Amazon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://oreillymedia.pxf.io/c/5719111/2146021/15173&quot;&gt;O&#39;Reilly&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://amzn.to/3Vq2ryu&quot;&gt;Kindle&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and most places where technical books are sold.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;This is NOT a tutorial book, so it doesn&#39;t have a lot of code snippets.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;What this book is about&lt;/h2&gt; 
&lt;p&gt;This book provides a framework for adapting foundation models, which include both large language models (LLMs) and large multimodal models (LMMs), to specific applications. It not only outlines various solutions for building an AI application but also raises questions you can ask to evaluate the best solution for your needs. Here are just some of the many questions that this book can help you answer:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Should I build this AI application?&lt;/li&gt; 
 &lt;li&gt;How do I evaluate my application? Can I use AI to evaluate AI outputs?&lt;/li&gt; 
 &lt;li&gt;What causes hallucinations? How do I detect and mitigate hallucinations?&lt;/li&gt; 
 &lt;li&gt;What are the best practices for prompt engineering?&lt;/li&gt; 
 &lt;li&gt;Why does RAG work? What are the strategies for doing RAG?&lt;/li&gt; 
 &lt;li&gt;Whatâ€™s an agent? How do I build and evaluate an agent?&lt;/li&gt; 
 &lt;li&gt;When to finetune a model? When not to finetune a model?&lt;/li&gt; 
 &lt;li&gt;How much data do I need? How do I validate the quality of my data?&lt;/li&gt; 
 &lt;li&gt;How do I make my model faster, cheaper, and secure?&lt;/li&gt; 
 &lt;li&gt;How do I create a feedback loop to improve my application continually?&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The book will also help you navigate the overwhelming AI landscape: types of models, evaluation benchmarks, and a seemingly infinite number of use cases and application patterns.&lt;/p&gt; 
&lt;p&gt;The content in this book is illustrated using actual case studies, many of which Iâ€™ve worked on, backed by ample references and extensively reviewed by experts from a wide range of backgrounds. Even though the book took two years to write, it draws from my experience working with language models and ML systems from the last decade.&lt;/p&gt; 
&lt;p&gt;Like my previous book, &lt;em&gt;&lt;a href=&quot;https://amzn.to/4fXVZH2&quot;&gt;Designing Machine Learning Systems (DMLS)&lt;/a&gt;&lt;/em&gt;, this book focuses on the fundamentals of AI engineering instead of any specific tool or API. Tools become outdated quickly, but fundamentals should last longer.&lt;/p&gt; 
&lt;h3&gt;Reading &lt;em&gt;AI Engineering&lt;/em&gt; (AIE) with &lt;em&gt;Designing Machine Learning Systems&lt;/em&gt; (DMLS)&lt;/h3&gt; 
&lt;p&gt;AIE can be a companion to DMLS. DMLS focuses on building applications on top of traditional ML models, which involves more tabular data annotations, feature engineering, and model training. AIE focuses on building applications on top of foundation models, which involves more prompt engineering, context construction, and parameter-efficient finetuning. Both books are self-contained and modular, so you can read either book independently.&lt;/p&gt; 
&lt;p&gt;Since foundation models are ML models, some concepts are relevant to working with both. If a topic is relevant to AIE but has been discussed extensively in DMLS, itâ€™ll still be covered in this book, but to a lesser extent, with pointers to relevant resources.&lt;/p&gt; 
&lt;p&gt;Note that many topics are covered in DMLS but not in AIE, and vice versa. The first chapter of this book also covers the differences between traditional ML engineering and AI engineering.&lt;/p&gt; 
&lt;p&gt;A real-world system often involves both traditional ML models and foundation models, so knowledge about working with both is often necessary.&lt;/p&gt; 
&lt;h2&gt;Who this book is for&lt;/h2&gt; 
&lt;p&gt;This book is for anyone who wants to leverage foundation models to solve real-world problems. This is a technical book, so the language of this book is geared towards technical roles, including AI engineers, ML engineers, data scientists, engineering managers, and technical product managers. This book is for you if you can relate to one of the following scenarios:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Youâ€™re building or optimizing an AI application, whether youâ€™re starting from scratch or looking to move beyond the demo phase into a production-ready stage. You may also be facing issues like hallucinations, security, latency, or costs, and need targeted solutions.&lt;/li&gt; 
 &lt;li&gt;You want to streamline your teamâ€™s AI development process, making it more systematic, faster, and reliable.&lt;/li&gt; 
 &lt;li&gt;You want to understand how your organization can leverage foundation models to improve the businessâ€™s bottom line and how to build a team to do so.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also benefit from the book if you belong to one of the following groups:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Tool developers who want to identify underserved areas in AI engineering to position your products in the ecosystem.&lt;/li&gt; 
 &lt;li&gt;Researchers who want to understand better AI use cases.&lt;/li&gt; 
 &lt;li&gt;Job candidates seeking clarity on the skills needed to pursue a career as an AI engineer.&lt;/li&gt; 
 &lt;li&gt;Anyone wanting to better understand AI&#39;s capabilities and limitations, and how it might affect different roles.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;I love getting to the bottom of things, so some sections dive a bit deeper into the technical side. While many early readers like the detail, I know it might not be for everyone. Iâ€™ll give you a heads-up before things get too technical. Feel free to skip ahead if it feels a little too in the weeds!&lt;/p&gt; 
&lt;h2&gt;Reviews&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;This book offers a comprehensive, well-structured guide to the essential aspects of building generative AI systems. A must-read for any professional looking to scale AI across the enterprise.&quot;&lt;/em&gt; - Vittorio Cretella, former global CIO at P&amp;amp;G and Mars&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;Chip Huyen gets generative AI. She is a remarkable teacher and writer whose work has been instrumental in helping teams bring AI into production. Drawing on her deep expertise, AI Engineering is a comprehensive and holistic guide to building generative AI applications in production.&quot;&lt;/em&gt; - Luke Metz, co-creator of ChatGPT, ex-research manager @ OpenAI&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;Every AI engineer building real-world applications should read this book. Itâ€™s a vital guide to end-to-end AI system design, from model development and evaluation to large-scale deployment and operation.&quot;&lt;/em&gt; - Andrei Lopatenko, Director Search and AI, Neuron7&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;This book serves as an essential guide for building AI products that can scale. Unlike other books that focus on tools or current trends that are constantly changing, Chip delivers timeless foundational knowledge. Whether you&#39;re a product manager or an engineer, this book effectively bridges the collaboration gap between cross-functional teams, making it a must-read for anyone involved in AI development.&quot;&lt;/em&gt; - Aileen Bui, AI Product Operations Manager, Google&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;This is the definitive segue into AI Engineering from one of the greats of ML Engineering! Chip has seen through successful projects and careers at every stage of a company and for the first time ever condensed her expertise for new AI Engineers entering the field.&quot;&lt;/em&gt; - swyx, Curator, AI.Engineer&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;AI Engineering is a practical guide that provides the most up-to-date information on AI development, making it approachable for novice and expert leaders alike. This book is an essential resource for anyone looking to build robust and scalable AI systems.&quot;&lt;/em&gt; - Vicki Reyzelman, Chief AI Solutions Architect, Mave Sparks&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;AI Engineering is a comprehensive guide that serves as an essential reference for both understanding and implementing AI systems in practice.&quot;&lt;/em&gt; - Han Lee, Director - Data Science, Moody&#39;s.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;AI Engineering is an essential guide for anyone building software with Generative AI! It demystifies the technology, highlights the importance of evaluation, and shares what should be done to achieve quality before starting with costly fine-tuning.&quot;&lt;/em&gt; - Rafal Kawala, Senior AI Engineering Director, 16 years of experience working in a Fortune 500 company&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See what people are talking about the book on Twitter &lt;a href=&quot;https://twitter.com/aisysbooks/likes&quot;&gt;@aisysbooks&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;This book would&#39;ve taken a lot longer to write and missed many important topics if it wasn&#39;t for so many wonderful people who helped me through the process.&lt;/p&gt; 
&lt;p&gt;Because the timeline for the project was tightâ€”two years for a 150,000-word book that covers so much groundâ€”I&#39;m grateful to the technical reviewers who put aside their precious time to review this book so quickly.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://x.com/luke_metz&quot;&gt;Luke Metz&lt;/a&gt; is an amazing soundboard who checked my assumptions and prevented me from going down the wrong path. &lt;a href=&quot;https://www.linkedin.com/in/hanchunglee/&quot;&gt;Han-chung Lee&lt;/a&gt;, always up to date with the latest AI news and community development, pointed me toward resources that I missed. Luke and Han were the first to review my drafts before I sent them to the next round of technical reviewers, and I&#39;m forever indebted to them for tolerating my follies and mistakes.&lt;/p&gt; 
&lt;p&gt;Having led AI innovation at Fortune 500 companies, &lt;a href=&quot;https://www.linkedin.com/in/vittorio-cretella/&quot;&gt;Vittorio Cretella&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/lopatenko/&quot;&gt;Andrei Lopatenko&lt;/a&gt; provided invaluable feedback that combined deep technical expertise with executive insights. &lt;a href=&quot;https://www.linkedin.com/in/vickireyzelman/&quot;&gt;Vicki Reyzelman&lt;/a&gt; helped me ground my content and keep it relevant for readers with a software engineering background.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://eugeneyan.com/&quot;&gt;Eugene Yan&lt;/a&gt;, a dear friend and amazing applied scientist, provided me with technical and emotional support. Shawn Wang (&lt;a href=&quot;https://x.com/swyx&quot;&gt;swyx&lt;/a&gt;), provided an important vibe check that helped me feel more confident about the book. &lt;a href=&quot;https://x.com/bhutanisanyam1&quot;&gt;Sanyam Bhutani&lt;/a&gt; is one of the best learners and most humble souls I know, who not only gave thoughtful written feedback but also recorded videos to explain his feedback.&lt;/p&gt; 
&lt;p&gt;Kyle Krannen is a star deep learning lead who interviewed his colleagues and shared with me an amazing writeup about their finetuning process, which guided the finetuning chapter. &lt;a href=&quot;https://x.com/marksaroufim&quot;&gt;Mark Saroufim&lt;/a&gt;, an inquisitive mind who always has his pulse on the most interesting problems, introduced me to great resources on efficiency. Both Kyle and Mark&#39;s feedback was critical in writing Chapters 7 and 9.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/kittipat-bot-kampa-1b1965/&quot;&gt;Kittipat &quot;Bot&quot; Kampa&lt;/a&gt;, on top of answering my many questions, shared with me a detailed visualization of how he thinks about AI platform. I appreciate &lt;a href=&quot;https://www.linkedin.com/in/denyslinkov/&quot;&gt;Denys Linkov&lt;/a&gt;&#39;s systematic approach to evaluation and platform development. &lt;a href=&quot;https://www.linkedin.com/in/chetantekur/&quot;&gt;Chetan Tekur&lt;/a&gt; gave great examples that helped me structure AI application patterns. I&#39;d also like to thank &lt;a href=&quot;https://www.linkedin.com/in/findalexli/&quot;&gt;Alex (Shengzhi Li) Li&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/hienluu/&quot;&gt;Hien Luu&lt;/a&gt; for their thoughtful feedback on my draft on AI architecture.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/aileenbui/&quot;&gt;Aileen Bui&lt;/a&gt; is a treasure who shared unique feedback and examples from a product manager&#39;s perspective. Thanks &lt;a href=&quot;https://www.linkedin.com/in/todor-markov-4aa38a67/&quot;&gt;Todor Markov&lt;/a&gt; for the actionable advice on the RAG and Agents chapter. Thanks &lt;a href=&quot;https://www.linkedin.com/in/tal-kachman/&quot;&gt;Tal Kachman&lt;/a&gt; for jumping in at the last minute to push the finetuning chapter over the finish line.&lt;/p&gt; 
&lt;p&gt;There are so many wonderful people whose company and conversations gave me ideas that guide the content of this book. I tried my best to include the names of everyone who has helped me here, but due to the inherent faultiness of human memory, I undoubtedly neglected to mention many. If I forgot to include your name, please know that it wasn&#39;t because I don&#39;t appreciate your contribution, and please kindly remind me so that I can rectify as soon as possible!&lt;/p&gt; 
&lt;p&gt;Andrew Francis, Anish Nag, &lt;a href=&quot;https://www.linkedin.com/in/wgalczak/&quot;&gt;Anthony Galczak&lt;/a&gt;, &lt;a href=&quot;https://x.com/abacaj&quot;&gt;Anton Bacaj&lt;/a&gt;, BalÃ¡zs Galambosi, Charles Frye, Charles Packer, Chris Brousseau, Eric Hartford, Goku Mohandas, Hamel Husain, Harpreet Sahota, Hassan El Mghari, Huu Nguyen, Jeremy Howard, Jesse Silver, John Cook, &lt;a href=&quot;https://www.linkedin.com/in/juan-pablo-bottaro/&quot;&gt;Juan Pablo Bottaro&lt;/a&gt;, Kyle Gallatin, Lance Martin, Lucio Dery, Matt Ross, Maxime Labonne, Miles Brundage, Nathan Lambert, Omar Khattab, &lt;a href=&quot;https://www.linkedin.com/in/xphongvn/&quot;&gt;Phong Nguyen&lt;/a&gt;, Purnendu Mukherjee, Sam Reiswig, Sebastian Raschka, Shahul ES, Sharif Shameem, Soumith Chintala, Teknium, Tim Dettmers, Undi5, Val Andrei Fajardo, Vern Liang, Victor Sanh, Wing Lian, Xiquan Cui, Ying Sheng, and Kristofer.&lt;/p&gt; 
&lt;p&gt;I&#39;d like to thank all early readers who have also reached out with feedback. Douglas Bailley is a super reader who shared so much thoughtful feedback. Nutan Sahoo for suggesting an elegant way to explain perplexity.&lt;/p&gt; 
&lt;p&gt;I learned so much from the online discussions with so many. Thanks to everyone who&#39;s ever answered my questions, commented on my posts, or sent me an email with your thoughts.&lt;/p&gt; 
&lt;p&gt;Of course, the book wouldn&#39;t have been possible without the team at O&#39;Reilly, especially my development editors (Melissa Potter, Corbin Collins, Jill Leonard) and my production editors (Kristen Brown and Elizabeth Kelly). Liz Wheeler is the most discerning editor I&#39;ve ever worked with. Nicole Butterfield is a force who oversaw this book from an idea to a final product.&lt;/p&gt; 
&lt;p&gt;This book, after all, is an accumulation of invaluable lessons I learned throughout my career. I owe these lessons to my extremely competent and patient coworkers and former coworkers. Every person I&#39;ve worked with has taught me something new about bringing ML into the world.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;Chip Huyen, &lt;em&gt;AI Engineering&lt;/em&gt;. O&#39;Reilly Media, 2025.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@book{aiebook2025,  
    address = {USA},  
    author = {Chip Huyen},  
    isbn = {978-1801819312},   
    publisher = {O&#39;Reilly Media},  
    title = {{AI Engineering}},  
    year = {2025}  
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/Data-Science-For-Beginners</title>
      <link>https://github.com/microsoft/Data-Science-For-Beginners</link>
      <description>&lt;p&gt;10 Weeks, 20 Lessons, Data Science for All!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Science for Beginners - A Curriculum&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/codespaces/new?hide_repo_select=true&amp;amp;ref=main&amp;amp;repo=344191198&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in GitHub Codespaces&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/Data-Science-For-Beginners/raw/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/graphs/contributors/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/issues/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/pulls/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/watchers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/network/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/stargazers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/ByRwuEEgH4&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://aka.ms/foundry/forum&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Azure_AI_Foundry_Developer_Forum-blue?style=for-the-badge&amp;amp;logo=github&amp;amp;color=000000&amp;amp;logoColor=fff&quot; alt=&quot;Azure AI Foundry Developer Forum&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. Each lesson includes pre-lesson and post-lesson quizzes, written instructions to complete the lesson, a solution, and an assignment. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Hearty thanks to our authors:&lt;/strong&gt; &lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine Greenaway&lt;/a&gt;, &lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry Soshnikov&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya Narasimhan&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/JalenMcG&quot;&gt;Jalen McGee&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen Looper&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud Levy&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany Souterre&lt;/a&gt;, &lt;a href=&quot;https://www.twitter.com/geektrainer&quot;&gt;Christopher Harrison&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ðŸ™ Special thanks ðŸ™ to our &lt;a href=&quot;https://studentambassadors.microsoft.com/&quot;&gt;Microsoft Student Ambassador&lt;/a&gt; authors, reviewers and content contributors,&lt;/strong&gt; notably Aaryan Arora, &lt;a href=&quot;https://github.com/AdityaGarg00&quot;&gt;Aditya Garg&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/alondra-sanchez-molina/&quot;&gt;Alondra Sanchez&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/ankitasingh007&quot;&gt;Ankita Singh&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/anupam--mishra/&quot;&gt;Anupam Mishra&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/arpitadas01/&quot;&gt;Arpita Das&lt;/a&gt;, ChhailBihari Dubey, &lt;a href=&quot;https://www.linkedin.com/in/dibrinsofor&quot;&gt;Dibri Nsofor&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/dishita-bhasin-7065281bb&quot;&gt;Dishita Bhasin&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/majd-s/&quot;&gt;Majd Safi&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/max-blum-6036a1186/&quot;&gt;Max Blum&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/miguelmque/&quot;&gt;Miguel Correa&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/iftu119&quot;&gt;Mohamma Iftekher (Iftu) Ebne Jalal&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/nawrin-tabassum&quot;&gt;Nawrin Tabassum&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/raymond-wp/&quot;&gt;Raymond Wangsa Putra&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/rty2423&quot;&gt;Rohit Yadav&lt;/a&gt;, Samridhi Sharma, &lt;a href=&quot;https://www.linkedin.com/mwlite/in/sanya-sinha-13aab1200&quot;&gt;Sanya Sinha&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/sheena-narua-n/&quot;&gt;Sheena Narula&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/tauqeerahmad5201/&quot;&gt;Tauqeer Ahmad&lt;/a&gt;, Yogendrasingh Pawar , &lt;a href=&quot;https://www.linkedin.com/in/vidushi-gupta07/&quot;&gt;Vidushi Gupta&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jasleen-sondhi/&quot;&gt;Jasleen Sondhi&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Title.png&quot; alt=&quot; Sketchnote by (@sketchthedocs) &quot; /&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Announcement - New Curriculum on Generative AI was just released!&lt;/h2&gt; 
&lt;p&gt;We just released a 12 lesson curriculum on generative AI. Come learn things like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;prompting and prompt engineering&lt;/li&gt; 
 &lt;li&gt;text and image app generation&lt;/li&gt; 
 &lt;li&gt;search apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As usual, there&#39;s a lesson, assignments to complete, knowledge checks and challenges.&lt;/p&gt; 
&lt;p&gt;Check it out:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners&quot;&gt;https://aka.ms/genai-beginners&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Are you a student?&lt;/h1&gt; 
&lt;p&gt;Get started with the following resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-gb/learn/student-hub?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Student Hub page&lt;/a&gt; In this page, you will find beginner resources, Student packs and even ways to get a free cert voucher. This is one page you want to bookmark and check from time to time as we switch out content at least monthly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://studentambassadors.microsoft.com?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Microsoft Learn Student Ambassadors&lt;/a&gt; Join a global community of student ambassadors, this could be your way into Microsoft.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Getting Started&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;: we have &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/for-teachers.md&quot;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum. We&#39;d love your feedback &lt;a href=&quot;https://github.com/microsoft/Data-Science-For-Beginners/discussions&quot;&gt;in our discussion forum&lt;/a&gt;!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://aka.ms/student-page&quot;&gt;Students&lt;/a&gt;&lt;/strong&gt;: to use this curriculum on your own, fork the entire repo and complete the exercises on your own, starting with a pre-lecture quiz. Then read the lecture and complete the rest of the activities. Try to create the projects by comprehending the lessons rather than copying the solution code; however, that code is available in the /solutions folders in each project-oriented lesson. Another idea would be to form a study group with friends and go through the content together. For further study, we recommend &lt;a href=&quot;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/qprpajyoy3x0g7?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Microsoft Learn&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Meet the Team&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://youtu.be/8mzavjQSMM4&quot; title=&quot;Promo video&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/ds-for-beginners.gif&quot; alt=&quot;Promo video&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&quot;https://www.linkedin.com/in/mohitjaisal&quot;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ðŸŽ¥ Click the image above for a video about the project the folks who created it!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Pedagogy&lt;/h2&gt; 
&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is project-based and that it includes frequent quizzes. By the end of this series, students will have learned basic principles of data science, including ethical concepts, data preparation, different ways of working with data, data visualization, data analysis, real-world use cases of data science, and more.&lt;/p&gt; 
&lt;p&gt;In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 10 week cycle.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Find our &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CODE_OF_CONDUCT.md&quot;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CONTRIBUTING.md&quot;&gt;Contributing&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&quot;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Each lesson includes:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optional sketchnote&lt;/li&gt; 
 &lt;li&gt;Optional supplemental video&lt;/li&gt; 
 &lt;li&gt;Pre-lesson warmup quiz&lt;/li&gt; 
 &lt;li&gt;Written lesson&lt;/li&gt; 
 &lt;li&gt;For project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; 
 &lt;li&gt;Knowledge checks&lt;/li&gt; 
 &lt;li&gt;A challenge&lt;/li&gt; 
 &lt;li&gt;Supplemental reading&lt;/li&gt; 
 &lt;li&gt;Assignment&lt;/li&gt; 
 &lt;li&gt;Post-lesson quiz&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained in the Quiz-App folder, for 40 total quizzes of three questions each. They are linked from within the lessons, but the quiz app can be run locally or deployed to Azure; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Roadmap.png&quot; alt=&quot; Sketchnote by (@sketchthedocs) &quot; /&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science For Beginners: Roadmap - &lt;em&gt;Sketchnote by &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Lesson Number&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Topic&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Lesson Grouping&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Learning Objectives&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Linked Lesson&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Author&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;01&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Defining Data Science&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Learn the basic concepts behind data science and how itâ€™s related to artificial intelligence, machine learning, and big data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/01-defining-data-science/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/beZ7Mb_oz9I&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;02&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science Ethics&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Ethics Concepts, Challenges &amp;amp; Frameworks.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/02-ethics/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;03&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Defining Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;How data is classified and its common sources.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/03-defining-data/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;04&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to Statistics &amp;amp; Probability&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;The mathematical techniques of probability and statistics to understand data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/04-stats-and-probability/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/Z5Zy85g4Yjw&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;05&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with Relational Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to relational data and the basics of exploring and analyzing relational data with the Structured Query Language, also known as SQL (pronounced â€œsee-quellâ€).&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/geektrainer&quot;&gt;Christopher&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;06&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with NoSQL Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to non-relational data, its various types and the basics of exploring and analyzing document databases.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/06-non-relational/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;07&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with Python&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Basics of using Python for data exploration with libraries such as Pandas. Foundational understanding of Python programming is recommended.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/07-python/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/dZjWOGbsN4Y&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;08&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Preparation&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Topics on data techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/08-data-preparation/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;09&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Quantities&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Learn how to use Matplotlib to visualize bird data ðŸ¦†&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/09-visualization-quantities/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;10&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Distributions of Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing observations and trends within an interval.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/10-visualization-distributions/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;11&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Proportions&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing discrete and grouped percentages.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/11-visualization-proportions/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;12&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Relationships&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing connections and correlations between sets of data and their variables.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/12-visualization-relationships/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;13&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Meaningful Visualizations&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Techniques and guidance for making your visualizations valuable for effective problem solving and insights.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/13-meaningful-visualizations/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to the Data Science lifecycle&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to the data science lifecycle and its first step of acquiring and extracting data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/14-Introduction/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;15&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Analyzing&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This phase of the data science lifecycle focuses on techniques to analyze data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/15-analyzing/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;16&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Communication&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This phase of the data science lifecycle focuses on presenting the insights from the data in a way that makes it easier for decision makers to understand.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/16-communication/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/JalenMcG&quot;&gt;Jalen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;17&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This series of lessons introduces data science in the cloud and its benefits.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/17-Introduction/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;18&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Training models using Low Code tools.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/18-Low-Code/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;19&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Deploying models with Azure Machine Learning Studio.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/19-Azure/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;20&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Wild&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/README.md&quot;&gt;In the Wild&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data science driven projects in the real world.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/20-Real-World-Examples/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;GitHub Codespaces&lt;/h2&gt; 
&lt;p&gt;Follow these steps to open this sample in a Codespace:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Click the Code drop-down menu and select the Open with Codespaces option.&lt;/li&gt; 
 &lt;li&gt;Select + New codespace at the bottom on the pane. For more info, check out the &lt;a href=&quot;https://docs.github.com/en/codespaces/developing-in-codespaces/creating-a-codespace-for-a-repository#creating-a-codespace&quot;&gt;GitHub documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;VSCode Remote - Containers&lt;/h2&gt; 
&lt;p&gt;Follow these steps to open this repo in a container using your local machine and VSCode using the VS Code Remote - Containers extension:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If this is your first time using a development container, please ensure your system meets the pre-reqs (i.e. have Docker installed) in &lt;a href=&quot;https://code.visualstudio.com/docs/devcontainers/containers#_getting-started&quot;&gt;the getting started documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To use this repository, you can either open the repository in an isolated Docker volume:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Under the hood, this will use the Remote-Containers: &lt;strong&gt;Clone Repository in Container Volume...&lt;/strong&gt; command to clone the source code in a Docker volume instead of the local filesystem. &lt;a href=&quot;https://docs.docker.com/storage/volumes/&quot;&gt;Volumes&lt;/a&gt; are the preferred mechanism for persisting container data.&lt;/p&gt; 
&lt;p&gt;Or open a locally cloned or downloaded version of the repository:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Clone this repository to your local filesystem.&lt;/li&gt; 
 &lt;li&gt;Press F1 and select the &lt;strong&gt;Remote-Containers: Open Folder in Container...&lt;/strong&gt; command.&lt;/li&gt; 
 &lt;li&gt;Select the cloned copy of this folder, wait for the container to start, and try things out.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Offline access&lt;/h2&gt; 
&lt;p&gt;You can run this documentation offline by using &lt;a href=&quot;https://docsify.js.org/#/&quot;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&quot;https://docsify.js.org/#/quickstart&quot;&gt;install Docsify&lt;/a&gt; on your local machine, then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note, notebooks will not be rendered via Docsify, so when you need to run a notebook, do that separately in VS Code running a Python kernel.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Help Wanted!&lt;/h2&gt; 
&lt;p&gt;If you would like to translate all or part of the curriculum, please follow our &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&quot;&gt;Translations&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h2&gt;Other Curricula&lt;/h2&gt; 
&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-beginners&quot;&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet&quot;&gt;Generative AI for Beginners .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/generative-ai-with-javascript&quot;&gt;Generative AI with JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genaijava&quot;&gt;Generative AI with Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming&quot;&gt;Mastering GitHub Copilot for Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>huggingface/smol-course</title>
      <link>https://github.com/huggingface/smol-course</link>
      <description>&lt;p&gt;A course on aligning smol models.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/banner.png&quot; alt=&quot;smolcourse image&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;a smol course&lt;/h1&gt; 
&lt;p&gt;This is a practical course on aligning language models for your specific use case. It&#39;s a handy way to get started with aligning language models, because everything runs on most local machines. There are minimal GPU requirements and no paid services. The course is based on the &lt;a href=&quot;https://github.com/huggingface/smollm/tree/main&quot;&gt;SmolLM2&lt;/a&gt; series of models, but you can transfer the skills you learn here to larger models or other small language models.&lt;/p&gt; 
&lt;a href=&quot;http://hf.co/join/discord&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Discord-7289DA?&amp;amp;logo=discord&amp;amp;logoColor=white&quot; /&gt; &lt;/a&gt; 
&lt;div style=&quot;background: linear-gradient(to right, #e0f7fa, #e1bee7, orange); padding: 20px; border-radius: 5px; margin-bottom: 20px; color: purple;&quot;&gt; 
 &lt;h2&gt;Participation is open, free, and now!&lt;/h2&gt; 
 &lt;p&gt;This course is open and peer reviewed. To get involved with the course &lt;strong&gt;open a pull request&lt;/strong&gt; and submit your work for review. Here are the steps:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Fork the repo &lt;a href=&quot;https://github.com/huggingface/smol-course/fork&quot;&gt;here&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Read the material, make changes, do the exercises, add your own examples.&lt;/li&gt; 
  &lt;li&gt;Open a PR on the december_2024 branch&lt;/li&gt; 
  &lt;li&gt;Get it reviewed and merged&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;This should help you learn and to build a community-driven course that is always improving.&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;We can discuss the process in this &lt;a href=&quot;https://github.com/huggingface/smol-course/discussions/2#discussion-7602932&quot;&gt;discussion thread&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Course Outline&lt;/h2&gt; 
&lt;p&gt;This course provides a practical, hands-on approach to working with small language models, from initial training through to production deployment.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Module&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Release Date&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/1_instruction_tuning&quot;&gt;Instruction Tuning&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Learn supervised fine-tuning, chat templating, and basic instruction following&lt;/td&gt; 
   &lt;td&gt;âœ… Ready&lt;/td&gt; 
   &lt;td&gt;Dec 3, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/2_preference_alignment&quot;&gt;Preference Alignment&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Explore DPO and ORPO techniques for aligning models with human preferences&lt;/td&gt; 
   &lt;td&gt;âœ… Ready&lt;/td&gt; 
   &lt;td&gt;Dec 6, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/3_parameter_efficient_finetuning&quot;&gt;Parameter-efficient Fine-tuning&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Learn LoRA, prompt tuning, and efficient adaptation methods&lt;/td&gt; 
   &lt;td&gt;âœ… Ready&lt;/td&gt; 
   &lt;td&gt;Dec 9, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/4_evaluation&quot;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Use automatic benchmarks and create custom domain evaluations&lt;/td&gt; 
   &lt;td&gt;âœ… Ready&lt;/td&gt; 
   &lt;td&gt;Dec 13, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/5_vision_language_models&quot;&gt;Vision-language Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Adapt multimodal models for vision-language tasks&lt;/td&gt; 
   &lt;td&gt;âœ… Ready&lt;/td&gt; 
   &lt;td&gt;Dec 16, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/6_synthetic_datasets&quot;&gt;Synthetic Datasets&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Create and validate synthetic datasets for training&lt;/td&gt; 
   &lt;td&gt;âœ… Ready&lt;/td&gt; 
   &lt;td&gt;Dec 20, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/7_inference&quot;&gt;Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Infer with models efficiently&lt;/td&gt; 
   &lt;td&gt;âœ… Ready&lt;/td&gt; 
   &lt;td&gt;Jan 8, 2025&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/8_agents&quot;&gt;Agents&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Build your own agentic AI&lt;/td&gt; 
   &lt;td&gt;âœ… Ready&lt;/td&gt; 
   &lt;td&gt;Jan 13, 2025&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Why Small Language Models?&lt;/h2&gt; 
&lt;p&gt;While large language models have shown impressive capabilities, they often require significant computational resources and can be overkill for focused applications. Small language models offer several advantages for domain-specific applications:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: Require significantly less computational resources to train and deploy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customization&lt;/strong&gt;: Easier to fine-tune and adapt to specific domains&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Control&lt;/strong&gt;: Better understanding and control of model behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: Lower operational costs for training and inference&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Privacy&lt;/strong&gt;: Can be run locally without sending data to external APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Green Technology&lt;/strong&gt;: Advocates efficient usage of resources with reduced carbon footprint&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easier Academic Research Development&lt;/strong&gt;: Provides an easy starter for academic research with cutting-edge LLMs with less logistical constraints&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before starting, ensure you have the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Basic understanding of machine learning and natural language processing.&lt;/li&gt; 
 &lt;li&gt;Familiarity with Python, PyTorch, and the &lt;code&gt;transformers&lt;/code&gt; library.&lt;/li&gt; 
 &lt;li&gt;Access to a pre-trained language model and a labeled dataset.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We maintain the course as a package so you can install dependencies easily via a package manager. We recommend &lt;a href=&quot;https://github.com/astral-sh/uv&quot;&gt;uv&lt;/a&gt; for this purpose, but you could use alternatives like &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;pdm&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Using &lt;code&gt;uv&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;With &lt;code&gt;uv&lt;/code&gt; installed, you can install the course like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv venv --python 3.11.0
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;All the examples run in the same &lt;strong&gt;python 3.11&lt;/strong&gt; environment, so you should create an environment and install dependencies like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# python -m venv .venv
# source .venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Google Colab&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;From Google Colab&lt;/strong&gt; you will need to install dependencies flexibly based on the hardware you&#39;re using. Like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install transformers trl datasets huggingface_hub
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-cookbook</title>
      <link>https://github.com/openai/openai-cookbook</link>
      <description>&lt;p&gt;Examples and guides for using the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;a href=&quot;https://cookbook.openai.com&quot; target=&quot;_blank&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;/images/openai-cookbook-white.png&quot; style=&quot;max-width: 100%; width: 400px; margin-bottom: 20px&quot; /&gt; 
  &lt;img alt=&quot;OpenAI Cookbook Logo&quot; src=&quot;https://raw.githubusercontent.com/openai/openai-cookbook/main/images/openai-cookbook.png&quot; width=&quot;400px&quot; /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h3&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;âœ¨ Navigate at &lt;a href=&quot;https://cookbook.openai.com&quot;&gt;cookbook.openai.com&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Example code and guides for accomplishing common tasks with the &lt;a href=&quot;https://platform.openai.com/docs/introduction&quot;&gt;OpenAI API&lt;/a&gt;. To run these examples, you&#39;ll need an OpenAI account and associated API key (&lt;a href=&quot;https://platform.openai.com/signup&quot;&gt;create a free account here&lt;/a&gt;). Set an environment variable called &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; with your API key. Alternatively, in most IDEs such as Visual Studio Code, you can create an &lt;code&gt;.env&lt;/code&gt; file at the root of your repo containing &lt;code&gt;OPENAI_API_KEY=&amp;lt;your API key&amp;gt;&lt;/code&gt;, which will be picked up by the notebooks.&lt;/p&gt; 
&lt;p&gt;Most code examples are written in Python, though the concepts can be applied in any language.&lt;/p&gt; 
&lt;p&gt;For other useful tools, guides and courses, check out these &lt;a href=&quot;https://cookbook.openai.com/related_resources&quot;&gt;related resources from around the web&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/generative-ai-for-beginners</title>
      <link>https://github.com/microsoft/generative-ai-for-beginners</link>
      <description>&lt;p&gt;21 Lessons, Get Started Building with Generative AI ðŸ”— https://microsoft.github.io/generative-ai-for-beginners/&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/images/repo-thumbnailv4-fixed.png?WT.mc_id=academic-105485-koreyst&quot; alt=&quot;Generative AI For Beginners&quot; /&gt;&lt;/p&gt; 
&lt;h3&gt;21 Lessons teaching everything you need to know to start building Generative AI applications&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-For-Beginners/raw/master/LICENSE?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/issues/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/pulls/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/watchers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/network/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/stargazers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://dcbadge.limes.pink/api/server/ByRwuEEgH4&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ðŸŒ Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fr/README.md&quot;&gt;French&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/es/README.md&quot;&gt;Spanish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/de/README.md&quot;&gt;German&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ru/README.md&quot;&gt;Russian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ar/README.md&quot;&gt;Arabic&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fa/README.md&quot;&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ur/README.md&quot;&gt;Urdu&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/zh/README.md&quot;&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/mo/README.md&quot;&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hk/README.md&quot;&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tw/README.md&quot;&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ja/README.md&quot;&gt;Japanese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ko/README.md&quot;&gt;Korean&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hi/README.md&quot;&gt;Hindi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/bn/README.md&quot;&gt;Bengali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/mr/README.md&quot;&gt;Marathi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ne/README.md&quot;&gt;Nepali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pa/README.md&quot;&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pt/README.md&quot;&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/br/README.md&quot;&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/it/README.md&quot;&gt;Italian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pl/README.md&quot;&gt;Polish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tr/README.md&quot;&gt;Turkish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/el/README.md&quot;&gt;Greek&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/th/README.md&quot;&gt;Thai&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sv/README.md&quot;&gt;Swedish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/da/README.md&quot;&gt;Danish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/no/README.md&quot;&gt;Norwegian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fi/README.md&quot;&gt;Finnish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/nl/README.md&quot;&gt;Dutch&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/he/README.md&quot;&gt;Hebrew&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/vi/README.md&quot;&gt;Vietnamese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/id/README.md&quot;&gt;Indonesian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ms/README.md&quot;&gt;Malay&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tl/README.md&quot;&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sw/README.md&quot;&gt;Swahili&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hu/README.md&quot;&gt;Hungarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/cs/README.md&quot;&gt;Czech&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sk/README.md&quot;&gt;Slovak&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ro/README.md&quot;&gt;Romanian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/bg/README.md&quot;&gt;Bulgarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sr/README.md&quot;&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hr/README.md&quot;&gt;Croatian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sl/README.md&quot;&gt;Slovenian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/uk/README.md&quot;&gt;Ukrainian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/my/README.md&quot;&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Generative AI for Beginners (Version 3) - A Course&lt;/h1&gt; 
&lt;p&gt;Learn the fundamentals of building Generative AI applications with our 21-lesson comprehensive course by Microsoft Cloud Advocates.&lt;/p&gt; 
&lt;h2&gt;ðŸŒ± Getting Started&lt;/h2&gt; 
&lt;p&gt;This course has 21 lessons. Each lesson covers its own topic so start wherever you like!&lt;/p&gt; 
&lt;p&gt;Lessons are labeled either &quot;Learn&quot; lessons explaining a Generative AI concept or &quot;Build&quot; lessons that explain a concept and code examples in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt; when possible.&lt;/p&gt; 
&lt;p&gt;For .NET Developers checkout &lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners (.NET Edition)&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Each lesson also includes a &quot;Keep Learning&quot; section with additional learning tools.&lt;/p&gt; 
&lt;h2&gt;What You Need&lt;/h2&gt; 
&lt;h3&gt;To run the code of this course, you can use either:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/azure-open-ai?WT.mc_id=academic-105485-koreyst&quot;&gt;Azure OpenAI Service&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;aoai-assignment&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/gh-models?WT.mc_id=academic-105485-koreyst&quot;&gt;GitHub Marketplace Model Catalog&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;githubmodels&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/open-ai?WT.mc_id=academic-105485-koreyst&quot;&gt;OpenAI API&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;oai-assignment&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Basic knowledge of Python or TypeScript is helpful - *For absolute beginners check out these &lt;a href=&quot;https://aka.ms/genai-beginners/python?WT.mc_id=academic-105485-koreyst&quot;&gt;Python&lt;/a&gt; and &lt;a href=&quot;https://aka.ms/genai-beginners/typescript?WT.mc_id=academic-105485-koreyst&quot;&gt;TypeScript&lt;/a&gt; courses&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;A GitHub account to &lt;a href=&quot;https://aka.ms/genai-beginners/github?WT.mc_id=academic-105485-koreyst&quot;&gt;fork this entire repo&lt;/a&gt; to your own GitHub account&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We have created a &lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/00-course-setup/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Course Setup&lt;/a&gt;&lt;/strong&gt; lesson to help you with setting up your development environment.&lt;/p&gt; 
&lt;p&gt;Don&#39;t forget to &lt;a href=&quot;https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst&quot;&gt;star (ðŸŒŸ) this repo&lt;/a&gt; to find it easier later.&lt;/p&gt; 
&lt;h2&gt;ðŸ§  Ready to Deploy?&lt;/h2&gt; 
&lt;p&gt;If you are looking for more advanced code samples, check out our &lt;a href=&quot;https://aka.ms/genai-beg-code?WT.mc_id=academic-105485-koreyst&quot;&gt;collection of Generative AI Code Samples&lt;/a&gt; in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;ðŸ—£ï¸ Meet Other Learners, Get Support&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href=&quot;https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst&quot;&gt;official Azure AI Foundry Discord server&lt;/a&gt; to meet and network with other learners taking this course and get support.&lt;/p&gt; 
&lt;p&gt;Ask questions or share product feedback in our &lt;a href=&quot;https://aka.ms/azureaifoundry/forum&quot;&gt;Azure AI Foundry Developer Forum&lt;/a&gt; on Github.&lt;/p&gt; 
&lt;h2&gt;ðŸš€ Building a Startup?&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href=&quot;https://www.microsoft.com/startups&quot;&gt;Microsoft for Startups&lt;/a&gt; to find out how to get started building with Azure credits today.&lt;/p&gt; 
&lt;h2&gt;ðŸ™ Want to help?&lt;/h2&gt; 
&lt;p&gt;Do you have suggestions or found spelling or code errors? &lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst&quot;&gt;Raise an issue&lt;/a&gt; or &lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners/pulls?WT.mc_id=academic-105485-koreyst&quot;&gt;Create a pull request&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ðŸ“‚ Each lesson includes:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A short video introduction to the topic&lt;/li&gt; 
 &lt;li&gt;A written lesson located in the README&lt;/li&gt; 
 &lt;li&gt;Python and TypeScript code samples supporting Azure OpenAI and OpenAI API&lt;/li&gt; 
 &lt;li&gt;Links to extra resources to continue your learning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;ðŸ—ƒï¸ Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Lesson Link&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Extra Learning&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;00&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/00-course-setup/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Course Setup&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to Setup Your Development Environment&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;01&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/01-introduction-to-genai/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Introduction to Generative AI and LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; Understanding what Generative AI is and how Large Language Models (LLMs) work.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson-1-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;02&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/02-exploring-and-comparing-different-llms/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Exploring and comparing different LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to select the right model for your use case&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;03&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Using Generative AI Responsibly&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to build Generative AI Applications responsibly&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;04&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Understanding Prompt Engineering Fundamentals&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; Hands-on Prompt Engineering Best Practices&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson4-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;05&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Creating Advanced Prompts&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to apply prompt engineering techniques that improve the outcome of your prompts.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson5-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;06&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Text Generation Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A text generation app using Azure OpenAI / OpenAI API&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson6-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;07&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/07-building-chat-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Chat Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; Techniques for efficiently building and integrating chat applications.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lessons7-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;08&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Search Apps Vector Databases&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A search application that uses Embeddings to search for data.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson8-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;09&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/09-building-image-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Image Generation Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An image generation application&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson9-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/10-building-low-code-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Low Code AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A Generative AI application using Low Code tools&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson10-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/11-integrating-with-function-calling/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Integrating External Applications with Function Calling&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; What is function calling and its use cases for applications&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson11-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Designing UX for AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to apply UX design principles when developing Generative AI Applications&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson12-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/13-securing-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Securing Your Generative AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The threats and risks to AI systems and methods to secure these systems.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;The Generative AI Application Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The tools and metrics to manage the LLM Lifecycle and LLMOps&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson14-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/15-rag-and-vector-databases/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Retrieval Augmented Generation (RAG) and Vector Databases&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using a RAG Framework to retrieve embeddings from a Vector Databases&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/16-open-source-models/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Open Source Models and Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using open source models available on Hugging Face&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson16-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/17-ai-agents/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;AI Agents&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using an AI Agent Framework&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson17-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/18-fine-tuning/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Fine-Tuning LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The what, why and how of fine-tuning LLMs&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/19-slm/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with SLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The benefits of building with Small Language Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/20-mistral/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with Mistral Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The features and differences of the Mistral Family Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/21-meta/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with Meta Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The features and differences of the Meta Family Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ðŸŒŸ Special thanks&lt;/h3&gt; 
&lt;p&gt;Special thanks to &lt;a href=&quot;https://www.linkedin.com/in/john0isaac/&quot;&gt;&lt;strong&gt;John Aziz&lt;/strong&gt;&lt;/a&gt; for creating all of the GitHub Actions and workflows&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/bernhard-merkle-738b73/&quot;&gt;&lt;strong&gt;Bernhard Merkle&lt;/strong&gt;&lt;/a&gt; for making key contributions to each lesson to improve the learner and code experience.&lt;/p&gt; 
&lt;h2&gt;ðŸŽ’ Other Courses&lt;/h2&gt; 
&lt;p&gt;Our team produces other courses! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;strong&gt;NEW&lt;/strong&gt; Model Context Protocol for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;AI Agents for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners using .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-js-course?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners using JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst&quot;&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>NirDiamant/RAG_Techniques</title>
      <link>https://github.com/NirDiamant/RAG_Techniques</link>
      <description>&lt;p&gt;This repository showcases various advanced techniques for Retrieval-Augmented Generation (RAG) systems. RAG systems combine information retrieval with generative models to provide accurate and contextually rich responses.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;http://makeapullrequest.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.linkedin.com/in/nir-diamant-759323134/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/LinkedIn-Connect-blue&quot; alt=&quot;LinkedIn&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/NirDiamantAI&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/NirDiamantAI?label=Follow%20@NirDiamantAI&amp;amp;style=social&quot; alt=&quot;Twitter&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.reddit.com/r/EducationalAI/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Reddit-Join%20our%20subreddit-FF4500?style=flat-square&amp;amp;logo=reddit&amp;amp;logoColor=white&quot; alt=&quot;Reddit&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/cA6Aa4uyDX&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20our%20community-7289da?style=flat-square&amp;amp;logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/sponsors/NirDiamant&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;color=ff69b4&quot; alt=&quot;Sponsor&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;ðŸŒŸ &lt;strong&gt;Support This Project:&lt;/strong&gt; Your sponsorship fuels innovation in RAG technologies. &lt;strong&gt;&lt;a href=&quot;https://github.com/sponsors/NirDiamant&quot;&gt;Become a sponsor&lt;/a&gt;&lt;/strong&gt; to help maintain and expand this valuable resource!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Sponsors â¤ï¸&lt;/h2&gt; 
&lt;p&gt;We gratefully acknowledge the organizations and individuals who have made significant contributions to this project.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Company Sponsors&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://zilliz.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/images/ziliz_logo.png&quot; style=&quot;border-radius: 12px; margin-right: 24px; vertical-align: middle;&quot; height=&quot;96&quot; alt=&quot;Zilliz: Key Collaborator&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Individual Sponsors&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/sponsors/Eisenh&quot;&gt;&lt;img src=&quot;https://github.com/Eisenh.png&quot; style=&quot;border-radius: 50%;&quot; width=&quot;64&quot; height=&quot;64&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Advanced RAG Techniques: Elevating Your Retrieval-Augmented Generation Systems ðŸš€&lt;/h1&gt; 
&lt;p&gt;Welcome to one of the most comprehensive and dynamic collections of Retrieval-Augmented Generation (RAG) tutorials available today. This repository serves as a hub for cutting-edge techniques aimed at enhancing the accuracy, efficiency, and contextual richness of RAG systems.&lt;/p&gt; 
&lt;h2&gt;ðŸ“« Stay Updated!&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;ðŸš€&lt;br /&gt;&lt;b&gt;Cutting-edge&lt;br /&gt;Updates&lt;/b&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;ðŸ’¡&lt;br /&gt;&lt;b&gt;Expert&lt;br /&gt;Insights&lt;/b&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;ðŸŽ¯&lt;br /&gt;&lt;b&gt;Top 0.1%&lt;br /&gt;Content&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;p&gt;&lt;a href=&quot;https://diamantai.substack.com/?r=336pe4&amp;amp;utm_campaign=pub-share-checklist&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/images/subscribe-button.svg?sanitize=true&quot; alt=&quot;Subscribe to DiamantAI Newsletter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Join over 20,000 of AI enthusiasts getting unique cutting-edge insights and free tutorials!&lt;/em&gt; &lt;em&gt;&lt;strong&gt;Plus, subscribers get exclusive early access and special 33% discounts to my book and the upcoming RAG Techniques course!&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://diamantai.substack.com/?r=336pe4&amp;amp;utm_campaign=pub-share-checklist&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/images/substack_image.png&quot; alt=&quot;DiamantAI&#39;s newsletter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Retrieval-Augmented Generation (RAG) is revolutionizing the way we combine information retrieval with generative AI. This repository showcases a curated collection of advanced techniques designed to supercharge your RAG systems, enabling them to deliver more accurate, contextually relevant, and comprehensive responses.&lt;/p&gt; 
&lt;p&gt;Our goal is to provide a valuable resource for researchers and practitioners looking to push the boundaries of what&#39;s possible with RAG. By fostering a collaborative environment, we aim to accelerate innovation in this exciting field.&lt;/p&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;p&gt;ðŸš€ Level up with my &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/agents-towards-production&quot;&gt;Agents Towards Production&lt;/a&gt;&lt;/strong&gt; repository. It delivers horizontal, code-first tutorials that cover every tool and step in the lifecycle of building production-grade GenAI agents, guiding you from spark to scale with proven patterns and reusable blueprints for real-world launches, making it the smartest place to start if you&#39;re serious about shipping agents to production.&lt;/p&gt; 
&lt;p&gt;ðŸ¤– Explore my &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents&quot;&gt;GenAI Agents Repository&lt;/a&gt;&lt;/strong&gt; to discover a variety of AI agent implementations and tutorials, showcasing how different AI technologies can be combined to create powerful, interactive systems.&lt;/p&gt; 
&lt;p&gt;ðŸ–‹ï¸ Check out my &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/Prompt_Engineering&quot;&gt;Prompt Engineering Techniques guide&lt;/a&gt;&lt;/strong&gt; for a comprehensive collection of prompting strategies, from basic concepts to advanced techniques, enhancing your ability to interact effectively with AI language models.&lt;/p&gt; 
&lt;h2&gt;A Community-Driven Knowledge Hub&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;This repository grows stronger with your contributions!&lt;/strong&gt; Join our vibrant communities - the central hubs for shaping and advancing this project together ðŸ¤&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.reddit.com/r/EducationalAI/&quot;&gt;Educational AI Subreddit&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://discord.gg/cA6Aa4uyDX&quot;&gt;RAG Techniques Discord Community&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Whether you&#39;re an expert or just starting out, your insights can shape the future of RAG. Join us to propose ideas, get feedback, and collaborate on innovative techniques. For contribution guidelines, please refer to our &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques/raw/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/strong&gt; file. Let&#39;s advance RAG technology together!&lt;/p&gt; 
&lt;p&gt;ðŸ”— For discussions on GenAI, RAG, or custom agents, or to explore knowledge-sharing opportunities, feel free to &lt;strong&gt;&lt;a href=&quot;https://www.linkedin.com/in/nir-diamant-759323134/&quot;&gt;connect on LinkedIn&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ðŸ§  State-of-the-art RAG enhancements&lt;/li&gt; 
 &lt;li&gt;ðŸ“š Comprehensive documentation for each technique&lt;/li&gt; 
 &lt;li&gt;ðŸ› ï¸ Practical implementation guidelines&lt;/li&gt; 
 &lt;li&gt;ðŸŒŸ Regular updates with the latest advancements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Advanced Techniques&lt;/h2&gt; 
&lt;p&gt;Explore our extensive list of cutting-edge RAG techniques:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;Category&lt;/th&gt; 
   &lt;th&gt;Technique&lt;/th&gt; 
   &lt;th&gt;View&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;â­ Key Collaboration&lt;/td&gt; 
   &lt;td&gt;Graph RAG with Milvus Vector DB&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/graphrag_with_milvus_vectordb.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graphrag_with_milvus_vectordb.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;Foundational ðŸŒ±&lt;/td&gt; 
   &lt;td&gt;Basic RAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/simple_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;Foundational ðŸŒ±&lt;/td&gt; 
   &lt;td&gt;RAG with CSV Files&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/simple_csv_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_csv_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;Foundational ðŸŒ±&lt;/td&gt; 
   &lt;td&gt;Reliable RAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/reliable_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reliable_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;Foundational ðŸŒ±&lt;/td&gt; 
   &lt;td&gt;Optimizing Chunk Sizes&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/choose_chunk_size.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/choose_chunk_size.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;Foundational ðŸŒ±&lt;/td&gt; 
   &lt;td&gt;Proposition Chunking&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/proposition_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/proposition_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;Query Enhancement ðŸ”&lt;/td&gt; 
   &lt;td&gt;Query Transformations&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/query_transformations.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/query_transformations.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;Query Enhancement ðŸ”&lt;/td&gt; 
   &lt;td&gt;HyDE (Hypothetical Document Embedding)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/HyDe_Hypothetical_Document_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/HyDe_Hypothetical_Document_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9&lt;/td&gt; 
   &lt;td&gt;Query Enhancement ðŸ”&lt;/td&gt; 
   &lt;td&gt;HyPE (Hypothetical Prompt Embedding)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/HyPE_Hypothetical_Prompt_Embeddings.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/HyPE_Hypothetical_Prompt_Embeddings.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;Context Enrichment ðŸ“š&lt;/td&gt; 
   &lt;td&gt;Contextual Chunk Headers&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/contextual_chunk_headers.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_chunk_headers.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;Context Enrichment ðŸ“š&lt;/td&gt; 
   &lt;td&gt;Relevant Segment Extraction&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/relevant_segment_extraction.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/relevant_segment_extraction.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;Context Enrichment ðŸ“š&lt;/td&gt; 
   &lt;td&gt;Context Window Enhancement&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/context_enrichment_window_around_chunk.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/context_enrichment_window_around_chunk.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;Context Enrichment ðŸ“š&lt;/td&gt; 
   &lt;td&gt;Semantic Chunking&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/semantic_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/semantic_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;Context Enrichment ðŸ“š&lt;/td&gt; 
   &lt;td&gt;Contextual Compression&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/contextual_compression.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_compression.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;Context Enrichment ðŸ“š&lt;/td&gt; 
   &lt;td&gt;Document Augmentation&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/document_augmentation.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/document_augmentation.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval ðŸš€&lt;/td&gt; 
   &lt;td&gt;Fusion Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/fusion_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval ðŸš€&lt;/td&gt; 
   &lt;td&gt;Reranking&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/reranking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval ðŸš€&lt;/td&gt; 
   &lt;td&gt;Multi-faceted Filtering&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/multi_faceted_filtering.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_faceted_filtering.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval ðŸš€&lt;/td&gt; 
   &lt;td&gt;Hierarchical Indices&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/hierarchical_indices.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/hierarchical_indices.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval ðŸš€&lt;/td&gt; 
   &lt;td&gt;Ensemble Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/ensemble_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/ensemble_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval ðŸš€&lt;/td&gt; 
   &lt;td&gt;Dartboard Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/dartboard.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/dartboard.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval ðŸš€&lt;/td&gt; 
   &lt;td&gt;Multi-modal RAG with Captioning&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/multi_model_rag_with_captioning.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_model_rag_with_captioning.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;Iterative Techniques ðŸ”&lt;/td&gt; 
   &lt;td&gt;Retrieval with Feedback Loop&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/retrieval_with_feedback_loop.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/retrieval_with_feedback_loop.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;Iterative Techniques ðŸ”&lt;/td&gt; 
   &lt;td&gt;Adaptive Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/adaptive_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/adaptive_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;Iterative Retrieval ðŸ”„&lt;/td&gt; 
   &lt;td&gt;Iterative Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/iterative_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/iterative_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;Evaluation ðŸ“Š&lt;/td&gt; 
   &lt;td&gt;DeepEval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/evaluation/evaluation_deep_eval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_deep_eval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;Evaluation ðŸ“Š&lt;/td&gt; 
   &lt;td&gt;GroUSE&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/evaluation/evaluation_grouse.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_grouse.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;Explainability ðŸ”¬&lt;/td&gt; 
   &lt;td&gt;Explainable Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/explainable_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/explainable_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;Advanced Architecture ðŸ—ï¸&lt;/td&gt; 
   &lt;td&gt;Graph RAG with LangChain&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/graph_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graph_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td&gt;Advanced Architecture ðŸ—ï¸&lt;/td&gt; 
   &lt;td&gt;Microsoft GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/Microsoft_GraphRag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/Microsoft_GraphRag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;31&lt;/td&gt; 
   &lt;td&gt;Advanced Architecture ðŸ—ï¸&lt;/td&gt; 
   &lt;td&gt;RAPTOR&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/raptor.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/raptor.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;Advanced Architecture ðŸ—ï¸&lt;/td&gt; 
   &lt;td&gt;Self-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/self_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/self_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;33&lt;/td&gt; 
   &lt;td&gt;Advanced Architecture ðŸ—ï¸&lt;/td&gt; 
   &lt;td&gt;Corrective RAG (CRAG)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/crag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/crag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;34&lt;/td&gt; 
   &lt;td&gt;Special Technique ðŸŒŸ&lt;/td&gt; 
   &lt;td&gt;Sophisticated Controllable Agent&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/Controllable-RAG-Agent&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ðŸŒ± Foundational RAG Techniques&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Simple RAG ðŸŒ±&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques/raw/main/all_rag_techniques_runnable_scripts/simple_rag.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Introducing basic RAG techniques ideal for newcomers.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Start with basic retrieval queries and integrate incremental learning mechanisms.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Simple RAG using a CSV file ðŸ§©&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_csv_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_csv_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_csv_rag_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_csv_rag_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Introducing basic RAG using CSV files.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;This uses CSV files to create basic retrieval and integrates with openai to create question and answering system.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reliable RAG ðŸ·ï¸&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reliable_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reliable_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Enhances the Simple RAG by adding validation and refinement to ensure the accuracy and relevance of retrieved information.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Check for retrieved document relevancy and highlight the segment of docs used for answering.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Choose Chunk Size ðŸ“&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/choose_chunk_size.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/choose_chunk_size.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/choose_chunk_size.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Selecting an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Experiment with different chunk sizes to find the optimal balance between preserving context and maintaining retrieval speed for your specific use case.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Proposition Chunking â›“ï¸â€ðŸ’¥&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/proposition_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/proposition_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Breaking down the text into concise, complete, meaningful sentences allowing for better control and handling of specific queries (especially extracting knowledge).&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ðŸ’ª &lt;strong&gt;Proposition Generation:&lt;/strong&gt; The LLM is used in conjunction with a custom prompt to generate factual statements from the document chunks.&lt;/li&gt; 
   &lt;li&gt;âœ… &lt;strong&gt;Quality Checking:&lt;/strong&gt; The generated propositions are passed through a grading system that evaluates accuracy, clarity, completeness, and conciseness.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/the-propositions-method-enhancing?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&quot;&gt;The Propositions Method: Enhancing Information Retrieval for AI Systems&lt;/a&gt;&lt;/strong&gt; - A comprehensive blog post exploring the benefits and implementation of proposition chunking in RAG systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ðŸ” Query Enhancement&lt;/h3&gt; 
&lt;ol start=&quot;6&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Query Transformations ðŸ”„&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/query_transformations.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/query_transformations.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/query_transformations.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Modifying and expanding queries to improve retrieval effectiveness.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;âœï¸ &lt;strong&gt;Query Rewriting:&lt;/strong&gt; Reformulate queries to improve retrieval.&lt;/li&gt; 
   &lt;li&gt;ðŸ”™ &lt;strong&gt;Step-back Prompting:&lt;/strong&gt; Generate broader queries for better context retrieval.&lt;/li&gt; 
   &lt;li&gt;ðŸ§© &lt;strong&gt;Sub-query Decomposition:&lt;/strong&gt; Break complex queries into simpler sub-queries.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Hypothetical Questions (HyDE Approach) â“&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/HyDe_Hypothetical_Document_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/HyDe_Hypothetical_Document_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/HyDe_Hypothetical_Document_Embedding.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Generating hypothetical questions to improve alignment between queries and data.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Create hypothetical questions that point to relevant locations in the data, enhancing query-data matching.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/hyde-exploring-hypothetical-document?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&quot;&gt;HyDE: Exploring Hypothetical Document Embeddings for AI Retrieval&lt;/a&gt;&lt;/strong&gt; - A short blog post explaining this method clearly.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸ“š Context and Content Enrichment&lt;/h3&gt; 
&lt;ol start=&quot;8&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Hypothetical Prompt Embeddings (HyPE) â“ðŸš€&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/HyPE_Hypothetical_Prompt_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/HyPE_Hypothetical_Prompt_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/HyPE_Hypothetical_Prompt_Embeddings.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;HyPE (Hypothetical Prompt Embeddings) is an enhancement to traditional RAG retrieval that &lt;strong&gt;precomputes hypothetical prompts at the indexing stage&lt;/strong&gt;, but inseting the chunk in their place. This transforms retrieval into a &lt;strong&gt;question-question matching task&lt;/strong&gt;. This avoids the need for runtime synthetic answer generation, reducing inference-time computational overhead while &lt;strong&gt;improving retrieval alignment&lt;/strong&gt;.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ðŸ“– &lt;strong&gt;Precomputed Questions:&lt;/strong&gt; Instead of embedding document chunks, HyPE &lt;strong&gt;generates multiple hypothetical queries per chunk&lt;/strong&gt; at indexing time.&lt;/li&gt; 
   &lt;li&gt;ðŸ” &lt;strong&gt;Question-Question Matching:&lt;/strong&gt; User queries are matched against stored hypothetical questions, leading to &lt;strong&gt;better retrieval alignment&lt;/strong&gt;.&lt;/li&gt; 
   &lt;li&gt;âš¡ &lt;strong&gt;No Runtime Overhead:&lt;/strong&gt; Unlike HyDE, HyPE does &lt;strong&gt;not require LLM calls at query time&lt;/strong&gt;, making retrieval &lt;strong&gt;faster and cheaper&lt;/strong&gt;.&lt;/li&gt; 
   &lt;li&gt;ðŸ“ˆ &lt;strong&gt;Higher Precision &amp;amp; Recall:&lt;/strong&gt; Improves retrieval &lt;strong&gt;context precision by up to 42 percentage points&lt;/strong&gt; and &lt;strong&gt;claim recall by up to 45 percentage points&lt;/strong&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5139335&quot;&gt;Preprint: Hypothetical Prompt Embeddings (HyPE)&lt;/a&gt;&lt;/strong&gt; - Research paper detailing the method, evaluation, and benchmarks.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Contextual Chunk Headers &lt;span&gt;ðŸ·&lt;/span&gt;&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_chunk_headers.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_chunk_headers.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Contextual chunk headers (CCH) is a method of creating document-level and section-level context, and prepending those chunk headers to the chunks prior to embedding them.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Create a chunk header that includes context about the document and/or section of the document, and prepend that to each chunk in order to improve the retrieval accuracy.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/D-Star-AI/dsRAG&quot;&gt;dsRAG&lt;/a&gt;&lt;/strong&gt;: open-source retrieval engine that implements this technique (and a few other advanced RAG techniques)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Relevant Segment Extraction ðŸ§©&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/relevant_segment_extraction.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/relevant_segment_extraction.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Relevant segment extraction (RSE) is a method of dynamically constructing multi-chunk segments of text that are relevant to a given query.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Perform a retrieval post-processing step that analyzes the most relevant chunks and identifies longer multi-chunk segments to provide more complete context to the LLM.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Context Enrichment Techniques ðŸ“&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/context_enrichment_window_around_chunk.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/context_enrichment_window_around_chunk.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/context_enrichment_window_around_chunk_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/context_enrichment_window_around_chunk_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/context_enrichment_window_around_chunk.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; 
&lt;p&gt;Enhancing retrieval accuracy by embedding individual sentences and extending context to neighboring sentences.&lt;/p&gt; 
&lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
&lt;p&gt;Retrieve the most relevant sentence while also accessing the sentences before and after it in the original text.&lt;/p&gt; 
&lt;ol start=&quot;12&quot;&gt; 
 &lt;li&gt;Semantic Chunking ðŸ§ &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/semantic_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/semantic_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques/raw/main/all_rag_techniques_runnable_scripts/semantic_chunking.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; 
&lt;p&gt;Dividing documents based on semantic coherence rather than fixed sizes.&lt;/p&gt; 
&lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
&lt;p&gt;Use NLP techniques to identify topic boundaries or coherent sections within documents for more meaningful retrieval units.&lt;/p&gt; 
&lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/semantic-chunking-improving-ai-information?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&quot;&gt;Semantic Chunking: Improving AI Information Retrieval&lt;/a&gt;&lt;/strong&gt; - A comprehensive blog post exploring the benefits and implementation of semantic chunking in RAG systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start=&quot;13&quot;&gt; 
 &lt;li&gt;Contextual Compression ðŸ—œï¸&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_compression.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_compression.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/contextual_compression.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; 
&lt;p&gt;Compressing retrieved information while preserving query-relevant content.&lt;/p&gt; 
&lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
&lt;p&gt;Use an LLM to compress or summarize retrieved chunks, preserving key information relevant to the query.&lt;/p&gt; 
&lt;ol start=&quot;14&quot;&gt; 
 &lt;li&gt;Document Augmentation through Question Generation for Enhanced Retrieval&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/document_augmentation.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/document_augmentation.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/document_augmentation.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; 
&lt;p&gt;This implementation demonstrates a text augmentation technique that leverages additional question generation to improve document retrieval within a vector database. By generating and incorporating various questions related to each text fragment, the system enhances the standard retrieval process, thus increasing the likelihood of finding relevant documents that can be utilized as context for generative question answering.&lt;/p&gt; 
&lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
&lt;p&gt;Use an LLM to augment text dataset with all possible questions that can be asked to each document.&lt;/p&gt; 
&lt;h3&gt;ðŸš€ Advanced Retrieval Methods&lt;/h3&gt; 
&lt;ol start=&quot;15&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Fusion Retrieval ðŸ”—&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/fusion_retrieval.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Optimizing search results by combining different retrieval methods.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Combine keyword-based search with vector-based search for more comprehensive and accurate retrieval.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Intelligent Reranking ðŸ“ˆ&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/reranking.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Applying advanced scoring mechanisms to improve the relevance ranking of retrieved results.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ðŸ§  &lt;strong&gt;LLM-based Scoring:&lt;/strong&gt; Use a language model to score the relevance of each retrieved chunk.&lt;/li&gt; 
   &lt;li&gt;ðŸ”€ &lt;strong&gt;Cross-Encoder Models:&lt;/strong&gt; Re-encode both the query and retrieved documents jointly for similarity scoring.&lt;/li&gt; 
   &lt;li&gt;ðŸ† &lt;strong&gt;Metadata-enhanced Ranking:&lt;/strong&gt; Incorporate metadata into the scoring process for more nuanced ranking.&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/relevance-revolution-how-re-ranking?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&quot;&gt;Relevance Revolution: How Re-ranking Transforms RAG Systems&lt;/a&gt;&lt;/strong&gt; - A comprehensive blog post exploring the power of re-ranking in enhancing RAG system performance.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multi-faceted Filtering ðŸ”&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Applying various filtering techniques to refine and improve the quality of retrieved results.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;ðŸ·ï¸ &lt;strong&gt;Metadata Filtering:&lt;/strong&gt; Apply filters based on attributes like date, source, author, or document type.&lt;/li&gt; 
   &lt;li&gt;ðŸ“Š &lt;strong&gt;Similarity Thresholds:&lt;/strong&gt; Set thresholds for relevance scores to keep only the most pertinent results.&lt;/li&gt; 
   &lt;li&gt;ðŸ“„ &lt;strong&gt;Content Filtering:&lt;/strong&gt; Remove results that don&#39;t match specific content criteria or essential keywords.&lt;/li&gt; 
   &lt;li&gt;ðŸŒˆ &lt;strong&gt;Diversity Filtering:&lt;/strong&gt; Ensure result diversity by filtering out near-duplicate entries.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Hierarchical Indices ðŸ—‚ï¸&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/hierarchical_indices.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/hierarchical_indices.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/hierarchical_indices.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Creating a multi-tiered system for efficient information navigation and retrieval.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Implement a two-tiered system for document summaries and detailed chunks, both containing metadata pointing to the same location in the data.&lt;/p&gt; &lt;h4&gt;Additional Resources ðŸ“š&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/hierarchical-indices-enhancing-rag?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&quot;&gt;Hierarchical Indices: Enhancing RAG Systems&lt;/a&gt;&lt;/strong&gt; - A comprehensive blog post exploring the power of hierarchical indices in enhancing RAG system performance.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ensemble Retrieval ðŸŽ­&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Combining multiple retrieval models or techniques for more robust and accurate results.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Apply different embedding models or retrieval algorithms and use voting or weighting mechanisms to determine the final set of retrieved documents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Dartboard Retrieval ðŸŽ¯&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/dartboard.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/dartboard.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Optimizing over Relevant Information Gain in Retrieval&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Combine both relevance and diversity into a single scoring function and directly optimize for it.&lt;/li&gt; 
   &lt;li&gt;POC showing plain simple RAG underperforming when the database is dense, and the dartboard retrieval outperforming it.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multi-modal Retrieval ðŸ“½ï¸&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Extending RAG capabilities to handle diverse data types for richer responses.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Multi-model RAG with Multimedia Captioning&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_model_rag_with_captioning.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_model_rag_with_captioning.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; - Caption and store all the other multimedia data like pdfs, ppts, etc., with text data in vector store and retrieve them together.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Multi-model RAG with Colpali&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_model_rag_with_colpali.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_model_rag_with_colpali.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; - Instead of captioning convert all the data into image, then find the most relevant images and pass them to a vision large language model.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸ” Iterative and Adaptive Techniques&lt;/h3&gt; 
&lt;ol start=&quot;22&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Retrieval with Feedback Loops ðŸ”&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/retrieval_with_feedback_loop.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/retrieval_with_feedback_loop.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/retrieval_with_feedback_loop.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Implementing mechanisms to learn from user interactions and improve future retrievals.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Collect and utilize user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Adaptive Retrieval ðŸŽ¯&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/adaptive_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/adaptive_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/adaptive_retrieval.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Dynamically adjusting retrieval strategies based on query types and user contexts.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Classify queries into different categories and use tailored retrieval strategies for each, considering user context and preferences.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Iterative Retrieval ðŸ”„&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Performing multiple rounds of retrieval to refine and enhance result quality.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Use the LLM to analyze initial results and generate follow-up queries to fill in gaps or clarify information.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸ“Š Evaluation&lt;/h3&gt; 
&lt;ol start=&quot;25&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepEval Evaluation&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_deep_eval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_deep_eval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; | Comprehensive RAG system evaluation |&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Performing evaluations Retrieval-Augmented Generation systems, by covering several metrics and creating test cases.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Use the &lt;code&gt;deepeval&lt;/code&gt; library to conduct test cases on correctness, faithfulness and contextual relevancy of RAG systems.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GroUSE Evaluation&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_grouse.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_grouse.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; | Contextually-grounded LLM evaluation |&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Evaluate the final stage of Retrieval-Augmented Generation using metrics of the GroUSE framework and meta-evaluate your custom LLM judge on GroUSE unit tests.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Use the &lt;code&gt;grouse&lt;/code&gt; package to evaluate contextually-grounded LLM generations with GPT-4 on the 6 metrics of the GroUSE framework and use unit tests to evaluate a custom Llama 3.1 405B evaluator.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸ”¬ Explainability and Transparency&lt;/h3&gt; 
&lt;ol start=&quot;27&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Explainable Retrieval ðŸ”&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/explainable_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/explainable_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/explainable_retrieval.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Providing transparency in the retrieval process to enhance user trust and system refinement.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Explain why certain pieces of information were retrieved and how they relate to the query.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;ðŸ—ï¸ Advanced Architectures&lt;/h3&gt; 
&lt;ol start=&quot;28&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Graph RAG with Milvus Vector Database ðŸ”&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Graph RAG with Milvus&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graphrag_with_milvus_vectordb.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graphrag_with_milvus_vectordb.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A simple yet powerful approach to implement Graph RAG using Milvus vector databases. This technique significantly improves performance on complex multi-hop questions by combining relationship-based retrieval with vector search and reranking.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Store both text passages and relationship triplets (subject-predicate-object) in separate Milvus collections&lt;/li&gt; 
   &lt;li&gt;Perform multi-way retrieval by querying both collections&lt;/li&gt; 
   &lt;li&gt;Use an LLM to rerank retrieved relationships based on their relevance to the query&lt;/li&gt; 
   &lt;li&gt;Retrieve the final passages based on the most relevant relationships&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Knowledge Graph Integration (Graph RAG) ðŸ•¸ï¸&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graph_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graph_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/graph_rag.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Incorporating structured data from knowledge graphs to enrich context and improve retrieval.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Retrieve entities and their relationships from a knowledge graph relevant to the query, combining this structured data with unstructured text for more informative responses.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GraphRag (Microsoft) ðŸŽ¯&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;GraphRag&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/Microsoft_GraphRag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/Microsoft_GraphRag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Microsoft GraphRAG (Open Source) is an advanced RAG system that integrates knowledge graphs to improve the performance of LLMs&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;â€¢ Analyze an input corpus by extracting entities, relationships from text units. generates summaries of each community and its constituents from the bottom-up.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval ðŸŒ³&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/raptor.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/raptor.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/raptor.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;Implementing a recursive approach to process and organize retrieved information in a tree structure.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;Use abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Self RAG ðŸ”&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/self_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/self_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/self_rag.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A dynamic approach that combines retrieval-based and generation-based methods, adaptively deciding whether to use retrieved information and how to best utilize it in generating responses.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;â€¢ Implement a multi-step process including retrieval decision, document retrieval, relevance evaluation, response generation, support assessment, and utility evaluation to produce accurate, relevant, and useful outputs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Corrective RAG ðŸ”§&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/crag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/crag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/crag.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;A sophisticated RAG approach that dynamically evaluates and corrects the retrieval process, combining vector databases, web search, and language models for highly accurate and context-aware responses.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;â€¢ Integrate Retrieval Evaluator, Knowledge Refinement, Web Search Query Rewriter, and Response Generator components to create a system that adapts its information sourcing strategy based on relevance scores and combines multiple sources when necessary.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ðŸŒŸ Special Advanced Technique ðŸŒŸ&lt;/h2&gt; 
&lt;ol start=&quot;34&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/Controllable-RAG-Agent&quot;&gt;Sophisticated Controllable Agent for Complex RAG Tasks ðŸ¤–&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview ðŸ”Ž&lt;/h4&gt; &lt;p&gt;An advanced RAG solution designed to tackle complex questions that simple semantic similarity-based retrieval cannot solve. This approach uses a sophisticated deterministic graph as the &quot;brain&quot; ðŸ§  of a highly controllable autonomous agent, capable of answering non-trivial questions from your own data.&lt;/p&gt; &lt;h4&gt;Implementation ðŸ› ï¸&lt;/h4&gt; &lt;p&gt;â€¢ Implement a multi-step process involving question anonymization, high-level planning, task breakdown, adaptive information retrieval and question answering, continuous re-planning, and rigorous answer verification to ensure grounded and accurate responses.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To begin implementing these advanced RAG techniques in your projects:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repository: &lt;pre&gt;&lt;code&gt;git clone https://github.com/NirDiamant/RAG_Techniques.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to the technique you&#39;re interested in: &lt;pre&gt;&lt;code&gt;cd all_rag_techniques/technique-name
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Follow the detailed implementation guide in each technique&#39;s directory.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! If you have a new technique or improvement to suggest:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create your feature branch: &lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Commit your changes: &lt;code&gt;git commit -m &#39;Add some AmazingFeature&#39;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Push to the branch: &lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Open a pull request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques/graphs/contributors&quot;&gt;&lt;img src=&quot;https://contrib.rocks/image?repo=NirDiamant/RAG_Techniques&quot; alt=&quot;Contributors&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under a custom non-commercial license - see the &lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;â­ï¸ If you find this repository helpful, please consider giving it a star!&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=main-readme&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;Keywords: RAG, Retrieval-Augmented Generation, NLP, AI, Machine Learning, Information Retrieval, Natural Language Processing, LLM, Embeddings, Semantic Search&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>maxim5/cs229-2018-autumn</title>
      <link>https://github.com/maxim5/cs229-2018-autumn</link>
      <description>&lt;p&gt;All notes and materials for the CS229: Machine Learning course by Stanford University&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CS229 Autumn 2018&lt;/h1&gt; 
&lt;p&gt;All lecture notes, slides and assignments for &lt;a href=&quot;http://cs229.stanford.edu/&quot;&gt;CS229: Machine Learning&lt;/a&gt; course by Stanford University.&lt;/p&gt; 
&lt;p&gt;The videos of all lectures are available &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&quot;&gt;on YouTube&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Useful links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/maxim5/cs229-2019-summer&quot;&gt;CS229 Summer 2019 edition&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>google-deepmind/deepmind-research</title>
      <link>https://github.com/google-deepmind/deepmind-research</link>
      <description>&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepMind Research&lt;/h1&gt; 
&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications. Along with publishing papers to accompany research conducted at DeepMind, we release open-source &lt;a href=&quot;https://deepmind.com/research/open-source/open-source-environments/&quot;&gt;environments&lt;/a&gt;, &lt;a href=&quot;https://deepmind.com/research/open-source/open-source-datasets/&quot;&gt;data sets&lt;/a&gt;, and &lt;a href=&quot;https://deepmind.com/research/open-source/open-source-code/&quot;&gt;code&lt;/a&gt; to enable the broader research community to engage with our work and build upon it, with the ultimate goal of accelerating scientific progress to benefit society. For example, you can build on our implementations of the &lt;a href=&quot;https://github.com/deepmind/dqn&quot;&gt;Deep Q-Network&lt;/a&gt; or &lt;a href=&quot;https://github.com/deepmind/dnc&quot;&gt;Differential Neural Computer&lt;/a&gt;, or experiment in the same environments we use for our research, such as &lt;a href=&quot;https://github.com/deepmind/lab&quot;&gt;DeepMind Lab&lt;/a&gt; or &lt;a href=&quot;https://github.com/deepmind/pysc2&quot;&gt;StarCraft II&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you enjoy building tools, environments, software libraries, and other infrastructure of the kind listed below, you can view open positions to work in related areas on our &lt;a href=&quot;https://deepmind.com/careers/&quot;&gt;careers page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For a full list of our publications, please see &lt;a href=&quot;https://deepmind.com/research/publications/&quot;&gt;https://deepmind.com/research/publications/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/fusion_tcv&quot;&gt;Magnetic control of tokamak plasmas through deep reinforcement learning&lt;/a&gt;, Nature 2022&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/density_functional_approximation_dm21&quot;&gt;Pushing the Frontiers of Density Functionals by Solving the Fractional Electron Problem&lt;/a&gt;, Science 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/pitfalls_static_language_models&quot;&gt;Mind the Gap: Assessing Temporal Generalization in Neural Language Models&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/tandem_dqn&quot;&gt;The Difficulty of Passive Learning in Deep Reinforcement Learning&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/nowcasting&quot;&gt;Skilful precipitation nowcasting using deep generative models of radar&lt;/a&gt;, Nature 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/cadl&quot;&gt;Compute-Aided Design as Language&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/continual_learning&quot;&gt;Encoders and ensembles for continual learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/hierarchical_transformer_memory&quot;&gt;Towards mental time travel: a hierarchical memory for reinforcement learning agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/perceiver&quot;&gt;Perceiver IO: A General Architecture for Structured Inputs &amp;amp; Outputs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/neural_mip_solving&quot;&gt;Solving Mixed Integer Programs Using Neural Networks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/noisy_label&quot;&gt;A Realistic Simulation Framework for Learning with Label Noise&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/rapid_task_solving&quot;&gt;Rapid Task-Solving in Novel Environments&lt;/a&gt;, ICLR 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/wikigraphs&quot;&gt;WikiGraphs: A Wikipedia - Knowledge Graph Paired Dataset&lt;/a&gt;, TextGraphs 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/box_arrangement&quot;&gt;Behavior Priors for Efficient Reinforcement Learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/meshgraphnets&quot;&gt;Learning Mesh-Based Simulation with Graph Networks&lt;/a&gt;, ICLR 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/ogb_lsc&quot;&gt;Open Graph Benchmark - Large-Scale Challenge (OGB-LSC)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/synthetic_returns&quot;&gt;Synthetic Returns for Long-Term Credit Assignment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/galaxy_mergers&quot;&gt;A Deep Learning Approach for Characterizing Major Galaxy Mergers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/kfac_ferminet_alpha&quot;&gt;Better, Faster Fermionic Neural Networks&lt;/a&gt; (KFAC implementation)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/object_attention_for_reasoning&quot;&gt;Object-based attention for spatio-temporal reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/enformer&quot;&gt;Effective gene expression prediction from sequence by integrating long-range interactions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/satore&quot;&gt;Satore: First-order logic saturation with atom rewriting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/nfnets&quot;&gt;Characterizing signal propagation to close the performance gap in unnormalized ResNets&lt;/a&gt;, ICLR 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/adversarial_robustness&quot;&gt;Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/cmtouch&quot;&gt;Learning rich touch representations through cross-modal self-supervision&lt;/a&gt;, CoRL 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/functional_regularisation_for_continual_learning&quot;&gt;Functional Regularisation for Continual Learning&lt;/a&gt;, ICLR 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/avae&quot;&gt;The Autoencoding Variational Autoencoder&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/mmv&quot;&gt;Self-Supervised MultiModal Versatile Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/ode_gan&quot;&gt;ODE-GAN: Training GANs by Solving Ordinary Differential Equations&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/causal_reasoning&quot;&gt;Algorithms for Causal Reasoning in Probability Trees&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/gated_linear_networks&quot;&gt;Gated Linear Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/himo&quot;&gt;Value-driven Hindsight Modelling&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/learned_free_energy_estimation&quot;&gt;Targeted free energy estimation via learned mappings&lt;/a&gt;, Journal of Chemical Physics 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/learning_to_simulate&quot;&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/a&gt;, ICML 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/physics_planning_games&quot;&gt;Physically Embedded Planning Problems&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/polygen&quot;&gt;PolyGen: PolyGen: An Autoregressive Generative Model of 3D Meshes&lt;/a&gt;, ICML 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/byol&quot;&gt;Bootstrap Your Own Latent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/catch_carry&quot;&gt;Catch &amp;amp; Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks&lt;/a&gt;, SIGGRAPH 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/memo&quot;&gt;MEMO: A Deep Network For Flexible Combination Of Episodic Memories&lt;/a&gt;, ICLR 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/rl_unplugged&quot;&gt;RL Unplugged: Benchmarks for Offline Reinforcement Learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/geomancer&quot;&gt;Disentangling by Subspace Diffusion (GEOMANCER)&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/affordances_theory&quot;&gt;What can I do here? A theory of affordances in reinforcement learning&lt;/a&gt;, ICML 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/sketchy&quot;&gt;Scaling data-driven robotics with reward sketching and batch reinforcement learning&lt;/a&gt;, RSS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/counterfactual_fairness&quot;&gt;Path-Specific Counterfactual Fairness&lt;/a&gt;, AAAI 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/option_keyboard&quot;&gt;The Option Keyboard: Combining Skills in Reinforcement Learning&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/visr&quot;&gt;VISR - Fast Task Inference with Variational Intrinsic Successor Features&lt;/a&gt;, ICLR 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/glassy_dynamics&quot;&gt;Unveiling the predictive power of static structure in glassy systems&lt;/a&gt;, Nature Physics 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/iodine&quot;&gt;Multi-Object Representation Learning with Iterative Variational Inference (IODINE)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/alphafold_casp13&quot;&gt;AlphaFold CASP13&lt;/a&gt;, Nature 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/unrestricted_advx&quot;&gt;Unrestricted Adversarial Challenge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/hierarchical_probabilistic_unet&quot;&gt;Hierarchical Probabilistic U-Net (HPU-Net)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/scratchgan&quot;&gt;Training Language GANs from Scratch&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/tvt&quot;&gt;Temporal Value Transport&lt;/a&gt;, Nature Communications 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/curl&quot;&gt;Continual Unsupervised Representation Learning (CURL)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/transporter&quot;&gt;Unsupervised Learning of Object Keypoints (Transporter)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/bigbigan&quot;&gt;BigBiGAN&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/cs_gan&quot;&gt;Deep Compressed Sensing&lt;/a&gt;, ICML 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/side_effects_penalties&quot;&gt;Side Effects Penalties&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/PrediNet&quot;&gt;PrediNet Architecture and Relations Game Datasets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/unsupervised_adversarial_training&quot;&gt;Unsupervised Adversarial Training&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/graph_matching_networks&quot;&gt;Graph Matching Networks for Learning the Similarity of Graph Structured Objects&lt;/a&gt;, ICML 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/regal&quot;&gt;REGAL: Transfer Learning for Fast Optimization of Computation Graphs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/ensemble_loss_landscape&quot;&gt;Deep Ensembles: A Loss Landscape Perspective&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/powerpropagation&quot;&gt;Powerpropagation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/physics_inspired_models&quot;&gt;Physics Inspired Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/langchain</title>
      <link>https://github.com/langchain-ai/langchain</link>
      <description>&lt;p&gt;ðŸ¦œðŸ”— Build context-aware reasoning applications&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;docs/static/img/logo-dark.svg&quot; /&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;docs/static/img/logo-light.svg&quot; /&gt; 
 &lt;img alt=&quot;LangChain Logo&quot; src=&quot;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/logo-dark.svg?sanitize=true&quot; width=&quot;80%&quot; /&gt; 
&lt;/picture&gt; 
&lt;div&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square&quot; alt=&quot;Release Notes&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml&quot;&gt;&lt;img src=&quot;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml/badge.svg?sanitize=true&quot; alt=&quot;CI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/langchain-core?style=flat-square&quot; alt=&quot;PyPI - License&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypistats.org/packages/langchain-core&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/dm/langchain-core?style=flat-square&quot; alt=&quot;PyPI - Downloads&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://star-history.com/#langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/langchain-ai/langchain?style=flat-square&quot; alt=&quot;GitHub star chart&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/langchain-ai/langchain/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/langchain-ai/langchain?style=flat-square&quot; alt=&quot;Open Issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&amp;amp;style=flat-square&quot; alt=&quot;Open in Dev Containers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codespaces.new/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in Github Codespace&quot; title=&quot;Open in Github Codespace&quot; width=&quot;150&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/langchainai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&amp;amp;label=Follow%20%40LangChainAI&quot; alt=&quot;Twitter&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codspeed.io/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://codspeed.io/badge.json&quot; alt=&quot;CodSpeed Badge&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JS/TS library? Check out &lt;a href=&quot;https://github.com/langchain-ai/langchainjs&quot;&gt;LangChain.js&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development â€” all while future-proofing decisions as the underlying technology evolves.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U langchain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To learn more about LangChain, check out &lt;a href=&quot;https://python.langchain.com/docs/introduction/&quot;&gt;the docs&lt;/a&gt;. If youâ€™re looking for more advanced customization or agent orchestration, check out &lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt;, our framework for building controllable agent workflows.&lt;/p&gt; 
&lt;h2&gt;Why use LangChain?&lt;/h2&gt; 
&lt;p&gt;LangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.&lt;/p&gt; 
&lt;p&gt;Use LangChain for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time data augmentation&lt;/strong&gt;. Easily connect LLMs to diverse data sources and external / internal systems, drawing from LangChainâ€™s vast library of integrations with model providers, tools, vector stores, retrievers, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model interoperability&lt;/strong&gt;. Swap models in and out as your engineering team experiments to find the best choice for your applicationâ€™s needs. As the industry frontier evolves, adapt quickly â€” LangChainâ€™s abstractions keep you moving without losing momentum.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;LangChainâ€™s ecosystem&lt;/h2&gt; 
&lt;p&gt;While the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.&lt;/p&gt; 
&lt;p&gt;To improve your LLM application development, pair LangChain with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.langchain.com/langsmith&quot;&gt;LangSmith&lt;/a&gt; - Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt; - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows â€” and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/&quot;&gt;LangGraph Platform&lt;/a&gt; - Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams â€” and iterate quickly with visual prototyping in &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/&quot;&gt;LangGraph Studio&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/tutorials/&quot;&gt;Tutorials&lt;/a&gt;: Simple walkthroughs with guided examples on getting started with LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/how_to/&quot;&gt;How-to Guides&lt;/a&gt;: Quick, actionable code snippets for topics such as tool calling, RAG use cases, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/concepts/&quot;&gt;Conceptual Guides&lt;/a&gt;: Explanations of key concepts behind the LangChain framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://forum.langchain.com/&quot;&gt;LangChain Forum&lt;/a&gt;: Connect with the community and share all of your technical questions, ideas, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/api_reference/&quot;&gt;API Reference&lt;/a&gt;: Detailed reference on navigating base packages and integrations for LangChain.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>unslothai/notebooks</title>
      <link>https://github.com/unslothai/notebooks</link>
      <description>&lt;p&gt;100+ Fine-tuning LLM Notebooks on Google Colab, Kaggle, and more.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://unsloth.ai&quot;&gt;
   &lt;picture&gt; 
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png&quot; /&gt; 
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot; /&gt; 
    &lt;img alt=&quot;unsloth logo&quot; src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot; height=&quot;110&quot; style=&quot;max-width: 100%;&quot; /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png&quot; height=&quot;48&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/unsloth&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png&quot; height=&quot;48&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://docs.unsloth.ai&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png&quot; height=&quot;48&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ðŸ“’ Fine-tuning Notebooks&lt;/h2&gt; 
&lt;p&gt;Below are our notebooks for Google Colab categorized by model. You can view our &lt;a href=&quot;https://github.com/unslothai/notebooks/#-kaggle-notebooks&quot;&gt;Kaggle notebooks here&lt;/a&gt;.&lt;br /&gt;Use our guided notebooks to prep data, train, evaluate, and save your model. View our main &lt;a href=&quot;https://github.com/unslothai/unsloth&quot;&gt;GitHub repo here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Main Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Multimodal&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open in Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_%2814B%29-Reasoning-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3-Base (4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_%284B%29-GRPO.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma 3 (4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_%284B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%281B_and_3B%29-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi-4 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 Vision (11B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%2811B%29-Vision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_%288B%29-Alpaca.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_%287B%29-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;DeepSeek-R1-0528-Qwen3 (8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/DeepSeek_R1_0528_Qwen3_(8B)_GRPO.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 (3B) by Meta&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Synthetic Data&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta_Synthetic_Data_Llama3_2_%283B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Sesame-CSM (1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_%281B%29-TTS.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Text-to-Speech (TTS) Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Sesame-CSM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_%281B%29-TTS.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Orpheus-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_%283B%29-TTS.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Spark-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Spark_TTS_%280_5B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Oute-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Oute_TTS_%281B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Oute-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Oute_TTS_%281B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llasa TTS (1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_%281B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llasa TTS (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_%283B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Whisper-Large-V3&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;STT&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Whisper.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Vision (Multimodal) Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 (11B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%2811B%29-Vision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5 VL (7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_VL_%287B%29-Vision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Pixtral (12B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_%2812B%29-Vision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;BERT Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ModernBERT-large&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/timothelaborie/text_classification_scripts/blob/main/bert_classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Specific use-case Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Usecase&lt;/th&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text Classification&lt;/td&gt; 
   &lt;td&gt;Llama 3.1 (8B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/timothelaborie/text_classification_scripts/blob/main/unsloth_classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tool Calling&lt;/td&gt; 
   &lt;td&gt;Qwen2.5-Coder (1.5B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(1.5B)-Tool_Calling.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multiple Datasets&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1njCCbE1YVal9xC83hjdo2hiGItpY_D6t?usp=sharing&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KTO&lt;/td&gt; 
   &lt;td&gt;Qwen2.5-Instruct (1.5B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference Chat UI&lt;/td&gt; 
   &lt;td&gt;LLaMa 3.2 Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;LLaMa 3.2 (1B and 3B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ChatML&lt;/td&gt; 
   &lt;td&gt;Mistral (7B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text Completion&lt;/td&gt; 
   &lt;td&gt;Mistral (7B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- ðŸ›‘ ðŸš¨ DO NOT EDIT MANUALLY THIS SECTION UNTIL `end of notebook links`!! ðŸ›‘ ðŸš¨ --&gt; 
&lt;!-- ðŸ›‘ ðŸš¨ THIS SECTION IS GENERATED BY `update_all_notebooks.py` AUTOMATICALLY ðŸ›‘ ðŸš¨  --&gt; 
&lt;!-- START OF EDITING --&gt; 
&lt;h3&gt;GRPO Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi 4&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Meta Synthetic Data Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta-Synthetic-Data-Llama3.1_(8B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Meta Synthetic Data Llama3 2&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta_Synthetic_Data_Llama3_2_(3B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(1B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;DeepSeek R1 0528 Qwen3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/DeepSeek_R1_0528_Qwen3_(8B)_GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;GPT-OSS Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;gpt oss&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;GPT OSS BNB&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_BNB_(20B)-Inference.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;GPT OSS MXFP4&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Gemma Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CodeGemma&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/CodeGemma_(7B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(2B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(2B)-Inference.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Multimodal&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Audio&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Audio.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma2&lt;/strong&gt; &lt;strong&gt;(9B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma2&lt;/strong&gt; &lt;strong&gt;(2B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(2B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Linear Attention Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Liquid LFM2&lt;/strong&gt; &lt;strong&gt;(1.2B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Liquid_LFM2_(1.2B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Falcon H1&lt;/strong&gt; &lt;strong&gt;(0.5B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Falcon_H1_(0.5B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Llama Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(11B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(1B and 3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RAFT&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B)-RAFT.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Inference.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llasa TTS&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_(3B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ORPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;TinyLlama&lt;/strong&gt; &lt;strong&gt;(1.1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/TinyLlama_(1.1B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llasa TTS&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_(1B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Mistral Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral Small&lt;/strong&gt; &lt;strong&gt;(22B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral Nemo&lt;/strong&gt; &lt;strong&gt;(12B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Nemo_(12B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Pixtral&lt;/strong&gt; &lt;strong&gt;(12B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Text Completion&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Zephyr&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;DPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;CPT&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-CPT.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Orpheus Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Orpheus&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Oute Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Oute TTS&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Oute_TTS_(1B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Phi Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi 4&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi 3.5 Mini&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi 3 Medium&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3_Medium-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Qwen Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5 Coder&lt;/strong&gt; &lt;strong&gt;(1.5B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Tool Calling&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(1.5B)-Tool_Calling.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5 Coder&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(14B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5 VL&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_VL_(7B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2 VL&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_(7B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Spark Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Spark TTS&lt;/strong&gt; &lt;strong&gt;(0 5B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Spark_TTS_(0_5B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Whisper Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Whisper.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Other Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Magistral&lt;/strong&gt; &lt;strong&gt;(24B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Magistral_(24B)-Reasoning-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Sesame CSM&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_(1B)-TTS.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Unsloth&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Studio&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CodeForces cot Finetune for Reasoning on CodeForces&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/CodeForces-cot-Finetune_for_Reasoning_on_CodeForces.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;ðŸ“’ Kaggle Notebooks&lt;/h1&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click for all our Kaggle notebooks categorized by model: &lt;/summary&gt; 
 &lt;h3&gt;GRPO Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Phi 4&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_4_(14B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Meta Synthetic Data Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Meta-Synthetic-Data-Llama3.1_(8B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.1_(8B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3_(1B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Meta Synthetic Data Llama3 2&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Meta_Synthetic_Data_Llama3_2_(3B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(4B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2.5&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_(3B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;DeepSeek R1 0528 Qwen3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-DeepSeek_R1_0528_Qwen3_(8B)_GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;GPT-OSS Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;GPT OSS BNB&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Inference&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-GPT_OSS_BNB_(20B)-Inference.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;gpt oss&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-gpt-oss-(20B)-Fine-tuning.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;GPT OSS MXFP4&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Inference&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-GPT_OSS_MXFP4_(20B)-Inference.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Gemma Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;CodeGemma&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-CodeGemma_(7B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3_(4B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Audio&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3N_(4B)-Audio.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(2B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Inference&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3N_(2B)-Inference.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3N_(4B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3_(4B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Multimodal&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3N_(4B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma2&lt;/strong&gt; &lt;strong&gt;(2B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma2_(2B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma2&lt;/strong&gt; &lt;strong&gt;(9B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma2_(9B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Linear Attention Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Liquid LFM2&lt;/strong&gt; &lt;strong&gt;(1.2B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Liquid_LFM2_(1.2B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Falcon H1&lt;/strong&gt; &lt;strong&gt;(0.5B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Falcon_H1_(0.5B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Llama Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(1B and 3B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.2_(1B_and_3B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(11B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.2_(11B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;RAFT&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.2_(1B)-RAFT.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Inference&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.1_(8B)-Inference.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.1_(8B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llasa TTS&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llasa_TTS_(3B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Ollama&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-Ollama.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;ORPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-ORPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;TinyLlama&lt;/strong&gt; &lt;strong&gt;(1.1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-TinyLlama_(1.1B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llasa TTS&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llasa_TTS_(1B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Mistral Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral Small&lt;/strong&gt; &lt;strong&gt;(22B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_Small_(22B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral Nemo&lt;/strong&gt; &lt;strong&gt;(12B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_Nemo_(12B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Pixtral&lt;/strong&gt; &lt;strong&gt;(12B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Pixtral_(12B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Text Completion&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_(7B)-Text_Completion.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Zephyr&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;DPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Zephyr_(7B)-DPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;CPT&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-CPT.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Orpheus Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Orpheus&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Orpheus_(3B)-TTS.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Oute Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Oute TTS&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Oute_TTS_(1B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Phi Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Phi 4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_4-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Phi 3.5 Mini&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_3.5_Mini-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Phi 3 Medium&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_3_Medium-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Qwen Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(14B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(14B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Reasoning Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(14B)-Reasoning-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2.5 Coder&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_Coder_(14B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2.5&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_(7B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2.5 Coder&lt;/strong&gt; &lt;strong&gt;(1.5B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Tool Calling&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_Coder_(1.5B)-Tool_Calling.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2.5 VL&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_VL_(7B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2 VL&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2_VL_(7B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2_(7B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Spark Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Spark TTS&lt;/strong&gt; &lt;strong&gt;(0 5B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Spark_TTS_(0_5B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Whisper Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Whisper.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Other Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Magistral&lt;/strong&gt; &lt;strong&gt;(24B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Reasoning Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Magistral_(24B)-Reasoning-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Sesame CSM&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Sesame_CSM_(1B)-TTS.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;CodeForces cot Finetune for Reasoning on CodeForces&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Reasoning&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-CodeForces-cot-Finetune_for_Reasoning_on_CodeForces.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Unsloth&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Studio&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Unsloth_Studio.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;!-- End of Notebook Links --&gt; 
&lt;h1&gt;âœ¨ Contributing to Notebooks&lt;/h1&gt; 
&lt;p&gt;If you&#39;d like to contribute to our notebooks, here&#39;s a guide to get you started:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Find the Template:&lt;/strong&gt; We&#39;ve provided a template notebook called &lt;code&gt;Template_Notebook.ipynb&lt;/code&gt; in the root directory of this project. This template contains the basic structure and formatting guidelines for all notebooks in this collection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create Your Notebook:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Make a copy of &lt;code&gt;Template_Notebook.ipynb&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Rename the copied file to follow this naming convention: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;LLM Notebooks:&lt;/strong&gt; &lt;code&gt;&amp;lt;Model Name&amp;gt;-&amp;lt;Type&amp;gt;.ipynb&lt;/code&gt; (e.g., &lt;code&gt;Mistral_v0.3_(7B)-Alpaca.ipynb&lt;/code&gt;)&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Vision Notebooks:&lt;/strong&gt; &lt;code&gt;&amp;lt;Model Name&amp;gt;-Vision.ipynb&lt;/code&gt; (e.g., &lt;code&gt;Llava_v1.6_(7B)-Vision.ipynb&lt;/code&gt;)&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Example of &lt;code&gt;&amp;lt;Type&amp;gt;&lt;/code&gt;:&lt;/strong&gt; &lt;code&gt;Alpaca&lt;/code&gt;, &lt;code&gt;Conversational&lt;/code&gt;, &lt;code&gt;CPT&lt;/code&gt;, &lt;code&gt;DPO&lt;/code&gt;, &lt;code&gt;ORPO&lt;/code&gt;, &lt;code&gt;Text_Completion&lt;/code&gt;, &lt;code&gt;CSV&lt;/code&gt;, &lt;code&gt;Inference&lt;/code&gt;, &lt;code&gt;Unsloth_Studio&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;!-- *   Modify the content of your notebook, adding your code, explanations, and any other relevant information. Make sure to follow the structure and guidelines from the template. --&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Place in &lt;code&gt;original_template&lt;/code&gt;:&lt;/strong&gt; Once your notebook is ready, move it to the &lt;code&gt;original_template&lt;/code&gt; directory.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Update Notebooks:&lt;/strong&gt; Run the following command in your terminal: &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python update_all_notebooks.py
&lt;/code&gt;&lt;/pre&gt; This script will automatically: 
  &lt;ul&gt; 
   &lt;li&gt;Copy your notebook from &lt;code&gt;original_template&lt;/code&gt; to the &lt;code&gt;notebooks&lt;/code&gt; directory.&lt;/li&gt; 
   &lt;li&gt;Update the notebook&#39;s internal sections (like Installation, News) to ensure consistency.&lt;/li&gt; 
   &lt;li&gt;Add your notebook to the appropriate list in this &lt;code&gt;README.md&lt;/code&gt; file.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create a Pull Request:&lt;/strong&gt; After that, just create a pull request (PR) to merge your changes, making it available for everyone! 
  &lt;ul&gt; 
   &lt;li&gt;We appreciate your contributions and look forward to reviewing your notebooks!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>yandexdataschool/nlp_course</title>
      <link>https://github.com/yandexdataschool/nlp_course</link>
      <description>&lt;p&gt;YSDA course in Natural Language Processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;YSDA Natural Language Processing course&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;This is the 2024 version. For previous year&#39; course materials, go to &lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/tree/2023&quot;&gt;this branch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Lecture and seminar materials for each week are in ./week* folders, see README.md for materials and instructions&lt;/li&gt; 
 &lt;li&gt;Any technical issues, ideas, bugs in course materials, contribution ideas - add an &lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/issues&quot;&gt;issue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Installing libraries and troubleshooting: &lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/issues/1&quot;&gt;this thread&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Syllabus&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week01_embeddings&quot;&gt;&lt;strong&gt;week01&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Word Embeddings&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Word embeddings. Distributional semantics. Count-based (pre-neural) methods. Word2Vec: learn vectors. GloVe: count, then learn. Evaluation: intrinsic vs extrinsic. Analysis and Interpretability. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_word_emb&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Seminar: Playing with word and sentence embeddings&lt;/li&gt; 
   &lt;li&gt;Homework: Embedding-based machine translation system&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week02_classification&quot;&gt;&lt;strong&gt;week02&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Text Classification&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Text classification: introduction and datasets. General framework: feature extractor + classifier. Classical approaches: Naive Bayes, MaxEnt (Logistic Regression), SVM. Neural Networks: General View, Convolutional Models, Recurrent Models. Practical Tips: Data Augmentation. Analysis and Interpretability. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_text_clf&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Seminar: Text classification with convolutional NNs.&lt;/li&gt; 
   &lt;li&gt;Homework: Statistical &amp;amp; neural text classification.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week03_lm&quot;&gt;&lt;strong&gt;week03&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Language Modeling&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Language Modeling: what does it mean? Left-to-right framework. N-gram language models. Neural Language Models: General View, Recurrent Models, Convolutional Models. Evaluation. Practical Tips: Weight Tying. Analysis and Interpretability. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_lang_models&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Seminar: Build a N-gram language model from scratch&lt;/li&gt; 
   &lt;li&gt;Homework: Neural LMs &amp;amp; smoothing in count-based models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week04_seq2seq&quot;&gt;&lt;strong&gt;week04&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Seq2seq and Attention&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Seq2seq Basics: Encoder-Decoder framework, Training, Simple Models, Inference (e.g., beam search). Attention: general, score functions, models. Transformer: self-attention, masked self-attention, multi-head attention; model architecture. Subword Segmentation (BPE). Analysis and Interpretability: functions of attention heads; probing for linguistic structure. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_seq2seq_attn&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Seminar: Basic sequence to sequence model&lt;/li&gt; 
   &lt;li&gt;Homework: Machine translation with attention&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week05_transfer&quot;&gt;&lt;strong&gt;week05&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Transfer Learning&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: What is Transfer Learning? Great idea 1: From Words to Words-in-Context (CoVe, ELMo). Great idea 2: From Replacing Embeddings to Replacing Models (GPT, BERT). (A Bit of) Adaptors. Analysis and Interpretability. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_transfer&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Homework: fine-tuning a pre-trained BERT model&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week06_llm&quot;&gt;&lt;strong&gt;week06&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;LLMs and Prompting&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Scaling laws. Emergent abilities. Prompting (aka &quot;in-context learning&quot;): techiques that work; questioning whether model &quot;understands&quot; prompts. Hypotheses for why and how in-context learning works. Analysis and Interpretability.&lt;/li&gt; 
   &lt;li&gt;Homework: manual prompt engneering and chain-of-thought reasoning&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week07_peft&quot;&gt;&lt;strong&gt;week07&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Transformer architecture and training&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: training tips for transformers; the evolution of transformer architecture from Vaswani et al (2017) to modern LLMs; parameter-efficient fine-tuning (PEFT)&lt;/li&gt; 
   &lt;li&gt;Homework: fine-tuning a large language model with PEFT algorithms&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week08_rlhf&quot;&gt;&lt;strong&gt;week08&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Reinforcement Learning from Human Feedback&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: model alignment, RLHF, case study of InstructGPT and ChatGPT&lt;/li&gt; 
   &lt;li&gt;Homework: fine-tune your own language model with RL (using HuggingFace &lt;code&gt;trl&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week_extra/domain_adaptation&quot;&gt;&lt;strong&gt;week09 (extra)&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Domain Adaptation in NLP&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: why do domain adaptation? Methods: reweighting, proxy labels, adversarial domain adaptation&lt;/li&gt; 
   &lt;li&gt;Optional homework: implement domain adaptation when fine-tuning BERT models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week10_efficiency&quot;&gt;&lt;strong&gt;week10&lt;/strong&gt;_&lt;/a&gt; &lt;strong&gt;Efficient Inference in NLP&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: how NLP models are deployed, a survey of compression and acceleration: quantization, sparsification, ACT &amp;amp; more&lt;/li&gt; 
   &lt;li&gt;Practice: implement RTN and GPTQ for 4-bit LLM quantization&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week_extra/retrieval&quot;&gt;&lt;strong&gt;week11 (extra)&lt;/strong&gt;_&lt;/a&gt; &lt;strong&gt;Retrieval Augmented Language Models&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Guest lecture: retrieval in LMs, token-level retrieval (KNNLM &amp;amp; more), RAG, RETRO, tools: langchain , HF Agents, open problems&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributors &amp;amp; course staff&lt;/h1&gt; 
&lt;p&gt;Course materials and teaching performed by&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://lena-voita.github.io&quot;&gt;Elena Voita&lt;/a&gt; - course author&lt;/li&gt; 
 &lt;li&gt;[Mikhail Diskin] [Ignat Romanov] [Ruslan Svirschevski] - lectures&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.hse.ru/org/persons/831207784/?_gl=1%2a1hz2yht%2a_ga%2aMTg3MTM2ODIwMS4xNjk4NTEyODg5%2a_ga_D145P1R4PL%2aMTY5ODUxMjg4OC4xLjAuMTY5ODUxMjg4OC42MC4wLjA.&quot;&gt;Valentina Broner&lt;/a&gt; - course admin for on-campus students&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kovarsky&quot;&gt;Boris Kovarsky&lt;/a&gt;, &lt;a href=&quot;https://github.com/drt7&quot;&gt;David Talbot&lt;/a&gt;, &lt;a href=&quot;https://github.com/esgv&quot;&gt;Sergey Gubanov&lt;/a&gt;, &lt;a href=&quot;https://github.com/justheuristic&quot;&gt;Just Heuristic&lt;/a&gt; - help build course materials and/or held some classes&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/graphs/contributors&quot;&gt;30+ volunteers&lt;/a&gt; who contributed and refined the notebooks and course materials. Without their help, the course would not be what it is today&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://lk.yandexdataschool.ru/courses/2023-autumn/7.1171-avtomaticheskaia-obrabotka-tekstov/&quot;&gt;A mighty host of TAs&lt;/a&gt; who stoically grade hundreds of homework submissions from on-campus students each year&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/self-llm</title>
      <link>https://github.com/datawhalechina/self-llm</link>
      <description>&lt;p&gt;ã€Šå¼€æºå¤§æ¨¡åž‹é£Ÿç”¨æŒ‡å—ã€‹é’ˆå¯¹ä¸­å›½å®å®é‡èº«æ‰“é€ çš„åŸºäºŽLinuxçŽ¯å¢ƒå¿«é€Ÿå¾®è°ƒï¼ˆå…¨å‚æ•°/Loraï¼‰ã€éƒ¨ç½²å›½å†…å¤–å¼€æºå¤§æ¨¡åž‹ï¼ˆLLMï¼‰/å¤šæ¨¡æ€å¤§æ¨¡åž‹ï¼ˆMLLMï¼‰æ•™ç¨‹&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/images/head-img.png&quot; /&gt; 
 &lt;h1&gt;å¼€æºå¤§æ¨¡åž‹é£Ÿç”¨æŒ‡å—&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;ä¸­æ–‡ | &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/README_en.md&quot;&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;â€ƒâ€ƒæœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå›´ç»•å¼€æºå¤§æ¨¡åž‹ã€é’ˆå¯¹å›½å†…åˆå­¦è€…ã€åŸºäºŽ Linux å¹³å°çš„ä¸­å›½å®å®ä¸“å±žå¤§æ¨¡åž‹æ•™ç¨‹ï¼Œé’ˆå¯¹å„ç±»å¼€æºå¤§æ¨¡åž‹æä¾›åŒ…æ‹¬çŽ¯å¢ƒé…ç½®ã€æœ¬åœ°éƒ¨ç½²ã€é«˜æ•ˆå¾®è°ƒç­‰æŠ€èƒ½åœ¨å†…çš„å…¨æµç¨‹æŒ‡å¯¼ï¼Œç®€åŒ–å¼€æºå¤§æ¨¡åž‹çš„éƒ¨ç½²ã€ä½¿ç”¨å’Œåº”ç”¨æµç¨‹ï¼Œè®©æ›´å¤šçš„æ™®é€šå­¦ç”Ÿã€ç ”ç©¶è€…æ›´å¥½åœ°ä½¿ç”¨å¼€æºå¤§æ¨¡åž‹ï¼Œå¸®åŠ©å¼€æºã€è‡ªç”±çš„å¤§æ¨¡åž‹æ›´å¿«èžå…¥åˆ°æ™®é€šå­¦ä¹ è€…çš„ç”Ÿæ´»ä¸­ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒæœ¬é¡¹ç›®çš„ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;åŸºäºŽ Linux å¹³å°çš„å¼€æº LLM çŽ¯å¢ƒé…ç½®æŒ‡å—ï¼Œé’ˆå¯¹ä¸åŒæ¨¡åž‹è¦æ±‚æä¾›ä¸åŒçš„è¯¦ç»†çŽ¯å¢ƒé…ç½®æ­¥éª¤ï¼›&lt;/li&gt; 
 &lt;li&gt;é’ˆå¯¹å›½å†…å¤–ä¸»æµå¼€æº LLM çš„éƒ¨ç½²ä½¿ç”¨æ•™ç¨‹ï¼ŒåŒ…æ‹¬ LLaMAã€ChatGLMã€InternLM ç­‰ï¼›&lt;/li&gt; 
 &lt;li&gt;å¼€æº LLM çš„éƒ¨ç½²åº”ç”¨æŒ‡å¯¼ï¼ŒåŒ…æ‹¬å‘½ä»¤è¡Œè°ƒç”¨ã€åœ¨çº¿ Demo éƒ¨ç½²ã€LangChain æ¡†æž¶é›†æˆç­‰ï¼›&lt;/li&gt; 
 &lt;li&gt;å¼€æº LLM çš„å…¨é‡å¾®è°ƒã€é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼å…¨é‡å¾®è°ƒã€LoRAã€ptuning ç­‰ã€‚&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;â€ƒâ€ƒ&lt;strong&gt;é¡¹ç›®çš„ä¸»è¦å†…å®¹å°±æ˜¯æ•™ç¨‹ï¼Œè®©æ›´å¤šçš„å­¦ç”Ÿå’Œæœªæ¥çš„ä»Žä¸šè€…äº†è§£å’Œç†Ÿæ‚‰å¼€æºå¤§æ¨¡åž‹çš„é£Ÿç”¨æ–¹æ³•ï¼ä»»ä½•äººéƒ½å¯ä»¥æå‡ºissueæˆ–æ˜¯æäº¤PRï¼Œå…±åŒæž„å»ºç»´æŠ¤è¿™ä¸ªé¡¹ç›®ã€‚&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒæƒ³è¦æ·±åº¦å‚ä¸Žçš„åŒå­¦å¯ä»¥è”ç³»æˆ‘ä»¬ï¼Œæˆ‘ä»¬ä¼šå°†ä½ åŠ å…¥åˆ°é¡¹ç›®çš„ç»´æŠ¤è€…ä¸­ã€‚&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â€ƒâ€ƒ&lt;em&gt;&lt;strong&gt;å­¦ä¹ å»ºè®®ï¼šæœ¬é¡¹ç›®çš„å­¦ä¹ å»ºè®®æ˜¯ï¼Œå…ˆå­¦ä¹ çŽ¯å¢ƒé…ç½®ï¼Œç„¶åŽå†å­¦ä¹ æ¨¡åž‹çš„éƒ¨ç½²ä½¿ç”¨ï¼Œæœ€åŽå†å­¦ä¹ å¾®è°ƒã€‚å› ä¸ºçŽ¯å¢ƒé…ç½®æ˜¯åŸºç¡€ï¼Œæ¨¡åž‹çš„éƒ¨ç½²ä½¿ç”¨æ˜¯åŸºç¡€ï¼Œå¾®è°ƒæ˜¯è¿›é˜¶ã€‚åˆå­¦è€…å¯ä»¥é€‰æ‹©Qwen1.5ï¼ŒInternLM2ï¼ŒMiniCPMç­‰æ¨¡åž‹ä¼˜å…ˆå­¦ä¹ ã€‚&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;â€ƒâ€ƒ&lt;strong&gt;è¿›é˜¶å­¦ä¹ æŽ¨è&lt;/strong&gt; ï¼šå¦‚æžœæ‚¨åœ¨å­¦ä¹ å®Œæœ¬é¡¹ç›®åŽï¼Œå¸Œæœ›æ›´æ·±å…¥åœ°ç†è§£å¤§è¯­è¨€æ¨¡åž‹çš„æ ¸å¿ƒåŽŸç†ï¼Œå¹¶æ¸´æœ›äº²æ‰‹ä»Žé›¶å¼€å§‹è®­ç»ƒå±žäºŽè‡ªå·±çš„å¤§æ¨¡åž‹ï¼Œæˆ‘ä»¬å¼ºçƒˆæŽ¨èå…³æ³¨ Datawhale çš„å¦ä¸€ä¸ªå¼€æºé¡¹ç›®â€”â€” &lt;a href=&quot;https://github.com/datawhalechina/happy-llm&quot;&gt;Happy-LLM ä»Žé›¶å¼€å§‹çš„å¤§è¯­è¨€æ¨¡åž‹åŽŸç†ä¸Žå®žè·µæ•™ç¨‹&lt;/a&gt; ã€‚è¯¥é¡¹ç›®å°†å¸¦æ‚¨æ·±å…¥æŽ¢ç´¢å¤§æ¨¡åž‹çš„åº•å±‚æœºåˆ¶ï¼ŒæŽŒæ¡å®Œæ•´çš„è®­ç»ƒæµç¨‹ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨ï¼šå¦‚æžœæœ‰åŒå­¦å¸Œæœ›äº†è§£å¤§æ¨¡åž‹çš„æ¨¡åž‹æž„æˆï¼Œä»¥åŠä»Žé›¶æ‰‹å†™RAGã€Agentå’ŒEvalç­‰ä»»åŠ¡ï¼Œå¯ä»¥å­¦ä¹ Datawhaleçš„å¦ä¸€ä¸ªé¡¹ç›®&lt;a href=&quot;https://github.com/datawhalechina/tiny-universe&quot;&gt;Tiny-Universe&lt;/a&gt;ï¼Œå¤§æ¨¡åž‹æ˜¯å½“ä¸‹æ·±åº¦å­¦ä¹ é¢†åŸŸçš„çƒ­ç‚¹ï¼Œä½†çŽ°æœ‰çš„å¤§éƒ¨åˆ†å¤§æ¨¡åž‹æ•™ç¨‹åªåœ¨äºŽæ•™ç»™å¤§å®¶å¦‚ä½•è°ƒç”¨apiå®Œæˆå¤§æ¨¡åž‹çš„åº”ç”¨ï¼Œè€Œå¾ˆå°‘æœ‰äººèƒ½å¤Ÿä»ŽåŽŸç†å±‚é¢è®²æ¸…æ¥šæ¨¡åž‹ç»“æž„ã€RAGã€Agent ä»¥åŠ Evalã€‚æ‰€ä»¥è¯¥ä»“åº“ä¼šæä¾›å…¨éƒ¨æ‰‹å†™ï¼Œä¸é‡‡ç”¨è°ƒç”¨apiçš„å½¢å¼ï¼Œå®Œæˆå¤§æ¨¡åž‹çš„ RAG ã€ Agent ã€Eval ä»»åŠ¡ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨ï¼šè€ƒè™‘åˆ°æœ‰åŒå­¦å¸Œæœ›åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œå¸Œæœ›å­¦ä¹ å¤§æ¨¡åž‹çš„ç†è®ºéƒ¨åˆ†ï¼Œå¦‚æžœæƒ³è¦è¿›ä¸€æ­¥æ·±å…¥å­¦ä¹  LLM çš„ç†è®ºåŸºç¡€ï¼Œå¹¶åœ¨ç†è®ºçš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥è®¤è¯†ã€åº”ç”¨ LLMï¼Œå¯ä»¥å‚è€ƒ Datawhale çš„ &lt;a href=&quot;https://github.com/datawhalechina/so-large-lm.git&quot;&gt;so-large-llm&lt;/a&gt;è¯¾ç¨‹ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨ï¼šå¦‚æžœæœ‰åŒå­¦åœ¨å­¦ä¹ æœ¬è¯¾ç¨‹ä¹‹åŽï¼Œæƒ³è¦è‡ªå·±åŠ¨æ‰‹å¼€å‘å¤§æ¨¡åž‹åº”ç”¨ã€‚åŒå­¦ä»¬å¯ä»¥å‚è€ƒ Datawhale çš„ &lt;a href=&quot;https://github.com/datawhalechina/llm-universe&quot;&gt;åŠ¨æ‰‹å­¦å¤§æ¨¡åž‹åº”ç”¨å¼€å‘&lt;/a&gt; è¯¾ç¨‹ï¼Œè¯¥é¡¹ç›®æ˜¯ä¸€ä¸ªé¢å‘å°ç™½å¼€å‘è€…çš„å¤§æ¨¡åž‹åº”ç”¨å¼€å‘æ•™ç¨‹ï¼Œæ—¨åœ¨åŸºäºŽé˜¿é‡Œäº‘æœåŠ¡å™¨ï¼Œç»“åˆä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹é¡¹ç›®ï¼Œå‘åŒå­¦ä»¬å®Œæ•´çš„å‘ˆçŽ°å¤§æ¨¡åž‹åº”ç”¨å¼€å‘æµç¨‹ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;é¡¹ç›®æ„ä¹‰&lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒä»€ä¹ˆæ˜¯å¤§æ¨¡åž‹ï¼Ÿ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;å¤§æ¨¡åž‹ï¼ˆLLMï¼‰ç‹­ä¹‰ä¸ŠæŒ‡åŸºäºŽæ·±åº¦å­¦ä¹ ç®—æ³•è¿›è¡Œè®­ç»ƒçš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ¨¡åž‹ï¼Œä¸»è¦åº”ç”¨äºŽè‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆç­‰é¢†åŸŸï¼Œå¹¿ä¹‰ä¸Šè¿˜åŒ…æ‹¬æœºå™¨è§†è§‰ï¼ˆCVï¼‰å¤§æ¨¡åž‹ã€å¤šæ¨¡æ€å¤§æ¨¡åž‹å’Œç§‘å­¦è®¡ç®—å¤§æ¨¡åž‹ç­‰ã€‚&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;â€ƒâ€ƒç™¾æ¨¡å¤§æˆ˜æ­£å€¼ç«çƒ­ï¼Œå¼€æº LLM å±‚å‡ºä¸ç©·ã€‚å¦‚ä»Šå›½å†…å¤–å·²ç»æ¶ŒçŽ°äº†ä¼—å¤šä¼˜ç§€å¼€æº LLMï¼Œå›½å¤–å¦‚ LLaMAã€Alpacaï¼Œå›½å†…å¦‚ ChatGLMã€BaiChuanã€InternLMï¼ˆä¹¦ç”ŸÂ·æµ¦è¯­ï¼‰ç­‰ã€‚å¼€æº LLM æ”¯æŒç”¨æˆ·æœ¬åœ°éƒ¨ç½²ã€ç§åŸŸå¾®è°ƒï¼Œæ¯ä¸€ä¸ªäººéƒ½å¯ä»¥åœ¨å¼€æº LLM çš„åŸºç¡€ä¸Šæ‰“é€ ä¸“å±žäºŽè‡ªå·±çš„ç‹¬ç‰¹å¤§æ¨¡åž‹ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒç„¶è€Œï¼Œå½“å‰æ™®é€šå­¦ç”Ÿå’Œç”¨æˆ·æƒ³è¦ä½¿ç”¨è¿™äº›å¤§æ¨¡åž‹ï¼Œéœ€è¦å…·å¤‡ä¸€å®šçš„æŠ€æœ¯èƒ½åŠ›ï¼Œæ‰èƒ½å®Œæˆæ¨¡åž‹çš„éƒ¨ç½²å’Œä½¿ç”¨ã€‚å¯¹äºŽå±‚å‡ºä¸ç©·åˆå„æœ‰ç‰¹è‰²çš„å¼€æº LLMï¼Œæƒ³è¦å¿«é€ŸæŽŒæ¡ä¸€ä¸ªå¼€æº LLM çš„åº”ç”¨æ–¹æ³•ï¼Œæ˜¯ä¸€é¡¹æ¯”è¾ƒæœ‰æŒ‘æˆ˜çš„ä»»åŠ¡ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒæœ¬é¡¹ç›®æ—¨åœ¨é¦–å…ˆåŸºäºŽæ ¸å¿ƒè´¡çŒ®è€…çš„ç»éªŒï¼Œå®žçŽ°å›½å†…å¤–ä¸»æµå¼€æº LLM çš„éƒ¨ç½²ã€ä½¿ç”¨ä¸Žå¾®è°ƒæ•™ç¨‹ï¼›åœ¨å®žçŽ°ä¸»æµ LLM çš„ç›¸å…³éƒ¨åˆ†ä¹‹åŽï¼Œæˆ‘ä»¬å¸Œæœ›å……åˆ†èšé›†å…±åˆ›è€…ï¼Œä¸€èµ·ä¸°å¯Œè¿™ä¸ªå¼€æº LLM çš„ä¸–ç•Œï¼Œæ‰“é€ æ›´å¤šã€æ›´å…¨é¢ç‰¹è‰² LLM çš„æ•™ç¨‹ã€‚æ˜Ÿç«ç‚¹ç‚¹ï¼Œæ±‡èšæˆæµ·ã€‚&lt;/p&gt; 
&lt;p&gt;â€ƒâ€ƒ&lt;em&gt;&lt;strong&gt;æˆ‘ä»¬å¸Œæœ›æˆä¸º LLM ä¸Žæ™®ç½—å¤§ä¼—çš„é˜¶æ¢¯ï¼Œä»¥è‡ªç”±ã€å¹³ç­‰çš„å¼€æºç²¾ç¥žï¼Œæ‹¥æŠ±æ›´æ¢å¼˜è€Œè¾½é˜”çš„ LLM ä¸–ç•Œã€‚&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;é¡¹ç›®å—ä¼—&lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒæœ¬é¡¹ç›®é€‚åˆä»¥ä¸‹å­¦ä¹ è€…ï¼š&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;æƒ³è¦ä½¿ç”¨æˆ–ä½“éªŒ LLMï¼Œä½†æ— æ¡ä»¶èŽ·å¾—æˆ–ä½¿ç”¨ç›¸å…³ APIï¼›&lt;/li&gt; 
 &lt;li&gt;å¸Œæœ›é•¿æœŸã€ä½Žæˆæœ¬ã€å¤§é‡åº”ç”¨ LLMï¼›&lt;/li&gt; 
 &lt;li&gt;å¯¹å¼€æº LLM æ„Ÿå…´è¶£ï¼Œæƒ³è¦äº²è‡ªä¸Šæ‰‹å¼€æº LLMï¼›&lt;/li&gt; 
 &lt;li&gt;NLP åœ¨å­¦ï¼Œå¸Œæœ›è¿›ä¸€æ­¥å­¦ä¹  LLMï¼›&lt;/li&gt; 
 &lt;li&gt;å¸Œæœ›ç»“åˆå¼€æº LLMï¼Œæ‰“é€ é¢†åŸŸç‰¹è‰²çš„ç§åŸŸ LLMï¼›&lt;/li&gt; 
 &lt;li&gt;ä»¥åŠæœ€å¹¿å¤§ã€æœ€æ™®é€šçš„å­¦ç”Ÿç¾¤ä½“ã€‚&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;é¡¹ç›®è§„åˆ’åŠè¿›å±•&lt;/h2&gt; 
&lt;p&gt;â€ƒâ€ƒ æœ¬é¡¹ç›®æ‹Ÿå›´ç»•å¼€æº LLM åº”ç”¨å…¨æµç¨‹ç»„ç»‡ï¼ŒåŒ…æ‹¬çŽ¯å¢ƒé…ç½®åŠä½¿ç”¨ã€éƒ¨ç½²åº”ç”¨ã€å¾®è°ƒç­‰ï¼Œæ¯ä¸ªéƒ¨åˆ†è¦†ç›–ä¸»æµåŠç‰¹ç‚¹å¼€æº LLMï¼š&lt;/p&gt; 
&lt;h3&gt;Example ç³»åˆ—&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/Chat-%E5%AC%9B%E5%AC%9B/readme.md&quot;&gt;Chat-å¬›å¬›&lt;/a&gt;ï¼š Chat-ç”„å¬›æ˜¯åˆ©ç”¨ã€Šç”„å¬›ä¼ ã€‹å‰§æœ¬ä¸­æ‰€æœ‰å…³äºŽç”„å¬›çš„å°è¯å’Œè¯­å¥ï¼ŒåŸºäºŽLLMè¿›è¡ŒLoRAå¾®è°ƒå¾—åˆ°çš„æ¨¡ä»¿ç”„å¬›è¯­æ°”çš„èŠå¤©è¯­è¨€æ¨¡åž‹ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/Tianji-%E5%A4%A9%E6%9C%BA/readme.md&quot;&gt;Tianji-å¤©æœº&lt;/a&gt;ï¼šå¤©æœºæ˜¯ä¸€æ¬¾åŸºäºŽäººæƒ…ä¸–æ•…ç¤¾äº¤åœºæ™¯ï¼Œæ¶µç›–æç¤ºè¯å·¥ç¨‹ ã€æ™ºèƒ½ä½“åˆ¶ä½œã€ æ•°æ®èŽ·å–ä¸Žæ¨¡åž‹å¾®è°ƒã€RAG æ•°æ®æ¸…æ´—ä¸Žä½¿ç”¨ç­‰å…¨æµç¨‹çš„å¤§è¯­è¨€æ¨¡åž‹ç³»ç»Ÿåº”ç”¨æ•™ç¨‹ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/AMchat-%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/readme.md&quot;&gt;AMChat&lt;/a&gt;: AM (Advanced Mathematics) chat æ˜¯ä¸€ä¸ªé›†æˆäº†æ•°å­¦çŸ¥è¯†å’Œé«˜ç­‰æ•°å­¦ä¹ é¢˜åŠå…¶è§£ç­”çš„å¤§è¯­è¨€æ¨¡åž‹ã€‚è¯¥æ¨¡åž‹ä½¿ç”¨ Math å’Œé«˜ç­‰æ•°å­¦ä¹ é¢˜åŠå…¶è§£æžèžåˆçš„æ•°æ®é›†ï¼ŒåŸºäºŽ InternLM2-Math-7B æ¨¡åž‹ï¼Œé€šè¿‡ xtuner å¾®è°ƒï¼Œä¸“é—¨è®¾è®¡ç”¨äºŽè§£ç­”é«˜ç­‰æ•°å­¦é—®é¢˜ã€‚&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/%E6%95%B0%E5%AD%97%E7%94%9F%E5%91%BD/readme.md&quot;&gt;æ•°å­—ç”Ÿå‘½&lt;/a&gt;: æœ¬é¡¹ç›®å°†ä»¥æˆ‘ä¸ºåŽŸåž‹ï¼Œåˆ©ç”¨ç‰¹åˆ¶çš„æ•°æ®é›†å¯¹å¤§è¯­è¨€æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼Œè‡´åŠ›äºŽåˆ›é€ ä¸€ä¸ªèƒ½å¤ŸçœŸæ­£åæ˜ æˆ‘çš„ä¸ªæ€§ç‰¹å¾çš„AIæ•°å­—äººâ€”â€”åŒ…æ‹¬ä½†ä¸é™äºŽæˆ‘çš„è¯­æ°”ã€è¡¨è¾¾æ–¹å¼å’Œæ€ç»´æ¨¡å¼ç­‰ç­‰ï¼Œå› æ­¤æ— è®ºæ˜¯æ—¥å¸¸èŠå¤©è¿˜æ˜¯åˆ†äº«å¿ƒæƒ…ï¼Œå®ƒéƒ½ä»¥ä¸€ç§æ—¢ç†Ÿæ‚‰åˆèˆ’é€‚çš„æ–¹å¼äº¤æµï¼Œä»¿ä½›æˆ‘åœ¨ä»–ä»¬èº«è¾¹ä¸€æ ·ã€‚æ•´ä¸ªæµç¨‹æ˜¯å¯è¿ç§»å¤åˆ¶çš„ï¼Œäº®ç‚¹æ˜¯æ•°æ®é›†çš„åˆ¶ä½œã€‚&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;å·²æ”¯æŒæ¨¡åž‹&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/openai/gpt-oss-20b&quot;&gt;gpt-oss-20b&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; gpt-oss-20b vllm éƒ¨ç½²è°ƒç”¨&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; gpt-oss-20b EvalScope å¹¶å‘è¯„æµ‹&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; gpt-oss-20b lmstudio æœ¬åœ°éƒ¨ç½²è°ƒç”¨&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; gpt-oss-20b Lora å¾®è°ƒåŠ SwanLab å¯è§†åŒ–è®°å½•&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; gpt-oss-20b DPO å¾®è°ƒåŠ SwanLab å¯è§†åŒ–è®°å½•&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zai-org/GLM-4.1V-Thinking&quot;&gt;GLM-4.1-Thinking&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/01-GLM-4%201V-Thinking%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4.1V-Thinking vLLM éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æž—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/02-GLM-4%201V-Thinking%20Gradio%E9%83%A8%E7%BD%B2.md&quot;&gt;GLM-4.1V-Thinking Gradioéƒ¨ç½²&lt;/a&gt; @æž—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/03-GLM-4%201V-Thinking%20LoRA%20%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;GLM-4.1V-Thinking Lora å¾®è°ƒåŠ SwanLab å¯è§†åŒ–è®°å½•&lt;/a&gt; @æž—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/GLM4.1V-Thinking-lora&quot;&gt;GLM-4.1V-Thinking Docker é•œåƒ&lt;/a&gt; @æž—æ’å®‡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zai-org/GLM-4.5&quot;&gt;GLM-4.5-Air&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/01-GLM-4.5-Air-vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4.5-Air vLLM éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/02-GLM-4.5-Air%20EvalScope%20%E5%B9%B6%E5%8F%91%E6%B5%8B%E8%AF%95.md&quot;&gt;GLM-4.5-Air EvalScope æ™ºå•†æƒ…å•† &amp;amp;&amp;amp; å¹¶å‘è¯„æµ‹&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/03-GLM-4.5-Air-Lora%20%E5%8F%8A%20Swanlab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4.5-Air Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.compshare.cn/images/lUQhKDCeCdZW?referral_code=ELukJdQS3vvCwYIfgsQf2C&amp;amp;ytag=GPU_yy_github_selfllm&quot;&gt;GLM-4.5-Air Ucloud Docker é•œåƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/baidu/ERNIE-4.5-0.3B-PT&quot;&gt;ERNIE-4.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ERNIE-4.5/01-ERNIE-4.5-0.3B-PT%20Lora%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;ERNIE-4.5-0.3B-PT Lora å¾®è°ƒåŠ SwanLab å¯è§†åŒ–è®°å½•&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/ERNIE-4.5-lora&quot;&gt;ERNIE-4.5-0.3B-PT Lora Docker é•œåƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Tencent-Hunyuan/Hunyuan-A13B&quot;&gt;Hunyuan-A13B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/01-Hunyuan-A13B-Instruct%20%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20Blog.md&quot;&gt;Hunyuan-A13B-Instruct æ¨¡åž‹æž¶æž„è§£æž Blog&lt;/a&gt; @å“å ‚è¶Š&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/03-Hunyuan-A13B-Instruct-SGLang%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Hunyuan-A13B-Instruct SGLang éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @fancy&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/05-Hunyuan-A13B-Instruct-LoRA%E5%8F%8ASwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Hunyuan-A13B-Instruct Lora SwanLab å¯è§†åŒ–å¾®è°ƒ&lt;/a&gt; @è°¢å¥½å†‰&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Hunyuan-A13B-Instruct-lora&quot;&gt;Hunyuan-A13B-Instruct Lora Docker é•œåƒ&lt;/a&gt; @è°¢å¥½å†‰&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3&quot;&gt;Qwen3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/01-Qwen3-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90-Blog.md&quot;&gt;Qwen3 æ¨¡åž‹ç»“æž„è§£æž Blog&lt;/a&gt; @çŽ‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/02-Qwen3-8B-vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen3-8B vllm éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æŽå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/03-Qwen3-7B-Instruct%20Windows%20LMStudio%20%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen3-8B Windows LMStudio éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @çŽ‹ç† æ˜Ž&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/04-Qwen3-8B%20EvalScope%E6%99%BA%E5%95%86%E6%83%85%E5%95%86%E8%AF%84%E6%B5%8B.md&quot;&gt;Qwen3-8B Evalscope æ™ºå•†æƒ…å•†è¯„æµ‹&lt;/a&gt; @æŽå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/05-Qwen3-8B-LoRA%E5%8F%8ASwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-8B Lora å¾®è°ƒåŠSwanLab å¯è§†åŒ–è®°å½•&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/06-Qwen3-30B-A3B%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-30B-A3B å¾®è°ƒåŠSwanLab å¯è§†åŒ–è®°å½•&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/07-Qwen3-Think-%E8%A7%A3%E5%AF%86-Blog.md&quot;&gt;Qwen3 Think è§£å¯† Blog&lt;/a&gt; @æ¨Šå¥‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Qwen3&quot;&gt;Qwen3-8B Docker é•œåƒ&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models//Qwen3/08-Qwen3_0_6B%E7%9A%84%E5%B0%8F%E6%A8%A1%E5%9E%8B%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8.md&quot;&gt;Qwen3-0.6B çš„å°æ¨¡åž‹æœ‰ä»€ä¹ˆç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/09-Qwen3-1.7B-%E5%8C%BB%E5%AD%A6%E6%8E%A8%E7%90%86%E5%BC%8F%E5%AF%B9%E8%AF%9D%E5%BE%AE%E8%B0%83%20%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-1.7B åŒ»å­¦æŽ¨ç†å¼å¯¹è¯å¾®è°ƒ åŠ SwanLab å¯è§†åŒ–è®°å½•&lt;/a&gt; @æž—æ³½æ¯…&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/10-Qwen3-8B%20GRPO%E5%BE%AE%E8%B0%83%E5%8F%8A%E9%80%9A%E8%BF%87swanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;Qwen3-8B GRPOå¾®è°ƒåŠé€šè¿‡swanlabå¯è§†åŒ–&lt;/a&gt; @éƒ­å®£ä¼¯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/MoonshotAI/Kimi-VL&quot;&gt;Kimi&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Kimi-VL/02-Kimi-VL-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB.md&quot;&gt;Kimi-VL-A3B æŠ€æœ¯æŠ¥å‘Šè§£è¯»&lt;/a&gt; @çŽ‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Kimi-VL/01-Kimi-VL-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B.md&quot;&gt;Kimi-VL-A3B-Thinking WebDemo éƒ¨ç½²ï¼ˆç½‘é¡µå¯¹è¯åŠ©æ‰‹ï¼‰&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;&gt;Llama4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama4/01-Llama4-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B/01-Llama4-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B.md&quot;&gt;Llama4 å¯¹è¯åŠ©æ‰‹&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/manycore-research/SpatialLM&quot;&gt;SpatialLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/SpatialLM/readme.md&quot;&gt;SpatialLM 3Dç‚¹äº‘ç†è§£ä¸Žç›®æ ‡æ£€æµ‹æ¨¡åž‹éƒ¨ç½²&lt;/a&gt; @çŽ‹æ³½å®‡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/tencent/Hunyuan3D-2&quot;&gt;Hunyuan3D-2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/01-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2.md&quot;&gt;Hunyuan3D-2 ç³»åˆ—æ¨¡åž‹éƒ¨ç½²&lt;/a&gt; @æž—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/02-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8.md&quot;&gt;Hunyuan3D-2 ç³»åˆ—æ¨¡åž‹ä»£ç è°ƒç”¨&lt;/a&gt; @æž—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/03-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8BGradio%E9%83%A8%E7%BD%B2.md&quot;&gt;Hunyuan3D-2 ç³»åˆ—æ¨¡åž‹Gradioéƒ¨ç½²&lt;/a&gt; @æž—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/04-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8BAPI%20Server.md&quot;&gt;Hunyuan3D-2 ç³»åˆ—æ¨¡åž‹API Server&lt;/a&gt; @æž—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Hunyuan3D-2&quot;&gt;Hunyuan3D-2 Docker é•œåƒ&lt;/a&gt; @æž—æ’å®‡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-3-4b-it&quot;&gt;Gemma3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/01-gemma-3-4b-it%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gemma-3-4b-it FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æœæ£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/03-gemma-3-4b-it-ollama%20+%20open-webui%E9%83%A8%E7%BD%B2.md&quot;&gt;gemma-3-4b-it ollama + open-webuiéƒ¨ç½²&lt;/a&gt; @å­™è¶…&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/04-Gemma3-4b%20%20evalscope%E6%99%BA%E5%95%86%E6%83%85%E5%95%86%E8%AF%84%E6%B5%8B.md&quot;&gt;gemma-3-4b-it evalscope æ™ºå•†æƒ…å•†è¯„æµ‹&lt;/a&gt; @å¼ é¾™æ–&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/05-gemma-3-4b-it%20LoRA.md&quot;&gt;gemma-3-4b-it Lora å¾®è°ƒ&lt;/a&gt; @èžéº¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/self-llm-gemma3&quot;&gt;gemma-3-4b-it Docker é•œåƒ&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/6-gemma3-4B-itGRPO%E5%BE%AE%E8%B0%83%E5%8F%8A%E9%80%9A%E8%BF%87swanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;gemma-3-4b-it GRPOå¾®è°ƒåŠé€šè¿‡swanlabå¯è§†åŒ–&lt;/a&gt; @éƒ­å®£ä¼¯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;&gt;DeepSeek-R1-Distill&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/01-DeepSeek-R1-Distill-Qwen-7B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éª†ç§€éŸ¬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/02-DeepSeek-R1-Distill-Qwen-7B%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B Langchain æŽ¥å…¥&lt;/a&gt; @éª†ç§€éŸ¬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/03-DeepSeek-R1-Distill-Qwen-7B%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B WebDemo éƒ¨ç½²&lt;/a&gt; @éª†ç§€éŸ¬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/04-DeepSeek-R1-Distill-Qwen-7B%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B vLLM éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éª†ç§€éŸ¬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/05-DeepSeek-R1-0528-Qwen3-8B-GRPO%E5%8F%8Aswanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;DeepSeek-R1-0528-Qwen3-8B-GRPOåŠswanlabå¯è§†åŒ–&lt;/a&gt; @éƒ­å®£ä¼¯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM-o&quot;&gt;MiniCPM-o-2_6&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/01MiniCPM-o%202%206%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8%20.md&quot;&gt;minicpm-o-2.6 FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æž—æ’å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/02minicpm-o-2.6WebDemo_streamlit.py&quot;&gt;minicpm-o-2.6 WebDemo éƒ¨ç½²&lt;/a&gt; @ç¨‹å®&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/03-MiniCPM-o-2.6%20%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%AD%E9%9F%B3%E8%83%BD%E5%8A%9B.md&quot;&gt;minicpm-o-2.6 å¤šæ¨¡æ€è¯­éŸ³èƒ½åŠ›&lt;/a&gt; @é‚“æºä¿Š&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/04-MiniCPM-0-2.6%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;minicpm-o-2.6 å¯è§†åŒ– LaTeX_OCR Lora å¾®è°ƒ&lt;/a&gt; @æž—æ³½æ¯…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM&quot;&gt;InternLM3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/01-InternLM3-8B-Instruct%20FastAPI.md&quot;&gt;internlm3-8b-instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @è‹å‘æ ‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/02-internlm3-8b-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;internlm3-8b-instruct LangchianæŽ¥å…¥&lt;/a&gt; @èµµæ–‡æº&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/03-InternLM3-8B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;internlm3-8b-instruct WebDemo éƒ¨ç½²&lt;/a&gt; @çŽ‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/04-InternLM3-8B-Instruct%20LoRA.md&quot;&gt;internlm3-8b-instruct Lora å¾®è°ƒ&lt;/a&gt; @ç¨‹å®&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/05-internlm3-8b-instruct%20%E4%B8%8Eo1%20.md&quot;&gt;internlm3-8b-instruct o1-likeæŽ¨ç†é“¾å®žçŽ°&lt;/a&gt; @é™ˆç¿&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/microsoft/phi-4&quot;&gt;phi4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/01-Phi-4%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;phi4 FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æœæ£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/02-Phi-4-Langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;phi4 langchain æŽ¥å…¥&lt;/a&gt; @å°ç½—&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/03-Phi-4%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;phi4 WebDemo éƒ¨ç½²&lt;/a&gt; @æœæ£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/04-Phi-4-Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;phi4 Lora å¾®è°ƒ&lt;/a&gt; @éƒ‘è¿œå©§&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/05-Phi-4-Lora%20%E5%BE%AE%E8%B0%83%20%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB.md&quot;&gt;phi4 Lora å¾®è°ƒ NERä»»åŠ¡ SwanLab å¯è§†åŒ–è®°å½•ç‰ˆ&lt;/a&gt; @æž—æ³½æ¯…&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/06-Phi-4-GRPO%E5%8F%8Aswanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;phi4 GRPOå¾®è°ƒåŠé€šè¿‡swanlabå¯è§†åŒ–&lt;/a&gt; @éƒ­å®£ä¼¯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Coder&quot;&gt;Qwen2.5-Coder&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/01-Qwen2.5-Coder-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-Coder-7B-Instruct FastApiéƒ¨ç½²è°ƒç”¨&lt;/a&gt; @èµµæ–‡æº&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/02-Qwen2.5-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2.5-Coder-7B-Instruct LangchianæŽ¥å…¥&lt;/a&gt; @æ¨æ™¨æ—­&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/03-Qwen2.5-Coder-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2.5-Coder-7B-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @çŽ‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/04-Qwen2.5-Coder-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-Coder-7B-Instruct vLLM éƒ¨ç½²&lt;/a&gt; @çŽ‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/Qwen2.5-Coder-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2.5-Coder-7B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @èžéº¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/05-Qwen2.5-Coder-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2.5-Coder-7B-Instruct Lora å¾®è°ƒ SwanLab å¯è§†åŒ–è®°å½•ç‰ˆ&lt;/a&gt; @æ¨å“&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2-VL&quot;&gt;Qwen2-vl&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/01-Qwen2-VL-2B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-vl-2B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/02-Qwen2-VL-2B-Instruct%20Web%20Demo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2-vl-2B WebDemo éƒ¨ç½²&lt;/a&gt; @èµµä¼Ÿ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/03-Qwen2-VL-2B-Instruct%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-vl-2B vLLM éƒ¨ç½²&lt;/a&gt; @èžéº¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/04-Qwen2-VL-2B%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2-vl-2B Lora å¾®è°ƒ&lt;/a&gt; @æŽæŸ¯è¾°&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/05-Qwen2-VL-2B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2-vl-2B Lora å¾®è°ƒ SwanLab å¯è§†åŒ–è®°å½•ç‰ˆ&lt;/a&gt; @æž—æ³½æ¯…&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/06-Qwen2-VL-2B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%E6%A1%88%E4%BE%8B%20-%20LaTexOCR.md&quot;&gt;Qwen2-vl-2B Lora å¾®è°ƒæ¡ˆä¾‹ - LaTexOCR&lt;/a&gt; @æž—æ³½æ¯…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5&quot;&gt;Qwen2.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/01-Qwen2.5-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-7B-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å¨„å¤©å¥¥&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/02-Qwen2.5-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2.5-7B-Instruct langchain æŽ¥å…¥&lt;/a&gt; @å¨„å¤©å¥¥&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/03-Qwen2.5-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-7B-Instruct vLLM éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/04-Qwen2_5-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2.5-7B-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/05-Qwen2.5-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2.5-7B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @å·¦æ˜¥ç”Ÿ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/06-Qwen2.5-7B-Instruct%20o1-like%20%E6%8E%A8%E7%90%86%E9%93%BE%E5%AE%9E%E7%8E%B0.md&quot;&gt;Qwen2.5-7B-Instruct o1-like æŽ¨ç†é“¾å®žçŽ°&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/07-Qwen2.5-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2.5-7B-Instruct Lora å¾®è°ƒ SwanLab å¯è§†åŒ–è®°å½•ç‰ˆ&lt;/a&gt; @æž—æ³½æ¯…&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://machinelearning.apple.com/research/openelm&quot;&gt;Apple OpenELM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/OpenELM/01-OpenELM-3B-Instruct%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;OpenELM-3B-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @çŽ‹æ³½å®‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/OpenELM/02-OpenELM-3B-Instruct%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;OpenELM-3B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @çŽ‹æ³½å®‡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct&quot;&gt;Llama3_1-8B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/01-Llama3_1-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Llama3_1-8B-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/02-Llama3_1-8B-Instruct%20langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;Llama3_1-8B-Instruct langchain æŽ¥å…¥&lt;/a&gt; @å¼ æ™‹&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/03-Llama3_1-8B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Llama3_1-8B-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @å¼ æ™‹&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/04-Llama3_1-8B--Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Llama3_1-8B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/%E5%8A%A8%E6%89%8B%E8%BD%AC%E6%8D%A2GGUF%E6%A8%A1%E5%9E%8B%E5%B9%B6%E4%BD%BF%E7%94%A8Ollama%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2.md&quot;&gt;åŠ¨æ‰‹è½¬æ¢GGUFæ¨¡åž‹å¹¶ä½¿ç”¨Ollamaæœ¬åœ°éƒ¨ç½²&lt;/a&gt; @Gaoboy&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-2-9b-it&quot;&gt;Gemma-2-9b-it&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/01-Gemma-2-9b-it%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Gemma-2-9b-it FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/02-Gemma-2-9b-it%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Gemma-2-9b-it langchain æŽ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/03-Gemma-2-9b-it%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;Gemma-2-9b-it WebDemo éƒ¨ç½²&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/04-Gemma-2-9b-it%20peft%20lora%E5%BE%AE%E8%B0%83.md&quot;&gt;Gemma-2-9b-it Peft Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/IEIT-Yuan/Yuan-2.0&quot;&gt;Yuan2.0&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/01-Yuan2.0-2B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-2B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/02-Yuan2.0-2B%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Yuan2.0-2B Langchain æŽ¥å…¥&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/03-Yuan2.0-2B%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Yuan2.0-2B WebDemoéƒ¨ç½²&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/04-Yuan2.0-2B%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-2B vLLMéƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/05-Yuan2.0-2B%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;Yuan2.0-2B Loraå¾®è°ƒ&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/IEIT-Yuan/Yuan2.0-M32&quot;&gt;Yuan2.0-M32&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/01-Yuan2.0-M32%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-M32 FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/02-Yuan2.0-M32%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Yuan2.0-M32 Langchain æŽ¥å…¥&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/03-Yuan2.0-M32%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Yuan2.0-M32 WebDemoéƒ¨ç½²&lt;/a&gt; @å¼ å¸†&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-Coder-V2&quot;&gt;DeepSeek-Coder-V2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/01-DeepSeek-Coder-V2-Lite-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/02-DeepSeek-Coder-V2-Lite-Instruct%20%E6%8E%A5%E5%85%A5%20LangChain.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct langchain æŽ¥å…¥&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/03-DeepSeek-Coder-V2-Lite-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/04-DeepSeek-Coder-V2-Lite-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct Lora å¾®è°ƒ&lt;/a&gt; @ä½™æ´‹&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/bilibili/Index-1.9B&quot;&gt;å“”å“©å“”å“© Index-1.9B&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/01-Index-1.9B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Index-1.9B-Chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @é‚“æºä¿Š&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/02-Index-1.9B-Chat%20%E6%8E%A5%E5%85%A5%20LangChain.md&quot;&gt;Index-1.9B-Chat langchain æŽ¥å…¥&lt;/a&gt; @å¼ å‹ä¸œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/03-Index-1.9B-chat%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Index-1.9B-Chat WebDemo éƒ¨ç½²&lt;/a&gt; @ç¨‹å®&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/04-Index-1.9B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Index-1.9B-Chat Lora å¾®è°ƒ&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2&quot;&gt;Qwen2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/01-Qwen2-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-7B-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @åº·å©§æ·‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/02-Qwen2-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2-7B-Instruct langchain æŽ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/03-Qwen2-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2-7B-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @ä¸‰æ°´&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/04-Qwen2-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-7B-Instruct vLLM éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å§œèˆ’å‡¡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/05-Qwen2-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2-7B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @æ•£æ­¥&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/THUDM/GLM-4.git&quot;&gt;GLM-4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/01-GLM-4-9B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4-9B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å¼ å‹ä¸œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/02-GLM-4-9B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;GLM-4-9B-chat langchain æŽ¥å…¥&lt;/a&gt; @è°­é€¸ç‚&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/03-GLM-4-9B-Chat%20WebDemo.md&quot;&gt;GLM-4-9B-chat WebDemo éƒ¨ç½²&lt;/a&gt; @ä½•è‡³è½©&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/04-GLM-4-9B-Chat%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4-9B-chat vLLM éƒ¨ç½²&lt;/a&gt; @çŽ‹ç† æ˜Ž&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/05-GLM-4-9B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4-9B-chat Lora å¾®è°ƒ&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/05-GLM-4-9B-chat-hf%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4-9B-chat-hf Lora å¾®è°ƒ&lt;/a&gt; @ä»˜å¿—è¿œ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen1.5.git&quot;&gt;Qwen 1.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/01-Qwen1.5-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen1.5-7B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @é¢œé‘«&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/02-Qwen1.5-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Qwen1.5-7B-chat langchain æŽ¥å…¥&lt;/a&gt; @é¢œé‘«&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/03-Qwen1.5-7B-Chat%20WebDemo.md&quot;&gt;Qwen1.5-7B-chat WebDemo éƒ¨ç½²&lt;/a&gt; @é¢œé‘«&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/04-Qwen1.5-7B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen1.5-7B-chat Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/05-Qwen1.5-7B-Chat-GPTQ-Int4%20%20WebDemo.md&quot;&gt;Qwen1.5-72B-chat-GPTQ-Int4 éƒ¨ç½²çŽ¯å¢ƒ&lt;/a&gt; @byx020119&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/06-Qwen1.5-MoE-A2.7B.md&quot;&gt;Qwen1.5-MoE-chat Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸æ‚¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/07-Qwen1.5-7B-Chat%20vLLM%20%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen1.5-7B-chat vLLMæŽ¨ç†éƒ¨ç½²&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/08-Qwen1.5-7B-chat%20LoRA%E5%BE%AE%E8%B0%83%E6%8E%A5%E5%85%A5%E5%AE%9E%E9%AA%8C%E7%AE%A1%E7%90%86.md&quot;&gt;Qwen1.5-7B-chat Lora å¾®è°ƒ æŽ¥å…¥SwanLabå®žéªŒç®¡ç†å¹³å°&lt;/a&gt; @é»„æŸç‰¹&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-7b-it&quot;&gt;è°·æ­Œ-Gemma&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/01-Gemma-2B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gemma-2b-it FastApi éƒ¨ç½²è°ƒç”¨ &lt;/a&gt; @ä¸œä¸œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/02-Gemma-2B-Instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;gemma-2b-it langchain æŽ¥å…¥ &lt;/a&gt; @ä¸œä¸œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/03-Gemma-2B-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;gemma-2b-it WebDemo éƒ¨ç½² &lt;/a&gt; @ä¸œä¸œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/04-Gemma-2B-Instruct%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;gemma-2b-it Peft Lora å¾®è°ƒ &lt;/a&gt; @ä¸œä¸œ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct&quot;&gt;phi-3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/01-Phi-3-mini-4k-instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Phi-3-mini-4k-instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éƒ‘çš“æ¡¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/02-Phi-3-mini-4k-instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Phi-3-mini-4k-instruct langchain æŽ¥å…¥&lt;/a&gt; @éƒ‘çš“æ¡¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/03-Phi-3-mini-4k-instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Phi-3-mini-4k-instruct WebDemo éƒ¨ç½²&lt;/a&gt; @ä¸æ‚¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/04-Phi-3-mini-4k-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Phi-3-mini-4k-instruct Lora å¾®è°ƒ&lt;/a&gt; @ä¸æ‚¦&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/thu-coai/CharacterGLM-6B&quot;&gt;CharacterGLM-6B&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/01-CharacterGLM-6B%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;CharacterGLM-6B Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å­™å¥å£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/02-CharacterGLM-6B%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;CharacterGLM-6B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å­™å¥å£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/03-CharacterGLM-6B-chat.md&quot;&gt;CharacterGLM-6B webdemo éƒ¨ç½²&lt;/a&gt; @å­™å¥å£®&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/04-CharacterGLM-6B%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;CharacterGLM-6B Lora å¾®è°ƒ&lt;/a&gt; @å­™å¥å£®&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/meta-llama/llama3.git&quot;&gt;LLaMA3-8B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/01-LLaMA3-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;LLaMA3-8B-Instruct FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/02-LLaMA3-8B-Instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;LLaMA3-8B-Instruct langchain æŽ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/03-LLaMA3-8B-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;LLaMA3-8B-Instruct WebDemo éƒ¨ç½²&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/04-LLaMA3-8B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;LLaMA3-8B-Instruct Lora å¾®è°ƒ&lt;/a&gt; @é«˜ç«‹ä¸š&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://modelscope.cn/models/xverse/XVERSE-7B-Chat/summary&quot;&gt;XVERSE-7B-Chat&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/01-XVERSE-7B-chat%20Transformers%E6%8E%A8%E7%90%86.md&quot;&gt;XVERSE-7B-Chat transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/02-XVERSE-7B-chat%20FastAPI%E9%83%A8%E7%BD%B2.md&quot;&gt;XVERSE-7B-Chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/03-XVERSE-7B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;XVERSE-7B-Chat langchain æŽ¥å…¥&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/04-XVERSE-7B-chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;XVERSE-7B-Chat WebDemo éƒ¨ç½²&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/05-XVERSE-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;XVERSE-7B-Chat Lora å¾®è°ƒ&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenNLPLab/TransnormerLLM.git&quot;&gt;TransNormerLLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/01-TransNormer-7B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;TransNormerLLM-7B-Chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @çŽ‹èŒ‚éœ–&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/02-TransNormer-7B%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;TransNormerLLM-7B-Chat langchain æŽ¥å…¥&lt;/a&gt; @çŽ‹èŒ‚éœ–&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/03-TransNormer-7B%20WebDemo.md&quot;&gt;TransNormerLLM-7B-Chat WebDemo éƒ¨ç½²&lt;/a&gt; @çŽ‹èŒ‚éœ–&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/04-TrasnNormer-7B%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;TransNormerLLM-7B-Chat Lora å¾®è°ƒ&lt;/a&gt; @çŽ‹èŒ‚éœ–&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/vivo-ai-lab/BlueLM.git&quot;&gt;BlueLM Vivo è“å¿ƒå¤§æ¨¡åž‹&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/01-BlueLM-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2.md&quot;&gt;BlueLM-7B-Chat FatApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/02-BlueLM-7B-Chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;BlueLM-7B-Chat langchain æŽ¥å…¥&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/03-BlueLM-7B-Chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;BlueLM-7B-Chat WebDemo éƒ¨ç½²&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/04-BlueLM-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;BlueLM-7B-Chat Lora å¾®è°ƒ&lt;/a&gt; @éƒ­å¿—èˆª&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM&quot;&gt;InternLM2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/01-InternLM2-7B-chat%20FastAPI%E9%83%A8%E7%BD%B2.md&quot;&gt;InternLM2-7B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/02-InternLM2-7B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;InternLM2-7B-chat langchain æŽ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/03-InternLM2-7B-chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;InternLM2-7B-chat WebDemo éƒ¨ç½²&lt;/a&gt; @éƒ‘çš“æ¡¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/04-InternLM2-7B-chat%20Xtuner%20Qlora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;InternLM2-7B-chat Xtuner Qlora å¾®è°ƒ&lt;/a&gt; @éƒ‘çš“æ¡¦&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-LLM&quot;&gt;DeepSeek æ·±åº¦æ±‚ç´¢&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/01-DeepSeek-7B-chat%20FastApi.md&quot;&gt;DeepSeek-7B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/02-DeepSeek-7B-chat%20langchain.md&quot;&gt;DeepSeek-7B-chat langchain æŽ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/03-DeepSeek-7B-chat%20WebDemo.md&quot;&gt;DeepSeek-7B-chat WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/04-DeepSeek-7B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-7B-chat Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/05-DeepSeek-7B-chat%204bits%E9%87%8F%E5%8C%96%20Qlora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-7B-chat 4bitsé‡åŒ– Qlora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/06-DeepSeek-MoE-16b-chat%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-MoE-16b-chat Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/06-DeepSeek-MoE-16b-chat%20FastApi.md&quot;&gt;DeepSeek-MoE-16b-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/07-deepseek_fine_tune.ipynb&quot;&gt;DeepSeek-coder-6.7b finetune colab&lt;/a&gt; @Swiftie&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/08-deepseek_web_demo.ipynb&quot;&gt;Deepseek-coder-6.7b webdemo colab&lt;/a&gt; @Swiftie&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM.git&quot;&gt;MiniCPM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20transformers%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;MiniCPM-2B-chat transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;MiniCPM-2B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;MiniCPM-2B-chat langchain æŽ¥å…¥&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;MiniCPM-2B-chat webdemo éƒ¨ç½²&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20Lora%20&amp;amp;&amp;amp;%20Full%20%E5%BE%AE%E8%B0%83.md&quot;&gt;MiniCPM-2B-chat Lora &amp;amp;&amp;amp; Full å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; å®˜æ–¹å‹æƒ…é“¾æŽ¥ï¼š&lt;a href=&quot;https://modelbest.feishu.cn/wiki/D2tFw8Pcsi5CIzkaHNacLK64npg&quot;&gt;é¢å£å°é’¢ç‚®MiniCPMæ•™ç¨‹&lt;/a&gt; @OpenBMB&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; å®˜æ–¹å‹æƒ…é“¾æŽ¥ï¼š&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM-CookBook&quot;&gt;MiniCPM-Cookbook&lt;/a&gt; @OpenBMB&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen-Audio.git&quot;&gt;Qwen-Audio&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen-Audio/01-Qwen-Audio-chat%20FastApi.md&quot;&gt;Qwen-Audio FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @é™ˆæ€å·ž&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen-Audio/02-Qwen-Audio-chat%20WebDemo.md&quot;&gt;Qwen-Audio WebDemo&lt;/a&gt; @é™ˆæ€å·ž&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen.git&quot;&gt;Qwen&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/01-Qwen-7B-Chat%20Transformers%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen-7B-chat Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æŽå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/02-Qwen-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen-7B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æŽå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/03-Qwen-7B-Chat%20WebDemo.md&quot;&gt;Qwen-7B-chat WebDemo&lt;/a&gt; @æŽå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/04-Qwen-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat Lora å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/05-Qwen-7B-Chat%20Ptuning%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat ptuning å¾®è°ƒ&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/06-Qwen-7B-chat%20%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat å…¨é‡å¾®è°ƒ&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/07-Qwen-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Qwen-7B-Chat æŽ¥å…¥langchainæ­å»ºçŸ¥è¯†åº“åŠ©æ‰‹&lt;/a&gt; @æŽå¨‡å¨‡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/08-Qwen-7B-Chat%20Lora%20%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat ä½Žç²¾åº¦è®­ç»ƒ&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/09-Qwen-1_8B-chat%20CPU%20%E9%83%A8%E7%BD%B2%20.md&quot;&gt;Qwen-1_8B-chat CPU éƒ¨ç½²&lt;/a&gt; @æ•£æ­¥&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/01-ai/Yi.git&quot;&gt;Yi é›¶ä¸€ä¸‡ç‰©&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/01-Yi-6B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yi-6B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æŽæŸ¯è¾°&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/02-Yi-6B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Yi-6B-chat langchainæŽ¥å…¥&lt;/a&gt; @æŽæŸ¯è¾°&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/03-Yi-6B-chat%20WebDemo.md&quot;&gt;Yi-6B-chat WebDemo&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/04-Yi-6B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Yi-6B-chat Lora å¾®è°ƒ&lt;/a&gt; @æŽå¨‡å¨‡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.baichuan-ai.com/home&quot;&gt;Baichuan ç™¾å·æ™ºèƒ½&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/BaiChuan/01-Baichuan2-7B-chat%2BFastApi%2B%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Baichuan2-7B-chat FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @æƒ ä½³è±ª&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/02-Baichuan-7B-chat%2BWebDemo.md&quot;&gt;Baichuan2-7B-chat WebDemo&lt;/a&gt; @æƒ ä½³è±ª&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/03-Baichuan2-7B-chat%E6%8E%A5%E5%85%A5LangChain%E6%A1%86%E6%9E%B6.md&quot;&gt;Baichuan2-7B-chat æŽ¥å…¥ LangChain æ¡†æž¶&lt;/a&gt; @æƒ ä½³è±ª&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/04-Baichuan2-7B-chat%2Blora%2B%E5%BE%AE%E8%B0%83.md&quot;&gt;Baichuan2-7B-chat Lora å¾®è°ƒ&lt;/a&gt; @æƒ ä½³è±ª&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM.git&quot;&gt;InternLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/01-InternLM-Chat-7B%20Transformers%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;InternLM-Chat-7B Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @å°ç½—&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/02-internLM-Chat-7B%20FastApi.md&quot;&gt;InternLM-Chat-7B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/03-InternLM-Chat-7B.md&quot;&gt;InternLM-Chat-7B WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/04-Lagent+InternLM-Chat-7B-V1.1.md&quot;&gt;Lagent+InternLM-Chat-7B-V1.1 WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/05-%E6%B5%A6%E8%AF%AD%E7%81%B5%E7%AC%94%E5%9B%BE%E6%96%87%E7%90%86%E8%A7%A3&amp;amp;%E5%88%9B%E4%BD%9C.md&quot;&gt;æµ¦è¯­çµç¬”å›¾æ–‡ç†è§£&amp;amp;åˆ›ä½œ WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/06-InternLM%E6%8E%A5%E5%85%A5LangChain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;InternLM-Chat-7B æŽ¥å…¥ LangChain æ¡†æž¶&lt;/a&gt; @Logan Zou&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://hf-mirror.com/FlagAlpha/Atom-7B-Chat&quot;&gt;Atom (llama2)&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/01-Atom-7B-chat-WebDemo.md&quot;&gt;Atom-7B-chat WebDemo&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/02-Atom-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Atom-7B-chat Lora å¾®è°ƒ&lt;/a&gt; @Logan Zou&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/03-Atom-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Atom-7B-Chat æŽ¥å…¥langchainæ­å»ºçŸ¥è¯†åº“åŠ©æ‰‹&lt;/a&gt; @é™ˆæ€å·ž&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/04-Atom-7B-chat%20%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83.md&quot;&gt;Atom-7B-chat å…¨é‡å¾®è°ƒ&lt;/a&gt; @Logan Zou&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/THUDM/ChatGLM3.git&quot;&gt;ChatGLM3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/01-ChatGLM3-6B%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;ChatGLM3-6B Transformers éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸æ‚¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/02-ChatGLM3-6B%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;ChatGLM3-6B FastApi éƒ¨ç½²è°ƒç”¨&lt;/a&gt; @ä¸æ‚¦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/03-ChatGLM3-6B-chat.md&quot;&gt;ChatGLM3-6B chat WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/04-ChatGLM3-6B-Code-Interpreter.md&quot;&gt;ChatGLM3-6B Code Interpreter WebDemo&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/05-ChatGLM3-6B%E6%8E%A5%E5%85%A5LangChain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;ChatGLM3-6B æŽ¥å…¥ LangChain æ¡†æž¶&lt;/a&gt; @Logan Zou&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/06-ChatGLM3-6B-Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;ChatGLM3-6B Lora å¾®è°ƒ&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;é€šç”¨çŽ¯å¢ƒé…ç½®&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/01-pip%E3%80%81conda%E6%8D%A2%E6%BA%90.md&quot;&gt;pipã€conda æ¢æº&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/02-AutoDL%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3.md&quot;&gt;AutoDL å¼€æ”¾ç«¯å£&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;æ¨¡åž‹ä¸‹è½½&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;hugging face&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;hugging face&lt;/a&gt; é•œåƒä¸‹è½½ @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;modelscope&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;git-lfs&lt;/a&gt; @ä¸è¦è‘±å§œè’œ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;Openxlab&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Issue &amp;amp;&amp;amp; PR&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;Issue æäº¤&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;PR æäº¤&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;forkæ›´æ–°&lt;/a&gt; @è‚–é¸¿å„’&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;è‡´è°¢&lt;/h2&gt; 
&lt;h3&gt;æ ¸å¿ƒè´¡çŒ®è€…&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KMnO4-zx&quot;&gt;å®‹å¿—å­¦(ä¸è¦è‘±å§œè’œ)-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜-ä¸­å›½çŸ¿ä¸šå¤§å­¦(åŒ—äº¬)ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/logan-zou&quot;&gt;é‚¹é›¨è¡¡-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜-å¯¹å¤–ç»æµŽè´¸æ˜“å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Tsumugii24&quot;&gt;å§œèˆ’å‡¡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Hongru0306&quot;&gt;è‚–é¸¿å„’&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜-åŒæµŽå¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/acwwt&quot;&gt;éƒ­å¿—èˆª&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Zeyi-Lin&quot;&gt;æž—æ³½æ¯…&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-SwanLabäº§å“è´Ÿè´£äººï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhangfanTJU&quot;&gt;å¼ å¸†&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/moyitech&quot;&gt;çŽ‹æ³½å®‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¤ªåŽŸç†å·¥å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Aphasia0515&quot;&gt;æŽå¨‡å¨‡&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/0-yy-0&quot;&gt;é«˜ç«‹ä¸š&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-DataWhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dingyue772&quot;&gt;ä¸æ‚¦&lt;/a&gt; ï¼ˆDatawhale-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LINHYYY&quot;&gt;æž—æ’å®‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-æ¸…åŽå¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/L4HeyXiao&quot;&gt;æƒ ä½³è±ª&lt;/a&gt; ï¼ˆDatawhale-å®£ä¼ å¤§ä½¿ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mlw67&quot;&gt;çŽ‹èŒ‚éœ–&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Caleb-Sun-jz&quot;&gt;å­™å¥å£®&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¯¹å¤–ç»æµŽè´¸æ˜“å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LucaChen&quot;&gt;ä¸œä¸œ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-è°·æ­Œå¼€å‘è€…æœºå™¨å­¦ä¹ æŠ€æœ¯ä¸“å®¶ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yeyeyeyeeeee&quot;&gt;èžéº¦&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Kailigithub&quot;&gt;Kailigithub&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/BaiYu96&quot;&gt;éƒ‘çš“æ¡¦&lt;/a&gt; ï¼ˆå†…å®¹åˆ›ä½œè€…ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Joe-2002&quot;&gt;æŽæŸ¯è¾°&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/chg0901&quot;&gt;ç¨‹å®&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæ„å‘æˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anine09&quot;&gt;éª†ç§€éŸ¬&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜-ä¼¼ç„¶å®žéªŒå®¤ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Twosugar666&quot;&gt;éƒ­å®£ä¼¯&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ilovexsir&quot;&gt;è°¢å¥½å†‰&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jjyaoao&quot;&gt;é™ˆæ€å·ž&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sanbuphy&quot;&gt;æ•£æ­¥&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/thomas-yanxin&quot;&gt;é¢œé‘«&lt;/a&gt; ï¼ˆDatawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/study520ai520&quot;&gt;æœæ£®&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜-å—é˜³ç†å·¥å­¦é™¢ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cswangxiaowei&quot;&gt;Swiftie&lt;/a&gt; ï¼ˆå°ç±³NLPç®—æ³•å·¥ç¨‹å¸ˆï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KashiwaByte&quot;&gt;é»„æŸç‰¹&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AXYZdong&quot;&gt;å¼ å‹ä¸œ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/YangYu-NUAA&quot;&gt;ä½™æ´‹&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Jin-Zhang-Yaoguang&quot;&gt;å¼ æ™‹&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lta155&quot;&gt;å¨„å¤©å¥¥&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-ä¸­å›½ç§‘å­¦é™¢å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LinChentang&quot;&gt;å·¦æ˜¥ç”Ÿ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/little1d&quot;&gt;æ¨å“&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lyj11111111&quot;&gt;å°ç½—&lt;/a&gt; ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Kedreamix&quot;&gt;é‚“æºä¿Š&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/XiLinky&quot;&gt;èµµæ–‡æº&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¤ªåŽŸç†å·¥å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/comfzy&quot;&gt;ä»˜å¿—è¿œ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-æµ·å—å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/isaacahahah&quot;&gt;éƒ‘è¿œå©§&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™-ç¦å·žå¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Bald0Wang&quot;&gt;çŽ‹ç† æ˜Ž&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LikeGiver&quot;&gt;è°­é€¸ç‚&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¯¹å¤–ç»æµŽè´¸æ˜“å¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pod2c&quot;&gt;ä½•è‡³è½©&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jodie-kang&quot;&gt;åº·å©§æ·‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sssanssss&quot;&gt;ä¸‰æ°´&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langlibai66&quot;&gt;æ¨æ™¨æ—­&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¤ªåŽŸç†å·¥å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/2710932616&quot;&gt;èµµä¼Ÿ&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gzhuuser&quot;&gt;è‹å‘æ ‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-å¹¿å·žå¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/riannyway&quot;&gt;é™ˆç¿&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-è¥¿äº¤åˆ©ç‰©æµ¦å¤§å­¦-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Feimike09&quot;&gt;å¼ é¾™æ–&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anarchysaiko&quot;&gt;å­™è¶…&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-Datawhaleæˆå‘˜ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fanqiNO1&quot;&gt;æ¨Šå¥‡&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-ä¸Šæµ·äº¤é€šå¤§å­¦ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nusakom&quot;&gt;å“å ‚è¶Š&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fancyboi999&quot;&gt;fancy&lt;/a&gt;ï¼ˆå†…å®¹åˆ›ä½œè€…-é²¸è‹±åŠ©æ•™ï¼‰&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;æ³¨ï¼šæŽ’åæ ¹æ®è´¡çŒ®ç¨‹åº¦æŽ’åº&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;å…¶ä»–&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ç‰¹åˆ«æ„Ÿè°¢&lt;a href=&quot;https://github.com/Sm1les&quot;&gt;@Sm1les&lt;/a&gt;å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸Žæ”¯æŒ&lt;/li&gt; 
 &lt;li&gt;éƒ¨åˆ†loraä»£ç å’Œè®²è§£å‚è€ƒä»“åº“ï¼š&lt;a href=&quot;https://github.com/zyds/transformers-code.git&quot;&gt;https://github.com/zyds/transformers-code.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;å¦‚æžœæœ‰ä»»ä½•æƒ³æ³•å¯ä»¥è”ç³»æˆ‘ä»¬ DataWhale ä¹Ÿæ¬¢è¿Žå¤§å®¶å¤šå¤šæå‡º issue&lt;/li&gt; 
 &lt;li&gt;ç‰¹åˆ«æ„Ÿè°¢ä»¥ä¸‹ä¸ºæ•™ç¨‹åšå‡ºè´¡çŒ®çš„åŒå­¦ï¼&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-top: 30px;&quot;&gt; 
 &lt;a href=&quot;https://github.com/datawhalechina/self-llm/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/self-llm&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Star History&lt;/h3&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-top: 30px;&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/images/star-history-202572.png&quot; /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>anthropics/anthropic-cookbook</title>
      <link>https://github.com/anthropics/anthropic-cookbook</link>
      <description>&lt;p&gt;A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anthropic Cookbook&lt;/h1&gt; 
&lt;p&gt;The Anthropic Cookbook provides code and guides designed to help developers build with Claude, offering copy-able code snippets that you can easily integrate into your own projects.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;To make the most of the examples in this cookbook, you&#39;ll need an Anthropic API key (sign up for free &lt;a href=&quot;https://www.anthropic.com&quot;&gt;here&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;While the code examples are primarily written in Python, the concepts can be adapted to any programming language that supports interaction with the Anthropic API.&lt;/p&gt; 
&lt;p&gt;If you&#39;re new to working with the Anthropic API, we recommend starting with our &lt;a href=&quot;https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals&quot;&gt;Anthropic API Fundamentals course&lt;/a&gt; to get a solid foundation.&lt;/p&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;p&gt;Looking for more resources to enhance your experience with Claude and AI assistants? Check out these helpful links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.anthropic.com/claude/docs/guide-to-anthropics-prompt-engineering-resources&quot;&gt;Anthropic developer documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://support.anthropic.com&quot;&gt;Anthropic support docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.anthropic.com/discord&quot;&gt;Anthropic Discord community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Anthropic Cookbook thrives on the contributions of the developer community. We value your input, whether it&#39;s submitting an idea, fixing a typo, adding a new guide, or improving an existing one. By contributing, you help make this resource even more valuable for everyone.&lt;/p&gt; 
&lt;p&gt;To avoid duplication of efforts, please review the existing issues and pull requests before contributing.&lt;/p&gt; 
&lt;p&gt;If you have ideas for new examples or guides, share them on the &lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/issues&quot;&gt;issues page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of recipes&lt;/h2&gt; 
&lt;h3&gt;Skills&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/skills/classification&quot;&gt;Classification&lt;/a&gt;: Explore techniques for text and data classification using Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/skills/retrieval_augmented_generation&quot;&gt;Retrieval Augmented Generation&lt;/a&gt;: Learn how to enhance Claude&#39;s responses with external knowledge.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/skills/summarization&quot;&gt;Summarization&lt;/a&gt;: Discover techniques for effective text summarization with Claude.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Tool Use and Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/tool_use&quot;&gt;Tool use&lt;/a&gt;: Learn how to integrate Claude with external tools and functions to extend its capabilities. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/tool_use/customer_service_agent.ipynb&quot;&gt;Customer service agent&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/tool_use/calculator_tool.ipynb&quot;&gt;Calculator integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_make_sql_queries.ipynb&quot;&gt;SQL queries&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Third-Party Integrations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/third_party&quot;&gt;Retrieval augmented generation&lt;/a&gt;: Supplement Claude&#39;s knowledge with external data sources. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Pinecone/rag_using_pinecone.ipynb&quot;&gt;Vector databases (Pinecone)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Wikipedia/wikipedia-search-cookbook.ipynb/&quot;&gt;Wikipedia&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/read_web_pages_with_haiku.ipynb&quot;&gt;Web pages&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Brave/web_search_using_brave.ipynb&quot;&gt;Internet search (Brave)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/VoyageAI/how_to_create_embeddings.md&quot;&gt;Embeddings with Voyage AI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multimodal Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/multimodal&quot;&gt;Vision with Claude&lt;/a&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/getting_started_with_vision.ipynb&quot;&gt;Getting started with images&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/best_practices_for_vision.ipynb&quot;&gt;Best practices for vision&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/reading_charts_graphs_powerpoints.ipynb&quot;&gt;Interpreting charts and graphs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/how_to_transcribe_text.ipynb&quot;&gt;Extracting content from forms&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/illustrated_responses.ipynb&quot;&gt;Generate images with Claude&lt;/a&gt;: Use Claude with Stable Diffusion for image generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Techniques&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/using_sub_agents.ipynb&quot;&gt;Sub-agents&lt;/a&gt;: Learn how to use Haiku as a sub-agent in combination with Opus.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/pdf_upload_summarization.ipynb&quot;&gt;Upload PDFs to Claude&lt;/a&gt;: Parse and pass PDFs as text to Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_evals.ipynb&quot;&gt;Automated evaluations&lt;/a&gt;: Use Claude to automate the prompt evaluation process.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_enable_json_mode.ipynb&quot;&gt;Enable JSON mode&lt;/a&gt;: Ensure consistent JSON output from Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_moderation_filter.ipynb&quot;&gt;Create a moderation filter&lt;/a&gt;: Use Claude to create a content moderation filter for your application.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/prompt_caching.ipynb&quot;&gt;Prompt caching&lt;/a&gt;: Learn techniques for efficient prompt caching with Claude.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/aws-samples/anthropic-on-aws&quot;&gt;Anthropic on AWS&lt;/a&gt;: Explore examples and solutions for using Claude on AWS infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/aws-samples/&quot;&gt;AWS Samples&lt;/a&gt;: A collection of code samples from AWS which can be adapted for use with Claude. Note that some samples may require modification to work optimally with Claude.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>greyhatguy007/Machine-Learning-Specialization-Coursera</title>
      <link>https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera</link>
      <description>&lt;p&gt;Contains Solutions and Notes for the Machine Learning Specialization By Stanford University and Deeplearning.ai - Coursera (2022) by Prof. Andrew NG&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Specialization Coursera&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/greyhatguy007/Machine-Learning-Specialization-Coursera/main/resources/title-head.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;Contains Solutions and Notes for the &lt;a href=&quot;https://www.coursera.org/specializations/machine-learning-introduction/?utm_medium=coursera&amp;amp;utm_source=home-page&amp;amp;utm_campaign=mlslaunch2022IN&quot;&gt;Machine Learning Specialization&lt;/a&gt; by Andrew NG on Coursera&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note : If you would like to have a deeper understanding of the concepts by understanding all the math required, have a look at &lt;a href=&quot;https://github.com/greyhatguy007/Mathematics-for-Machine-Learning-and-Data-Science-Specialization-Coursera&quot;&gt;Mathematics for Machine Learning and Data Science&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Course 1 : &lt;a href=&quot;https://www.coursera.org/learn/machine-learning?specialization=machine-learning-introduction&quot;&gt;Supervised Machine Learning: Regression and Classification &lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1&quot;&gt;Week 1&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Regression&quot;&gt;Practice quiz: Regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Supervised%20vs%20unsupervised%20learning&quot;&gt;Practice quiz: Supervised vs unsupervised learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Train%20the%20model%20with%20gradient%20descent&quot;&gt;Practice quiz: Train the model with gradient descent&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab03_Model_Representation_Soln.ipynb&quot;&gt;Model Representation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab04_Cost_function_Soln.ipynb&quot;&gt;Cost Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab05_Gradient_Descent_Soln.ipynb&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2&quot;&gt;Week 2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Practice%20quiz%20-%20Gradient%20descent%20in%20practice&quot;&gt;Practice quiz: Gradient descent in practice&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Practice%20quiz%20-%20Multiple%20linear%20regression&quot;&gt;Practice quiz: Multiple linear regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb&quot;&gt;Numpy Vectorization&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab02_Multiple_Variable_Soln.ipynb&quot;&gt;Multi Variate Regression&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb&quot;&gt;Feature Scaling&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb&quot;&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab05_Sklearn_GD_Soln.ipynb&quot;&gt;Sklearn Gradient Descent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab05_Sklearn_GD_Soln.ipynb&quot;&gt;Sklearn Normal Method&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/C1W2A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/C1W2A1/C1_W2_Linear_Regression.ipynb&quot;&gt;Linear Regression&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3&quot;&gt;Week 3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Practice%20quiz%20-%20Cost%20function%20for%20logistic%20regression&quot;&gt;Practice quiz: Cost function for logistic regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Practice%20quiz%20-%20Gradient%20descent%20for%20logistic%20regression&quot;&gt;Practice quiz: Gradient descent for logistic regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab01_Classification_Soln.ipynb&quot;&gt;Classification&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab02_Sigmoid_function_Soln.ipynb&quot;&gt;Sigmoid Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab03_Decision_Boundary_Soln.ipynb&quot;&gt;Decision Boundary&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab04_LogisticLoss_Soln.ipynb&quot;&gt;Logistic Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab05_Cost_Function_Soln.ipynb&quot;&gt;Cost Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab06_Gradient_Descent_Soln.ipynb&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab07_Scikit_Learn_Soln.ipynb&quot;&gt;Scikit Learn - Logistic Regression&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab08_Overfitting_Soln.ipynb&quot;&gt;Overfitting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab09_Regularization_Soln.ipynb&quot;&gt;Regularization&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/C1W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/C1W3A1/C1_W3_Logistic_Regression.ipynb&quot;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/195768f3c1a83e42298d3f61dae99d01&quot;&gt;Certificate Of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;br /&gt; 
&lt;h2&gt;Course 2 : &lt;a href=&quot;https://www.coursera.org/learn/advanced-learning-algorithms?specialization=machine-learning-introduction&quot;&gt;Advanced Learning Algorithms&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1&quot;&gt;Week 1&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20Neural%20networks%20intuition&quot;&gt;Practice quiz: Neural networks intuition&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20Neural%20network%20model&quot;&gt;Practice quiz: Neural network model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20TensorFlow%20implementation&quot;&gt;Practice quiz: TensorFlow implementation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice-Quiz-Neural-Networks-Implementation-in-python&quot;&gt;Practice quiz : Neural Networks Implementation in Numpy&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab01_Neurons_and_Layers.ipynb&quot;&gt;Neurons and Layers&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab02_CoffeeRoasting_TF.ipynb&quot;&gt;Coffee Roasting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab02_CoffeeRoasting_TF.ipynb&quot;&gt;Coffee Roasting Using Numpy&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/C2W1A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/C2W1A1/C2_W1_Assignment.ipynb&quot;&gt;Neural Networks for Binary Classification&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;br /&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2&quot;&gt;Week 2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Neural-Network-Training&quot;&gt;Practice quiz : Neural Networks Training&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Activation-Functions&quot;&gt;Practice quiz : Activation Functions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-quiz-Multiclass-Classification&quot;&gt;Practice quiz : Multiclass Classification&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Additional-Neural-Network-Concepts&quot;&gt;Practice quiz : Additional Neural Networks Concepts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_Relu.ipynb&quot;&gt;RElu&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_SoftMax.ipynb&quot;&gt;Softmax&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_Multiclass_TF.ipynb&quot;&gt;Multiclass Classification&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/C2W2A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/C2W2A1/C2_W2_Assignment.ipynb&quot;&gt;Neural Networks For Handwritten Digit Recognition - Multiclass&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3&quot;&gt;Week 3&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/Practice-Quiz-Advice-for-applying-machine-learning&quot;&gt;Practice quiz : Advice for Applying Machine Learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/practice-quiz-bias-and-variance&quot;&gt;Practice quiz : Bias and Variance&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/practice-quiz-machine-learning-development-process&quot;&gt;Practice quiz : Machine Learning Development Process&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/C2W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/C2W3A1/C2_W3_Assignment.ipynb&quot;&gt;Advice for Applied Machine Learning&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4&quot;&gt;Week 4&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-decision-trees&quot;&gt;Practice quiz : Decision Trees&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-decision-tree-learning&quot;&gt;Practice quiz : Decision Trees Learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-tree-ensembles&quot;&gt;Practice quiz : Decision Trees Ensembles&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/C2W4A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/C2W4A1/C2_W4_Decision_Tree_with_Markdown.ipynb&quot;&gt;Decision Trees&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/c9a7766b0c6eab27db2e955376d29bf7&quot;&gt;Certificate of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;br /&gt; 
&lt;h2&gt;Course 3 : &lt;a href=&quot;https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning?specialization=machine-learning-introduction&quot;&gt;Unsupervised Learning, Recommenders, Reinforcement Learning&lt;/a&gt;&lt;/h2&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1&quot;&gt;Week 1&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/Practice%20Quiz%20-%20Clustering&quot;&gt;Practice quiz : Clustering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/Practice%20Quiz%20-%20Anomaly%20Detection&quot;&gt;Practice quiz : Anomaly Detection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A&quot;&gt;Programming Assignments&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A/C3W1A1/C3_W1_KMeans_Assignment.ipynb&quot;&gt;K means&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A/C3W1A2/C3_W1_Anomaly_Detection.ipynb&quot;&gt;Anomaly Detection&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2&quot;&gt;Week 2&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Collaborative%20Filtering&quot;&gt;Practice quiz : Collaborative Filtering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Recommender%20systems%20implementation&quot;&gt;Practice quiz : Recommender systems implementation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Content-based%20filtering&quot;&gt;Practice quiz : Content-based filtering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2&quot;&gt;Programming Assignments&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2/C3W2A1/C3_W2_Collaborative_RecSys_Assignment.ipynb&quot;&gt;Collaborative Filtering RecSys&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2/C3W2A2/C3_W2_RecSysNN_Assignment.ipynb&quot;&gt;RecSys using Neural Networks&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3&quot;&gt;Week 3&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20Reinforcement%20learning%20introduction&quot;&gt;Practice quiz : Reinforcement learning introduction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20State-action%20value%20function&quot;&gt;Practice Quiz : State-action value function&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20Continuous%20state%20spaces&quot;&gt;Practice Quiz : Continuous state spaces&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/C3W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/C3W3A1/C3_W3_A1_Assignment.ipynb&quot;&gt;Deep Q-Learning - Lunar Lander&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/5bf5ee456b0c806df9b8622067b47ca6&quot;&gt;Certificate of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;h3&gt;&lt;a href=&quot;https://coursera.org/share/a15ac6426f90924491a542850700a759&quot;&gt;Specialization Certificate&lt;/a&gt;&lt;/h3&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;hr /&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;h3&gt;Stargazers over time&lt;/h3&gt; 
 &lt;p&gt;&lt;a href=&quot;https://starchart.cc/greyhatguy007/Machine-Learning-Specialization-Coursera&quot;&gt;&lt;img src=&quot;https://starchart.cc/greyhatguy007/Machine-Learning-Specialization-Coursera.svg?variant=adaptive&quot; alt=&quot;Stargazers over time&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://hits.seeyoufarm.com&quot;&gt;&lt;img src=&quot;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fgreyhatguy007%2FMachine-Learning-Specialization-Coursera&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=hits&amp;amp;edge_flat=false&quot; alt=&quot;Hits&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;Course Review :&lt;/h3&gt; 
&lt;p&gt;This Course is a best place towards becoming a Machine Learning Engineer. Even if you&#39;re an expert, many algorithms are covered in depth such as decision trees which may help in further improvement of skills.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Special thanks to &lt;a href=&quot;https://www.andrewng.org/&quot;&gt;Professor Andrew Ng&lt;/a&gt; for structuring and tailoring this Course.&lt;/strong&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;hr /&gt; 
&lt;h4&gt;An insight of what you might be able to accomplish at the end of this specialization :&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;i&gt;Write an unsupervised learning algorithm to &lt;strong&gt;Land the Lunar Lander&lt;/strong&gt; Using Deep Q-Learning&lt;/i&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The Rover was trained to land correctly on the surface, correctly between the flags as indicators after many unsuccessful attempts in learning how to do it.&lt;/li&gt; 
   &lt;li&gt;The final landing after training the agent using appropriate parameters :&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/77543865/182395635-703ae199-ba79-4940-86eb-23dd90093ab3.mp4&quot;&gt;https://user-images.githubusercontent.com/77543865/182395635-703ae199-ba79-4940-86eb-23dd90093ab3.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;i&gt;Write an algorithm for a &lt;strong&gt;Movie Recommender System&lt;/strong&gt;&lt;/i&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;A movie database is collected based on its genre.&lt;/li&gt; 
   &lt;li&gt;A content based filtering and collaborative filtering algorithm is trained and the movie recommender system is implemented.&lt;/li&gt; 
   &lt;li&gt;It gives movie recommendentations based on the movie genre.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/77543865/182398093-c7387754-34a9-4044-b842-0085060c3525.png&quot; alt=&quot;movie_recommendation&quot; /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;i&gt; And Much More !! &lt;/i&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Concluding, this is a course which I would recommend everyone to take. Not just because you learn many new stuffs, but also the assignments are real life examples which are &lt;em&gt;exciting to complete&lt;/em&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;strong&gt;Happy Learning :))&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AI4Finance-Foundation/FinRobot</title>
      <link>https://github.com/AI4Finance-Foundation/FinRobot</link>
      <description>&lt;p&gt;FinRobot: An Open-Source AI Agent Platform for Financial Analysis using LLMs ðŸš€ ðŸš€ ðŸš€&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; width=&quot;30%&quot; alt=&quot;image&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139&quot; /&gt; 
&lt;/div&gt; 
&lt;h1&gt;FinRobot: An Open-Source AI Agent Platform for Financial Analysis using Large Language Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/%5Bhttps://pepy.tech/project/finrobot%5D(https://pepy.tech/project/finrobot)&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/finrobot&quot; alt=&quot;Downloads&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/finrobot&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/finrobot/week&quot; alt=&quot;Downloads&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.python.org/downloads/release/python-360/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.6-blue.svg?sanitize=true&quot; alt=&quot;Python 3.8&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/finrobot/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/finrobot.svg?sanitize=true&quot; alt=&quot;PyPI&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/github/license/AI4Finance-Foundation/finrobot.svg?color=brightgreen&quot; alt=&quot;License&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/issues-raw/AI4Finance-Foundation/finrobot?label=Issues&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/issues-closed-raw/AI4Finance-Foundation/finrobot?label=Closed+Issues&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/issues-pr-raw/AI4Finance-Foundation/finrobot?label=Open+PRs&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/issues-pr-closed-raw/AI4Finance-Foundation/finrobot?label=Closed+PRs&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/figs/logo_white_background.jpg&quot; width=&quot;40%&quot; /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;FinRobot&lt;/strong&gt; is an AI Agent Platform that transcends the scope of FinGPT, representing a comprehensive solution meticulously designed for financial applications. It integrates &lt;strong&gt;a diverse array of AI technologies&lt;/strong&gt;, extending beyond mere language models. This expansive vision highlights the platform&#39;s versatility and adaptability, addressing the multifaceted needs of the financial industry.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Concept of AI Agent&lt;/strong&gt;: an AI Agent is an intelligent entity that uses large language models as its brain to perceive its environment, make decisions, and execute actions. Unlike traditional artificial intelligence, AI Agents possess the ability to independently think and utilize tools to progressively achieve given objectives.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14767&quot;&gt;Whitepaper of FinRobot&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/trsr8SXpW5&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/trsr8SXpW5&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://api.visitorbadge.io/api/VisitorHit?user=AI4Finance-Foundation&amp;amp;repo=FinRobot&amp;amp;countColor=%23B17A&quot; alt=&quot;Visitors&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;FinRobot Ecosystem&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/6b30d9c1-35e5-4d36-a138-7e2769718f62&quot; width=&quot;90%&quot; /&gt; 
&lt;/div&gt; 
&lt;h3&gt;The overall framework of FinRobot is organized into four distinct layers, each designed to address specific aspects of financial AI processing and application:&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Financial AI Agents Layer&lt;/strong&gt;: The Financial AI Agents Layer now includes Financial Chain-of-Thought (CoT) prompting, enhancing complex analysis and decision-making capacity. Market Forecasting Agents, Document Analysis Agents, and Trading Strategies Agents utilize CoT to dissect financial challenges into logical steps, aligning their advanced algorithms and domain expertise with the evolving dynamics of financial markets for precise, actionable insights.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial LLMs Algorithms Layer&lt;/strong&gt;: The Financial LLMs Algorithms Layer configures and utilizes specially tuned models tailored to specific domains and global market analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLMOps and DataOps Layers&lt;/strong&gt;: The LLMOps layer implements a multi-source integration strategy that selects the most suitable LLMs for specific financial tasks, utilizing a range of state-of-the-art models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-source LLM Foundation Models Layer&lt;/strong&gt;: This foundational layer supports the plug-and-play functionality of various general and specialized LLMs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;FinRobot: Agent Workflow&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/ff8033be-2326-424a-ac11-17e2c9c4983d&quot; width=&quot;60%&quot; /&gt; 
&lt;/div&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Perception&lt;/strong&gt;: This module captures and interprets multimodal financial data from market feeds, news, and economic indicators, using sophisticated techniques to structure the data for thorough analysis.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Brain&lt;/strong&gt;: Acting as the core processing unit, this module perceives data from the Perception module with LLMs and utilizes Financial Chain-of-Thought (CoT) processes to generate structured instructions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;: This module executes instructions from the Brain module, applying tools to translate analytical insights into actionable outcomes. Actions include trading, portfolio adjustments, generating reports, or sending alerts, thereby actively influencing the financial environment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;FinRobot: Smart Scheduler&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/06fa0b78-ac53-48d3-8a6e-98d15386327e&quot; width=&quot;60%&quot; /&gt; 
&lt;/div&gt; 
&lt;p&gt;The Smart Scheduler is central to ensuring model diversity and optimizing the integration and selection of the most appropriate LLM for each task.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Director Agent&lt;/strong&gt;: This component orchestrates the task assignment process, ensuring that tasks are allocated to agents based on their performance metrics and suitability for specific tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Registration&lt;/strong&gt;: Manages the registration and tracks the availability of agents within the system, facilitating an efficient task allocation process.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Adaptor&lt;/strong&gt;: Tailor agent functionalities to specific tasks, enhancing their performance and integration within the overall system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task Manager&lt;/strong&gt;: Manages and stores different general and fine-tuned LLMs-based agents tailored for various financial tasks, updated periodically to ensure relevance and efficacy.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;p&gt;The main folder &lt;strong&gt;finrobot&lt;/strong&gt; has three subfolders &lt;strong&gt;agents, data_source, functional&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FinRobot
â”œâ”€â”€ finrobot (main folder)
â”‚   â”œâ”€â”€ agents
â”‚   	â”œâ”€â”€ agent_library.py
â”‚   	â””â”€â”€ workflow.py
â”‚   â”œâ”€â”€ data_source
â”‚   	â”œâ”€â”€ finnhub_utils.py
â”‚   	â”œâ”€â”€ finnlp_utils.py
â”‚   	â”œâ”€â”€ fmp_utils.py
â”‚   	â”œâ”€â”€ sec_utils.py
â”‚   	â””â”€â”€ yfinance_utils.py
â”‚   â”œâ”€â”€ functional
â”‚   	â”œâ”€â”€ analyzer.py
â”‚   	â”œâ”€â”€ charting.py
â”‚   	â”œâ”€â”€ coding.py
â”‚   	â”œâ”€â”€ quantitative.py
â”‚   	â”œâ”€â”€ reportlab.py
â”‚   	â””â”€â”€ text.py
â”‚   â”œâ”€â”€ toolkits.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ configs
â”œâ”€â”€ experiments
â”œâ”€â”€ tutorials_beginner (hands-on tutorial)
â”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb
â”‚   â””â”€â”€ agent_annual_report.ipynb 
â”œâ”€â”€ tutorials_advanced (advanced tutorials for potential finrobot developers)
â”‚   â”œâ”€â”€ agent_trade_strategist.ipynb
â”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb
â”‚   â”œâ”€â”€ agent_annual_report.ipynb 
â”‚   â”œâ”€â”€ lmm_agent_mplfinance.ipynb
â”‚   â””â”€â”€ lmm_agent_opt_smacross.ipynb
â”œâ”€â”€ setup.py
â”œâ”€â”€ OAI_CONFIG_LIST_sample
â”œâ”€â”€ config_api_keys_sample
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation:&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. (Recommended) Create a new virtual environment&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;conda create --name finrobot python=3.10
conda activate finrobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. download the FinRobot repo use terminal or download it manually&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git clone https://github.com/AI4Finance-Foundation/FinRobot.git
cd FinRobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. install finrobot &amp;amp; dependencies from source or pypi&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;get our latest release from pypi&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U finrobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or install from this repo directly&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. modify OAI_CONFIG_LIST_sample file&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;1) rename OAI_CONFIG_LIST_sample to OAI_CONFIG_LIST
2) remove the four lines of comment within the OAI_CONFIG_LIST file
3) add your own openai api-key &amp;lt;your OpenAI API key here&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;5. modify config_api_keys_sample file&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;1) rename config_api_keys_sample to config_api_keys
2) remove the comment within the config_api_keys file
3) add your own finnhub-api &quot;YOUR_FINNHUB_API_KEY&quot;
4) add your own financialmodelingprep and sec-api keys &quot;YOUR_FMP_API_KEY&quot; and &quot;YOUR_SEC_API_KEY&quot; (for financial report generation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;6. start navigating the tutorials or the demos below:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# find these notebooks in tutorials
1) agent_annual_report.ipynb
2) agent_fingpt_forecaster.ipynb
3) agent_trade_strategist.ipynb
4) lmm_agent_mplfinance.ipynb
5) lmm_agent_opt_smacross.ipynb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;h3&gt;1. Market Forecaster Agent (Predict Stock Movements Direction)&lt;/h3&gt; 
&lt;p&gt;Takes a company&#39;s ticker symbol, recent basic financials, and market news as input and predicts its stock movements.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Import&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import autogen
from finrobot.utils import get_current_date, register_keys_from_json
from finrobot.agents.workflow import SingleAssistant
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Config&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Read OpenAI API keys from a JSON file
llm_config = {
    &quot;config_list&quot;: autogen.config_list_from_json(
        &quot;../OAI_CONFIG_LIST&quot;,
        filter_dict={&quot;model&quot;: [&quot;gpt-4-0125-preview&quot;]},
    ),
    &quot;timeout&quot;: 120,
    &quot;temperature&quot;: 0,
}

# Register FINNHUB API keys
register_keys_from_json(&quot;../config_api_keys&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;company = &quot;NVDA&quot;

assitant = SingleAssistant(
    &quot;Market_Analyst&quot;,
    llm_config,
    # set to &quot;ALWAYS&quot; if you want to chat instead of simply receiving the prediciton
    human_input_mode=&quot;NEVER&quot;,
)
assitant.chat(
    f&quot;Use all the tools provided to retrieve information available for {company} upon {get_current_date()}. Analyze the positive developments and potential concerns of {company} &quot;
    &quot;with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. &quot;
    f&quot;Then make a rough prediction (e.g. up/down by 2-3%) of the {company} stock price movement for next week. Provide a summary analysis to support your prediction.&quot;
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Result&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/812ec23a-9cb3-4fad-b716-78533ddcd9dc&quot; width=&quot;40%&quot; /&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/9a2f9f48-b0e1-489c-8679-9a4c530f313c&quot; width=&quot;41%&quot; /&gt; 
&lt;/div&gt; 
&lt;h3&gt;2. Financial Analyst Agent for Report Writing (Equity Research Report)&lt;/h3&gt; 
&lt;p&gt;Take a company&#39;s 10-k form, financial data, and market data as input and output an equity research report&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Import&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
import autogen
from textwrap import dedent
from finrobot.utils import register_keys_from_json
from finrobot.agents.workflow import SingleAssistantShadow
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Config&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;llm_config = {
    &quot;config_list&quot;: autogen.config_list_from_json(
        &quot;../OAI_CONFIG_LIST&quot;,
        filter_dict={
            &quot;model&quot;: [&quot;gpt-4-0125-preview&quot;],
        },
    ),
    &quot;timeout&quot;: 120,
    &quot;temperature&quot;: 0.5,
}
register_keys_from_json(&quot;../config_api_keys&quot;)

# Intermediate strategy modules will be saved in this directory
work_dir = &quot;../report&quot;
os.makedirs(work_dir, exist_ok=True)

assistant = SingleAssistantShadow(
    &quot;Expert_Investor&quot;,
    llm_config,
    max_consecutive_auto_reply=None,
    human_input_mode=&quot;TERMINATE&quot;,
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;company = &quot;Microsoft&quot;
fyear = &quot;2023&quot;

message = dedent(
    f&quot;&quot;&quot;
    With the tools you&#39;ve been provided, write an annual report based on {company}&#39;s {fyear} 10-k report, format it into a pdf.
    Pay attention to the followings:
    - Explicitly explain your working plan before you kick off.
    - Use tools one by one for clarity, especially when asking for instructions. 
    - All your file operations should be done in &quot;{work_dir}&quot;. 
    - Display any image in the chat once generated.
    - All the paragraphs should combine between 400 and 450 words, don&#39;t generate the pdf until this is explicitly fulfilled.
&quot;&quot;&quot;
)

assistant.chat(message, use_cache=True, max_turns=50,
               summary_method=&quot;last_msg&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Result&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/d2d999e0-dc0e-4196-aca1-218f5fadcc5b&quot; width=&quot;60%&quot; /&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/3a21873f-9498-4d73-896b-3740bf6d116d&quot; width=&quot;60%&quot; /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Financial CoT&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Gather Preliminary Data&lt;/strong&gt;: 10-K report, market data, financial ratios&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Analyze Financial Statements&lt;/strong&gt;: balance sheet, income statement, cash flow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Company Overview and Performance&lt;/strong&gt;: company description, business highlights, segment analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Risk Assessment&lt;/strong&gt;: assess risks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial Performance Visualization&lt;/strong&gt;: plot PE ratio and EPS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Synthesize Findings into Paragraphs&lt;/strong&gt;: combine all parts into a coherent summary&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generate PDF Report&lt;/strong&gt;: use tools to generate PDF automatically&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality Assurance&lt;/strong&gt;: check word counts&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;3. Trade Strategist Agent with multimodal capabilities&lt;/h3&gt; 
&lt;h2&gt;AI Agent Papers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Stanford University + Microsoft Research] &lt;a href=&quot;https://arxiv.org/abs/2401.03568&quot;&gt;Agent AI: Surveying the Horizons of Multimodal Interaction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Stanford University] &lt;a href=&quot;https://arxiv.org/abs/2304.03442&quot;&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Fudan NLP Group] &lt;a href=&quot;https://arxiv.org/abs/2309.07864&quot;&gt;The Rise and Potential of Large Language Model Based Agents: A Survey&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Fudan NLP Group] &lt;a href=&quot;https://github.com/WooooDyy/LLM-Agent-Paper-List&quot;&gt;LLM-Agent-Paper-List&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Tsinghua University] &lt;a href=&quot;https://arxiv.org/abs/2312.11970&quot;&gt;Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Renmin University] &lt;a href=&quot;https://arxiv.org/pdf/2308.11432.pdf&quot;&gt;A Survey on Large Language Model-based Autonomous Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Nanyang Technological University] &lt;a href=&quot;https://arxiv.org/abs/2402.18485&quot;&gt;FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI Agent Blogs and Videos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Medium] &lt;a href=&quot;https://medium.com/humansdotai/an-introduction-to-ai-agents-e8c4afd2ee8f&quot;&gt;An Introduction to AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Medium] &lt;a href=&quot;https://medium.com/@aitrendorbit/unmasking-the-best-character-ai-chatbots-2024-351de43792f4#the-best-character-ai-chatbots&quot;&gt;Unmasking the Best Character AI Chatbots | 2024&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[big-picture] &lt;a href=&quot;https://blog.big-picture.com/en/chatgpt-next-level-meet-10-autonomous-ai-agents-auto-gpt-babyagi-agentgpt-microsoft-jarvis-chaosgpt-friends/&quot;&gt;ChatGPT, Next Level: Meet 10 Autonomous AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[TowardsDataScience] &lt;a href=&quot;https://towardsdatascience.com/navigating-the-world-of-llm-agents-a-beginners-guide-3b8d499db7a9&quot;&gt;Navigating the World of LLM Agents: A Beginnerâ€™s Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[YouTube] &lt;a href=&quot;https://www.youtube.com/watch?v=iVbN95ica_k&quot;&gt;Introducing Devin - The &quot;First&quot; AI Agent Software Engineer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI Agent Open-Source Framework &amp;amp; Tool&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Significant-Gravitas/AutoGPT&quot;&gt;AutoGPT (163k stars)&lt;/a&gt; is a tool for everyone to use, aiming to democratize AI, making it accessible for everyone to use and build upon.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain&quot;&gt;LangChain (87.4k stars)&lt;/a&gt; is a framework for developing context-aware applications powered by language models, enabling them to connect to sources of context and rely on the model&#39;s reasoning capabilities for responses and actions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/geekan/MetaGPT&quot;&gt;MetaGPT (41k stars)&lt;/a&gt; is a multi-agent open-source framework that assigns different roles to GPTs, forming a collaborative software entity to execute complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langgenius/dify&quot;&gt;dify (34.1.7k stars)&lt;/a&gt; is an LLM application development platform. It integrates the concepts of Backend as a Service and LLMOps, covering the core tech stack required for building generative AI-native applications, including a built-in RAG engine&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/autogen&quot;&gt;AutoGen (27.4k stars)&lt;/a&gt; is a framework for developing LLM applications with conversational agents that collaborate to solve tasks. These agents are customizable, support human interaction, and operate in modes combining LLMs, human inputs, and tools.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/ChatDev&quot;&gt;ChatDev (24.1k stars)&lt;/a&gt; is a framework that focuses on developing conversational AI Agents capable of dialogue and question-answering. It provides a range of pre-trained models and interactive interfaces, facilitating the development of customized chat Agents for users.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yoheinakajima/babyagi&quot;&gt;BabyAGI (19.5k stars)&lt;/a&gt; is an AI-powered task management system, dedicated to building AI Agents with preliminary general intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/joaomdmoura/crewAI&quot;&gt;CrewAI (16k stars)&lt;/a&gt; is a framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/TransformerOptimus/SuperAGI&quot;&gt;SuperAGI (14.8k stars)&lt;/a&gt; is a dev-first open-source autonomous AI agent framework enabling developers to build, manage &amp;amp; run useful autonomous agents.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/labring/FastGPT&quot;&gt;FastGPT (14.6k stars)&lt;/a&gt; is a knowledge-based platform built on the LLM, offers out-of-the-box data processing and model invocation capabilities, allows for workflow orchestration through Flow visualization.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/XAgent&quot;&gt;XAgent (7.8k stars)&lt;/a&gt; is an open-source experimental Large Language Model (LLM) driven autonomous agent that can automatically solve various tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dataelement/bisheng&quot;&gt;Bisheng (7.8k stars)&lt;/a&gt; is a leading open-source platform for developing LLM applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/XAgent&quot;&gt;Voyager (5.3k stars)&lt;/a&gt; An Open-Ended Embodied Agent with Large Language Models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/camel-ai/camel&quot;&gt;CAMEL (4.7k stars)&lt;/a&gt; is a framework that offers a comprehensive set of tools and algorithms for building multimodal AI Agents, enabling them to handle various data forms such as text, images, and speech.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langfuse/langfuse&quot;&gt;Langfuse (4.3k stars)&lt;/a&gt; is a language fusion framework that can integrate the language abilities of multiple AI Agents, enabling them to simultaneously possess multilingual understanding and generation capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citing FinRobot&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{
zhou2024finrobot,
title={FinRobot: {AI} Agent for Equity Research and Valuation with Large Language Models},
author={Tianyu Zhou and Pinqiao Wang and Yilin Wu and Hongyang Yang},
booktitle={ICAIF 2024: The 1st Workshop on Large Language Models and Generative AI for Finance},
year={2024}
}

@article{yang2024finrobot,
  title={FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models},
  author={Yang, Hongyang and Zhang, Boyu and Wang, Neng and Guo, Cheng and Zhang, Xiaoli and Lin, Likun and Wang, Junlin and Zhou, Tianyu and Guan, Mao and Zhang, Runjia and others},
  journal={arXiv preprint arXiv:2405.14767},
  year={2024}
}

@inproceedings{han2024enhancing,
  title={Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research},
  author={Han, Xuewen and Wang, Neng and Che, Shangkun and Yang, Hongyang and Zhang, Kunpeng and Xu, Sean Xin},
  booktitle={ICAIF 2024: Proceedings of the 5th ACM International Conference on AI in Finance},
  pages={538--546},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: The codes and documents provided herein are released under the Apache-2.0 license. They should not be construed as financial counsel or recommendations for live trading. It is imperative to exercise caution and consult with qualified financial professionals prior to any trading or investment actions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>spmallick/learnopencv</title>
      <link>https://github.com/spmallick/learnopencv</link>
      <description>&lt;p&gt;Learn OpenCV : C++ and Python Examples&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LearnOpenCV&lt;/h1&gt; 
&lt;p&gt;This repository contains code for Computer Vision, Deep learning, and AI research articles shared on our blog &lt;a href=&quot;https://www.LearnOpenCV.com&quot;&gt;LearnOpenCV.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Want to become an expert in AI? &lt;a href=&quot;https://opencv.org/courses/&quot;&gt;AI Courses by OpenCV&lt;/a&gt; is a great place to start.&lt;/p&gt; 
&lt;a href=&quot;https://opencv.org/courses/&quot;&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://learnopencv.com/wp-content/uploads/2023/01/AI-Courses-By-OpenCV-Github.png&quot; /&gt; &lt;/p&gt; &lt;/a&gt; 
&lt;h2&gt;List of Blog Posts&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Blog Post&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;Code&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/video-rag-for-long-videos/&quot;&gt;Video-RAG: Training-Free Retrieval for Long-Video LVLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Video-RAG_Training_Free_Retrieval_for_Long_Video_LVLMs&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-detection-with-vlms-ft-qwen2-5-vl/&quot;&gt;Object Detection and Spatial Understanding with VLMs ft. Qwen2.5-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/object-detection-with-vlms&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/langgraph-self-correcting-agent-code-generation/&quot;&gt;LangGraph: Building Self-Correcting RAG Agent for Code Generation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/LangGraph_Building_Self_Correcting_RAG_Agent_for_Code_Generation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/sinusoidal-position-embeddings/&quot;&gt;Inside Sinusoidal Position Embeddings: A Sense of Order&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Sinusoidal_Position_Embeddings&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/rope-position-embeddings/&quot;&gt;Inside RoPE: Rotary Magic into Position Embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Inside_RoPE_Position_Embeddings&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/simlingo-vision-language-action-model-for-autonomous-driving/&quot;&gt;SimLingo-Vision-Language-Action-Model-for-Autonomous-Driving&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SimLingo-Vision-Language-Action-Model-for-Autonomous-Driving&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/finetuning-gemma-3n-medical-vqa/&quot;&gt;FineTuning Gemma 3n for Medical VQA on ROCOv2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/finetuning-gemma3n&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/smollm3-explained/&quot;&gt;SmolLM3 Blueprint: SOTA 3B-Parameter LLM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/langgraph-building-a-visual-web-browser-agent/&quot;&gt;LangGraph-A-Visual-Automation-and-Summarization-Pipeline&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/LangGraph-A-Visual-Automation-and-Summarization-Pipeline&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-anomalyclip-medical-anomaly-clip/&quot;&gt;Fine-Tuning AnomalyCLIP: Class-Agnostic Zero-Shot Anomaly Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-AnomalyCLIP&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/siglip-2-deepminds-multilingual-vision-language-model/&quot;&gt;SigLIP 2: DeepMindâ€™s Multilingual Vision-Language Model&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/medgemma-explained/&quot;&gt;MedGemma: Googleâ€™s Medico VLM for Clinical QA, Imaging, and More&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/medgemma&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nanonets-ocr-s/&quot;&gt;Nanonets-OCR-s: Enabling Rich, Structured Markdown for Document Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/optimizing-vjepa-2-in-real-time-video-classification/&quot;&gt;Optimizing VJEPA-2: Tackling Latency &amp;amp; Context in Real-Time Video Classification Scripts&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/VJEPA-2-Video-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/?p=73731&amp;amp;preview_id=73731&amp;amp;preview_nonce=beb70ccf8e&amp;amp;preview=true#heading-7&quot;&gt;V-JEPA 2: Metaâ€™s Breakthrough in AI for the Physical World&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/V-JEPA-2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/cosmos-reason-vlm-video-vqa/&quot;&gt;NVIDIA Cosmos Reason1: Video Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Cosmos-Reason1-Video-Understanding&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/gr00t-n1_5-explained/&quot;&gt;GR00T N1.5 Explained&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/llava-training-a-visual-assistant/&quot;&gt;LLaVA&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/LLaVA&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/smolvla-lerobot-vision-language-action-model/&quot;&gt;SmolVLA: Affordable &amp;amp; Efficient VLA Robotics on Consumer GPUs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/smolvla&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-grounding-dino/&quot;&gt;Fine-Tuning Grounding DINO: Open-Vocabulary Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Grounding-DINO-Open-Vocabulary-Object-Detection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/qwen3/&quot;&gt;Getting Started with Qwen3 â€“ The Thinking Expert&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/qwen3&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/modern-gpu-architecture-explained/&quot;&gt;Inside the GPU: A Comprehensive Guide to Modern Graphics Architecture&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/distributed-parallel-training-pytorch-multi-gpu-setup/&quot;&gt;Distributed Parallel Training: PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Distributed-Training-PyTorch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/monai-medical-imaging-pytorch/&quot;&gt;MONAI: The Definitive Framework for Medical Imaging Powered by PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/sana-sprint-the-one-step-revolution-in-high-quality-ai-image-synthesis/&quot;&gt;SANA-Sprint: The One-Step Revolution in High-Quality AI Image Synthesis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/framepack-video-diffusion/&quot;&gt;FramePack-Video-Diffusion-but-feels-like-Image-Diffusion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FramePack-Video-Diffusion-but-feels-like-Image-Diffusion&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/model-weights-file-formats-in-machine-learning/&quot;&gt;Model Weights File Formats in Machine Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/unsloth-guide-efficient-llm-fine-tuning/&quot;&gt;Unsloth: A Guide from Basics to Fine-Tuning Vision Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Unsloth_A_Guide_From_Basics_to_Fine_Tuning_Vision_Models&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/iterative-closest-point-icp-explained/&quot;&gt;Iterative Closest Point (ICP) Algorithm Explained&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/Iterative-Closest-Point-ICP&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/medsam2-explained/&quot;&gt;MedSAM2 Explained: One Prompt to Segment Anything in Medical Imaging&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/medsam2-explained&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/batch-normalization-and-dropout-as-regularizers/&quot;&gt;Batch Normalization and Dropout as Regularizers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/dinov2-self-supervised-vision-transformer/&quot;&gt;DINOv2_by_Meta_A_Self-Supervised_foundational_vision_model&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/DINOv2_by_Meta_A_Self-Supervised_foundational_vision_model&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/embedding-models-explained/&quot;&gt;Beginner&#39;s Guide to Embedding Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mast3r-slam-realtime-dense-slam-explained/&quot;&gt;MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/MASt3R-SLAM&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/googles-a2a-protocol-heres-what-you-need-to-know/&quot;&gt;Google&#39;s A2A Protocol&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-sana-image-generation-model/&quot;&gt;Nvidia SANA : Faster Image Generation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/rf-detr-object-detection/&quot;&gt;Fine-tuning RF-DETR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/Fine-tuning-RF-DETR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/qwen2.5-omni/&quot;&gt;Qwen2.5-Omni: A Real-Time Multimodal AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/vision-language-action-models-lerobot-policy/&quot;&gt;Vision Language Action Models: Robotic Control&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Vision-Language-Action-Models&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-gemma-3/&quot;&gt;Fine-Tuning Gemma 3 VLM using QLoRA for LaTeX-OCR Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Gemma-3-VLM-using-QLoRA-for-LaTeX-OCR-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-comfyui-for-stable-diffusion/&quot;&gt;ComfyUI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ComfyUI&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/gemma-3/&quot;&gt;Gemma-3: A Comprehensive Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo11-on-raspberry-pi/&quot;&gt;YOLO11 on Raspberry Pi: Optimizing Object Detection for Edge Devices&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/yolo11-on-raspberry-pi&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/vggt-visual-geometry-grounded-transformer-3d-reconstruction/&quot;&gt;VGGT: Visual Geometry Grounded Transformer â€“ For Dense 3D Reconstruction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/VGGT-3D-Reconstruction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/understanding-ddim/&quot;&gt;DDIM: The Faster, Improved Version of DDPM for Efficient AI Image Generation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DDIM-The-Faster-Improved-Version-of-DDPM-for-Efficient-AI-Image-Generation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-model-context-protocol/&quot;&gt;Introduction to Model Context Protocol (MCP)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mast3r-sfm-grounding-image-matching-3d/&quot;&gt;MASt3R and MASt3R-SfM Explanation: Image Matching and 3D Reconstruction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/MASt3R-SfM-3D-Reconstruction-Image-Matching&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/matanyone-for-better-video-matting/&quot;&gt;MatAnyone Explained: Consistent Memory for Better Video Matting&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/MatAnyone-Explained-Consistent-Memory-for-Better-Video-Matting&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/graphrag-explained-knowledge-graphs-medical/&quot;&gt;GraphRAG: For Medical Document Analysis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Graphrag-Medical-Document-Analysis&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/omniparser-vision-based-gui-agent/&quot;&gt;OmniParser: Vision Based GUI Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-yolov12/&quot;&gt;Fine-Tuning-YOLOv12-Comparison-With-YOLOv11-And-YOLOv7-Based-Darknet&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv12-Comparison-With-YOLOv11-And-YOLOv7-Based-Darknet&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/finetuning-retinanet&quot;&gt;FineTuning RetinaNet for Wildlife Detection with PyTorch: A Step-by-Step Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/finetuning-retinanet&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/dust3r-geometric-3d-vision/&quot;&gt;DUSt3R: Geometric 3D Vision Made Easy : Explanation and Results&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DUSt3R-Dense-3D-Reconstruction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov12&quot;&gt;YOLOv12: Attention Meets Speed&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv12&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/video-generation-models/&quot;&gt;Video Generation: A Diffusion based approach&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Video-Generation-A-Diffusion-based-approach&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/agentic-ai/&quot;&gt;Agentic AI: A Comprehensive Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Agentic-AI-A-Comprehensive-Introduction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/finetuning-sam2/&quot;&gt;Finetuning SAM2 for Leaf Disease Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/finetuning-sam2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-insertion-in-gaussian-splatting/&quot;&gt;Object Insertion in Gaussian Splatting: Paper Explained and Training Code for MCMC and Bilateral Grid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Insertion-in-Gaussian-Splatting&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/depth-pro-monocular-metric-depth&quot;&gt;Depth Pro: Sharp Monocular Metric Depth&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DepthPro-Monocular-Metric-Depth&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-stable-diffusion-3-5m/&quot;&gt;Fine-tuning-Stable-Diffusion-3_5-UI-images&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-Stable-Diffusion-3_5-UI-images&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/simsiam/&quot;&gt;SimSiam: Streamlining SSL with Stop-Gradient Mechanism&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SimSiam-Streamlining-SSL-with-Stop-Gradient-Mechanism&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/image-captioning/&quot;&gt;Image Captioning using ResNet and LSTM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Image-Captioning-using-ResNet-and-LSTM&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/molmo-vlm&quot;&gt;Molmo VLM: Paper Explanation and Demo&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Molmo-VLM-SAM2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/3d-gaussian-splatting/&quot;&gt;3D Gaussian Splatting Paper Explanation: Training Custom Datasets with NeRF-Studio Gsplats&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/3D-Gaussian-Splatting-Code&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/flux-ai-image-generator/&quot;&gt;FLUX Image Generation: Experimenting with the Parameters&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Flux-Image-Generation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/contrastive-learning-simclr-and-byol-with-code-example/&quot;&gt;Contrastive-Learning-SimCLR-and-BYOL(With Code Example)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Contrastive-Learning-SimCLR-and-BYOL&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/annotated-nerf-pytorch/&quot;&gt;The Annotated NeRF : Training on Custom Dataset from Scratch in Pytorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Annotated-NeRF&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/stable-diffusion-3/&quot;&gt;Stable Diffusion 3 and 3.5: Paper Explanation and Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Stable-Diffusion-3&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/lightrag/&quot;&gt;LightRAG - Legal Document Analysis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/LightRAG-Legal&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-ai-summit-2024-india-overview/&quot;&gt;NVIDIA AI Summit 2024 â€“ India Overview&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/speech-to-speech/&quot;&gt;Introduction to Speech to Speech: Most Efficient Form of NLP&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/speech-to-speech&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/3d-u-net-brats/&quot;&gt;Training 3D U-Net for Brain Tumor Segmentation (BraTS-GLI)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Training_3D_U-Net_Brain_Tumor_Seg&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/detr-overview-and-inference/&quot;&gt;DETR: Overview and Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DETR-Overview_and_Inference&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo11/&quot;&gt;YOLO11: Faster Than You Can Imagine!&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLO11&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tune-dino-self-supervised-learning-segmentation/&quot;&gt;Exploring DINO: Self-Supervised Transformers for Road Segmentation with ResNet50 and U-Net&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Exploring-DINO-dino-road-segmentation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/sapiens-human-vision-models&quot;&gt;Sapiens: Foundation for Human Vision Models by Meta&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Sapiens-Human-Vision-Model-Meta&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/multimodal-rag-with-colpali&quot;&gt;Multimodal RAG with ColPali and Gemini&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Multimodal-RAG-with-ColPali-Gemini&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/pid-controller-ros-2-carla/&quot;&gt;Building Autonomous Vehicle in Carla: Path Following with PID Control &amp;amp; ROS 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Building_Autonomous_Vehicle_in_Carla_Path_Following_with_PID_Control_ROS2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/handwritten-text-recognition-using-ocr/&quot;&gt;Handwritten Text Recognition using OCR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Handwritten_Text_Recognition_using_OCR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/clip-model&quot;&gt;Training CLIP from Sratch for Image Retrieval&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Training-CLIP-from-Scratch-for-Image-Retrieval&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/lidar-slam-with-ros2&quot;&gt;Introduction to LiDAR SLAM: LOAM and LeGO-LOAM Paper and Code Explanation with ROS 2 Implementation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/LeGO-LOAM-ROS2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/recommendation-system-using-vector-search&quot;&gt;Recommendation System using Vector Search&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Recommendation-System-using-Vector-Search&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-whisper-on-custom-dataset/&quot;&gt;Fine Tuning Whisper on Custom Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Whisper-on-Custom-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/sam-2/&quot;&gt;SAM 2 â€“ Promptable Segmentation for Images and Videos&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SAM_2_Segment_Anything_Model_2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/feature-matching/&quot;&gt;Introduction to Feature Matching Using Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Feature-Matching-Using-Neural-Networks&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/robot-operating-system-introduction&quot;&gt;Introduction to ROS2 (Robot Operating System 2): Tutorial on ROS2 Working, DDS, ROS1 RMW, Topics, Nodes, Publisher, Subscriber in Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-ROS2-in-python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/cvpr-2024-research-papers&quot;&gt;CVPR 2024 Research Papers - Part- 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/cvpr-2024-research-papers-part2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/cvpr2024/&quot;&gt;CVPR 2024: An Overview and Key Papers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/CVPR-2024&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-detection-on-edge-device&quot;&gt;Object Detection on Edge Device - OAK-D-Lite&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Detection-on-Edge-Devices&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-yolov10/&quot;&gt;Fine-Tuning YOLOv10 Models on Custom Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv10-Models-Custom-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ros2-and-carla-setup-guide/&quot;&gt;ROS2 and Carla Setup Guide for Ubuntu 22.04&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/monocular-slam-in-python/&quot;&gt;Understanding Visual SLAM for Robotics Perception: Building Monocular SLAM from Scratch in Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Monocular%20SLAM%20for%20Robotics%20implementation%20in%20python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/u2-net-image-segmentation/&quot;&gt;Enhancing Image Segmentation using U2-Net: An Approach to Efficient Background Removal&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Efficient-Background-Removal-using-U2-Net&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov10/&quot;&gt;YOLOv10: The Dual-Head OG of YOLO Series&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv10&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-faster-r-cnn/&quot;&gt;Fine-tuning Faster R-CNN on Sea Rescue Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-Faster-R-CNN-on-SeaRescue-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/recommendation-system/&quot;&gt;Mastering Recommendation System: A Complete Guide&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/automatic-speech-recognition/&quot;&gt;Automatic Speech Recognition with Diarization : Speech-to-Text&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Automatic-Speech-Recognition-with-Diarization-Speech-to-Text&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mobilevit-keras-3/&quot;&gt;Building MobileViT Image Classification Model from Scratch In Keras 3&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Building%20MobileViT%20from%20Scratch%20in%20Keras%203&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/sdxl-inpainting/&quot;&gt;SDXL Inpainting: Fusing Image Inpainting with Stable Diffusion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SDXL-inpainting&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov9-instance-segmentation-on-medical-dataset/&quot;&gt;YOLOv9 Instance Segmentation on Medical Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv9-Instance-Segmentation-on-Medical-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/a-comprehensive-guide-to-robotics/&quot;&gt;A Comprehensive Guide to Robotics&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/integrating-gradio-with-opencv-dnn/&quot;&gt;Integrating Gradio with OpenCV DNN&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Integrating-Gradio-with-OpenCV-DNN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-yolov9/&quot;&gt;Fine-Tuning YOLOv9 on Custom Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv9-Models-Custom-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/dreambooth-using-diffusers/&quot;&gt;Dreambooth using Diffusers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Dreambooth_using_Diffusers&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/hugging-face-diffusers/&quot;&gt;Introduction to Hugging Face Diffusers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction_to_Diffusers&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ultralytics-explorer-api/&quot;&gt;Introduction to Ultralytics Explorer API&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-Ultralytics-Explorer-API&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov9-advancing-the-yolo-legacy/&quot;&gt;YOLOv9: Advancing the YOLO Legacy&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv9-Advancing-the-YOLO-Legacy&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-llms-using-peft/&quot;&gt;Fine-Tuning LLMs using PEFT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-LLMs-using-PEFT&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deciphering-llms/&quot;&gt;Depth Anything: Accelerating Monocular Depth Perception&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Depth-Anything&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deciphering-llms/&quot;&gt;Deciphering LLMs: From Transformers to Quantization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Deciphering-LLMs&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo-loss-function-gfl-vfl-loss/&quot;&gt;YOLO Loss Function Part 2: GFL and VFL Loss&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLO-Loss-Functions-Part2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov8-object-tracking-and-counting-with-opencv/&quot;&gt;YOLOv8-Object-Tracking-and-Counting-with-OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv8-Object-Tracking-and-Counting-with-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/adas-stereo-vision/&quot;&gt;Stereo Vision in ADAS: Pioneering Depth Perception Beyond LiDAR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ADAS-Stereo-Vision&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo-loss-function-siou-focal-loss/&quot;&gt;YOLO Loss Function Part 1: SIoU and Focal Loss&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLO-Loss-Functions-Part1&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/moving-object-detection-with-opencv/&quot;&gt;Moving Object Detection with OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Moving-Object-Detection-with-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/3d-lidar-object-detection/&quot;&gt;Integrating ADAS with Keypoint Feature Pyramid Network for 3D LiDAR Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.dropbox.com/scl/fi/3n1s68jtfkjmw2f5e5ctv/3D-LiDAR-Object-Detection.zip?rlkey=d8q6xvlxis4oxso4qki87omvc&amp;amp;dl=1&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mastering-all-yolo-models&quot;&gt;Mastering All YOLO Models from YOLOv1 to YOLO-NAS: Papers Explained (2024)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/intro-to-gradcam/&quot;&gt;GradCAM: Enhancing Neural Network Interpretability in the Realm of Explainable AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.dropbox.com/scl/fo/3p3sg5fnvhrvi9vp00i0w/h?rlkey=1x01uz5o7esex7p6c8r534iyn&amp;amp;dl=1&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/text-summarization-using-t5/&quot;&gt;Text Summarization using T5: Fine-Tuning and Building Gradio App&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Text-Summarization-using-T5-Fine-Tuning-and-Building-Gradio-App&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/3d-lidar-visualization/&quot;&gt;3D LiDAR Visualization using Open3D: A Case Study on 2D KITTI Depth Frames for Autonomous Driving&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/3D-LiDAR-Perception&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-t5/&quot;&gt;Fine Tuning T5: Text2Text Transfer Transformer for Building a Stack Overflow Tag Generator&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-T5-Text2Text-Transformer-for-Strack-Overflow-Tag-Generation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/segformer-fine-tuning-for-lane-detection&quot;&gt;SegFormer ðŸ¤— : Fine-Tuning for Improved Lane Detection in Autonomous Vehicles&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-SegFormer-For-Lane-Detection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-bert&quot;&gt;Fine-Tuning BERT using Hugging Face Transformers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-BERT-using-Hugging-Face-Transformers&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo-nas-pose&quot;&gt;YOLO-NAS Pose&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLO-NAS-Pose&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/bert-bidirectional-encoder-representations-from-transformers/&quot;&gt;BERT: Bidirectional Encoder Representations from Transformers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/BERT-Bidirectional-Encoder-Representations-from-Transformers&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/comparing-kerascv-yolov8-models/&quot;&gt;Comparing KerasCV YOLOv8 Models on the Global Wheat Data 2020&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Comparing-KerasCV-YOLOv8-Models-on-the-Global-Wheat-Data-2020&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/top-5-ai-papers-of-september-2023/&quot;&gt;Top 5 AI papers of September 2023&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/advanced-driver-assistance-systems/&quot;&gt;Empowering Drivers: The Rise and Role of Advanced Driver Assistance Systems&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/kerascv-deeplabv3-plus-semantic-segmentation/&quot;&gt;Semantic Segmentation using KerasCV DeepLabv3+&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Semantic-Segmentation-using-KerasCV-with-DeepLabv3-Plus&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-detection-using-kerascv-yolov8/&quot;&gt;Object Detection using KerasCV YOLOv8&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Detection-using-KerasCV-YOLOv8&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/animal-pose-estimation/&quot;&gt;Fine-tuning YOLOv8 Pose Models for Animal Pose Estimation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-YOLOv8-Pose-Models-for-Animal-Pose-Estimation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/top-5-ai-papers-of-august-2023/&quot;&gt;Top 5 AI papers of August 2023&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-trocr-training-trocr-to-recognize-curved-text/&quot;&gt;Fine Tuning TrOCR - Training TrOCR to Recognize Curved Text&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-TrOCR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/trocr-getting-started-with-transformer-based-ocr/&quot;&gt;TrOCR - Getting Started with Transformer Based OCR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TrOCR-Getting-Started-with-Transformer-Based-OCR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/facial-emotion-recognition/&quot;&gt;Facial Emotion Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Facial-Emotion-Recognition&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-keypoint-similarity/&quot;&gt;Object Keypoint Similarity in Keypoint Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Keypoint-Similarity-in-Keypoint-Detection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/real-time-deep-sort-with-torchvision-detectors/&quot;&gt;Real Time Deep SORT with Torchvision Detectors&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Real_Time_Deep_SORT_using_Torchvision_Detectors&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/top-5-ai-papers-of-july-2023/&quot;&gt;Top 5 AI papers of July 2023&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/medical-image-segmentation/&quot;&gt;Medical Image Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Medical-Image-Segmentation-Using-HuggingFace-&amp;amp;-PyTorch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/weighted-boxes-fusion/&quot;&gt;Weighted Boxes Fusion in Object Detection: A Comparison with Non-Maximum Suppression&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Weighted-Boxes-Fusion-in-Object-Detection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/medical-multi-label/&quot;&gt;Medical Multi-label Classification with PyTorch &amp;amp; Lightning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Medical_Multi-label_Classification_with_PyTorch_&amp;amp;_Lightning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/paddlepaddle/&quot;&gt;Getting Started with PaddlePaddle: Exploring Object Detection, Segmentation, and Keypoints&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-PaddlePaddle&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/drone-programming-with-computer-vision/&quot;&gt;Drone Programming With Computer Vision A Beginners Guide&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Drone-Programming-With-Computer-Vision-A-Beginners-Guide&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/building-pip-installable-package-pypi/&quot;&gt;How to Build a Pip Installable Package &amp;amp; Upload to PyPi&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/iou-loss-functions-object-detection/&quot;&gt;IoU Loss Functions for Faster &amp;amp; More Accurate Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/slicing-aided-hyper-inference/&quot;&gt;Exploring Slicing Aided Hyper Inference for Small Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Exploring-Slicing-Aided-Hyper-Inference&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/face-recognition-models/&quot;&gt;Advancements in Face Recognition Models, Toolkit and Datasets&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/train-yolo-nas-on-custom-dataset/&quot;&gt;Train YOLO NAS on Custom Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Train-YOLO-NAS-on-Custom-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/train-yolov8-instance-segmentation/&quot;&gt;Train YOLOv8 Instance Segmentation on Custom Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Train-YOLOv8-Instance-Segmentation-on-Custom-Data&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo-nas/&quot;&gt;YOLO-NAS: New Object Detection Model Beats YOLOv6 &amp;amp; YOLOv8&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLO-NAS_Introduction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/segment-anything/&quot;&gt;Segment Anything â€“ A Foundation Model for Image Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Segment-Anything-A-Foundation-Model-for-Image-Segmentation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/video-to-slides-converter-using-background-subtraction/&quot;&gt;Build a Video to Slides Converter Application using the Power of Background Estimation and Frame Differencing in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Build-a-Video-to-Slides-Converter-Application-using-the-Power-of-Background-Estimation-and-Frame-Differencing-in-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/a-closer-look-at-cvat-perfecting-your-annotations/&quot;&gt;A Closer Look at CVAT: Perfecting Your Annotations&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=yxX_0-zr-2U&amp;amp;list=PLfYPZalDvZDLvFhjuflhrxk_lLplXUqqB&quot;&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/controlnet/&quot;&gt;ControlNet - Achieving Superior Image Generation Results&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ControlNet-Achieving-Superior-Image-Generation-Results&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/instructpix2pix/&quot;&gt;InstructPix2Pix - Edit Images With Prompts&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/InstructPix2Pix-Edit-Images-With-Prompts&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-spring-gtc-2023-day-4/&quot;&gt;NVIDIA Spring GTC 2023 Day 4: Ending on a High Note with Top Moments from the Finale!&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-spring-gtc-2023-day-3-digging-deeper-into-deep-learning-semiconductors-more/&quot;&gt;NVIDIA Spring GTC 2023 Day 3: Digging deeper into Deep Learning, Semiconductors &amp;amp; more!&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-spring-gtc-2023-day-2-jensens-keynote-the-iphone-moment-of-ai-is-here/&quot;&gt;NVIDIA Spring GTC 2023 Day 2: Jensenâ€™s keynote &amp;amp; the iPhone moment of AI is here!&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-spring-gtc-2023-day-1-highlights-welcome-to-the-future/&quot;&gt;NVIDIA Spring GTC 2023 Day 1: Welcome to the future!&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-gtc-spring-2023-curtain-raiser/&quot;&gt;NVIDIA GTC Spring 2023 Curtain Raiser&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/stable-diffusion-generative-ai/&quot;&gt;Stable Diffusion - A New Paradigm in Generative AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Stable-Diffusion-A-New-Paradigm-in-Generative-AI&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/opencv-face-recognition-api/&quot;&gt;OpenCV Face Recognition â€“ Does Face Recognition Work on AI-Generated Images?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/denoising-diffusion-probabilistic-models/&quot;&gt;An In-Depth Guide to Denoising Diffusion Probabilistic Models â€“ From Theory to Implementation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Guide-to-training-DDPMs-from-Scratch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/rise-of-midjourney-ai-art/&quot;&gt;From Pixels to Paintings: The Rise of Midjourney AI Art&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mastering-dall-e-2/&quot;&gt;Mastering DALLÂ·E 2: A Breakthrough in AI Art Generation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ai-art-generation-tools/&quot;&gt;Top 10 AI Art Generation Tools using Diffusion Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/the-future-of-image-recognition-is-here-pytorch-vision-transformer/&quot;&gt;The Future of Image Recognition is Here: PyTorch Vision Transformer&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Vision_Transformer_PyTorch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/attention-mechanism-in-transformer-neural-networks/&quot;&gt;Understanding Attention Mechanism in Transformer Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Attention_Mechanism_Introduction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deploy-deep-learning-model-huggingface-spaces/&quot;&gt;Deploying a Deep Learning Model using Hugging Face Spaces and Gradio&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Deploying-a-Deep-Learning-Model-using-Hugging-Face-Spaces-and-Gradio&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/train-yolov8-on-custom-dataset/&quot;&gt;Train YOLOv8 on Custom Dataset â€“ A Complete Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Train-YOLOv8-on-Custom-Dataset-A-Complete-Tutorial&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/image-generation-using-diffusion-models/&quot;&gt;Introduction to Diffusion Models for Image Generation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-Diffusion-Models-for-Image-Generation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/building-automated-image-annotation-tool-pyopenannotate/&quot;&gt;Building An Automated Image Annotation Tool: PyOpenAnnotate&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Building-An-Automated-Image-Annotation-Tool-PyOpenAnnotate/&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ultralytics-yolov8/&quot;&gt;Ultralytics YOLOv8: State-of-the-Art YOLO Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Ultralytics-YOLOv8-State-of-the-Art-YOLO-Models&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/getting-started-with-yolov5-instance-segmentation/&quot;&gt;Getting Started with YOLOv5 Instance Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Getting-Started-with-YOLOv5-Instance-Segmentation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deeplabv3-ultimate-guide/&quot;&gt;The Ultimate Guide To DeepLabv3 - With PyTorch Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/The-ultimate-guide-to-deeplabv3&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ai-fitness-trainer-using-mediapipe/&quot;&gt;AI Fitness Trainer using MediaPipe: Squats Analysis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/AI-Fitness-Trainer-Using-MediaPipe-Analyzing-Squats&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolor-paper-explanation-inference-an-in-depth-analysis/&quot;&gt;YoloR - Paper Explanation &amp;amp; Inference -An In-Depth Analysis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YoloR-paper-explanation-analysis&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/automated-image-annotation-tool-using-opencv-python/&quot;&gt;Roadmap To an Automated Image Annotation Tool Using Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Roadmap-To-an-Automated-Image-Annotation-Tool-Using-Python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/performance-comparison-of-yolo-models/&quot;&gt;Performance Comparison of YOLO Object Detection Models â€“ An Intensive Study&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fcos-anchor-free-object-detection-explained/&quot;&gt;FCOS - Anchor Free Object Detection Explained&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FCOS-Inference-using-PyTorch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov6-custom-dataset-training/&quot;&gt;YOLOv6 Custom Dataset Training â€“ Underwater Trash Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv6-Custom-Dataset-Training-Underwater-Trash-Detection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/what-is-exif-data-in-images/&quot;&gt;What is EXIF Data in Images?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/What-is-EXIF-Data-in-Images&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/t-sne-t-distributed-stochastic-neighbor-embedding-explained/&quot;&gt;t-SNE: T-Distributed Stochastic Neighbor Embedding Explained&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/t-SNE-with-Tensorboard&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/centernet-anchor-free-object-detection-explained/&quot;&gt;CenterNet: Objects as Points â€“ Anchor-free Object Detection Explained&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/centernet-with-tf-hub&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov7-pose-vs-mediapipe-in-human-pose-estimation/&quot;&gt;YOLOv7 Pose vs MediaPipe in Human Pose Estimation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv7-Pose-vs-MediaPipe-in-Human-Pose-Estimation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov6-object-detection/&quot;&gt;YOLOv6 Object Detection â€“ Paper Explanation and Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv6-Object-Detection-Paper-Explanation-and-Inference&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolox-object-detector-paper-explanation-and-custom-training/&quot;&gt;YOLOX Object Detector Paper Explanation and Custom Training&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/driver-drowsiness-detection-using-mediapipe-in-python/&quot;&gt;Driver Drowsiness Detection Using Mediapipe In Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Driver-Drowsiness-detection-using-Mediapipe-in-Python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/gtc-2022-big-bang-ai-announcements-everything-you-need-to-know/&quot;&gt;GTC 2022 Big Bang AI announcements: Everything you need to know&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-gtc-2022-the-most-important-ai-event-this-fall/&quot;&gt;NVIDIA GTC 2022 : The most important AI event this Fall&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-tracking-and-reidentification-with-fairmot/&quot;&gt;Object Tracking and Reidentification with FairMOT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Tracking-and-Reidentification-with-FairMOT&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/what-is-face-detection-the-ultimate-guide/&quot;&gt;What is Face Detection? â€“ The Ultimate Guide for 2022&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Face-Detection-Ultimate-Guide&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/custom-document-segmentation-using-deep-learning/&quot;&gt;Document Scanner: Custom Semantic Segmentation using PyTorch-DeepLabV3&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Document-Scanner-Custom-Semantic-Segmentation-using-PyTorch-DeepLabV3&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-yolov7-on-custom-dataset/&quot;&gt;Fine Tuning YOLOv7 on Custom Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv7&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/Center-Stage-for-zoom-call-using-mediapipe/&quot;&gt;Center Stage for Zoom Calls using MediaPipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/CenterStage&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/&quot;&gt;Mean Average Precision (mAP) in Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov7-object-detection-paper-explanation-and-inference/&quot;&gt;YOLOv7 Object Detection Paper Explanation and Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv7-Object-Detection-Paper-Explanation-and-Inference&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/pothole-detection-using-yolov4-and-darknet/&quot;&gt;Pothole Detection using YOLOv4 and Darknet&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Pothole-Detection-using-YOLOv4-and-Darknet&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/automatic-document-scanner-using-opencv/&quot;&gt;Automatic Document Scanner using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Automatic-Document-Scanner&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning-part-2/&quot;&gt;Demystifying GPU architectures for deep learning: Part 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/gpu_arch_and_CUDA&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning/&quot;&gt;Demystifying GPU Architectures For Deep Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/gpu_arch_and_CUDA&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/intersection-over-unioniou-in-object-detection-and-segmentation/&quot;&gt;Intersection-over-Union(IoU)-in-Object-Detection-and-Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Intersection-over-Union-IoU-in-Object-Detection-and-Segmentation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/understanding-multiple-object-tracking-using-deepsort/&quot;&gt;Understanding Multiple Object Tracking using DeepSORT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Understanding-Multiple-Object-Tracking-using-DeepSORT&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/optical-character-recognition-using-paddleocr/&quot;&gt;Optical Character Recognition using PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Optical-Character-Recognition-using-PaddleOCR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/gesture-control-in-zoom-call-using-mediapipe/&quot;&gt;Gesture Control in Zoom Call using Mediapipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/zoom-gestures&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deep-dive-into-tensorflow-model-optimization-toolkit/&quot;&gt;A Deep Dive into Tensorflow Model Optimization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/A-Deep-Dive-into-Tensorflow-Model-Optimization&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/depthai-pipeline-overview-creating-a-complex-pipeline/&quot;&gt;DepthAI Pipeline Overview: Creating a Complex Pipeline&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OAK-DepthAi-Pipeline-Overview&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/tensorflow-lite-model-maker-create-models-for-on-device-machine-learning/&quot;&gt;TensorFlow Lite Model Maker: Create Models for On-Device Machine Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Tensorflow-Lite-Model-Maker-Create-Models-for-On-Device-ML&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/tensorflow-lite-model-optimization-for-on-device-machine-learning&quot;&gt;TensorFlow Lite: Model Optimization for On Device Machine Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Lite-Model-Optimization-for-On-Device-MachineLearning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-detection-with-depth-measurement-with-oak-d/&quot;&gt;Object detection with depth measurement using pre-trained models with OAK-D&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OAK-Object-Detection-with-Depth&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/custom-object-detection-training-using-yolov5/&quot;&gt;Custom Object Detection Training using YOLOv5&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Custom-Object-Detection-Training-using-YOLOv5&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-detection-using-yolov5-and-opencv-dnn-in-c-and-python/&quot;&gt;Object Detection using Yolov5 and OpenCV DNN (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Detection-using-YOLOv5-and-OpenCV-DNN-in-CPP-and-Python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/create-snapchat-instagram-filters-using-mediapipe/&quot;&gt;Create Snapchat/Instagram filters using Mediapipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Create-AR-filters-using-Mediapipe&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/autosar-c-compliant-deep-learning-inference-with-tensorrt/&quot;&gt;AUTOSAR C++ compliant deep learning inference with TensorRT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/industrial_cv_TensorRT_cpp&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-gtc-2022-day-4-highlights-meet-the-new-jetson-orin/&quot;&gt;NVIDIA GTC 2022 Day 4 Highlights: Meet the new Jetson Orin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-gtc-2022-day-3-highlights-deep-dive-into-hopper-architecture/&quot;&gt;NVIDIA GTC 2022 Day 3 Highlights: Deep Dive into Hopper architecture&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-gtc-2022-day-2-highlights/&quot;&gt;NVIDIA GTC 2022 Day 2 Highlights: Jensenâ€™s Keynote&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/gtc-day-1-highlights/&quot;&gt;NVIDIA GTC 2022 Day 1 Highlights: Brilliant Start&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/automatic-license-plate-recognition-using-deep-learning/&quot;&gt;Automatic License Plate Recognition using Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ALPR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/building-a-body-posture-analysis-system-using-mediapipe/&quot;&gt;Building a Poor Body Posture Detection and Alert System using MediaPipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Posture-analysis-system-using-MediaPipe-Pose&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-mediapipe/&quot;&gt;Introduction to MediaPipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-MediaPipe&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/disparity-estimation-using-deep-learning/&quot;&gt;Disparity Estimation using Deep Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Disparity-Estimation-Using-Deep-Learning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/how-to-build-chrome-dino-game-bot-using-opencv-feature-matching/&quot;&gt;How to build Chrome Dino game bot using OpenCV Feature Matching&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Chrome-Dino-Bot-using-OpenCV-feature-matching&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/top-10-sources-to-find-computer-vision-and-ai-models/&quot;&gt;Top 10 Sources to Find Computer Vision and AI Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/multi-attribute-and-graph-based-object-detection/&quot;&gt;Multi-Attribute and Graph-based Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/plastic-waste-detection-with-deep-learning/&quot;&gt;Plastic Waste Detection with Deep Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Plastic-Waste-Detection-with-Deep-Learning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ensemble-deep-learning-based-defect-classification-and-detection-in-sem-images/&quot;&gt;Ensemble Deep Learning-based Defect Classification and Detection in SEM Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/building-industrial-embedded-deep-learning-inference-pipelines-with-tensorrt/&quot;&gt;Building Industrial embedded deep learning inference pipelines with TensorRT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/industrial_cv_TensorRT_python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/transfer-learning-for-medical-images/&quot;&gt;Transfer Learning for Medical Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/stereo-vision-and-depth-estimation-using-opencv-ai-kit/&quot;&gt;Stereo Vision and Depth Estimation using OpenCV AI Kit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/oak-getting-started&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-opencv-ai-kit-and-depthai/&quot;&gt;Introduction to OpenCV AI Kit and DepthAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/oak-getting-started&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/wechat-qr-code-scanner-in-opencv&quot;&gt;WeChat QR Code Scanner in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/WeChat-QRCode-Scanner-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ai-behind-the-diwali-2021-not-just-a-cadbury-ad/&quot;&gt;AI behind the Diwali 2021 â€˜Not just a Cadbury adâ€™&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/model-selection-and-benchmarking-with-modelplace-ai/&quot;&gt;Model Selection and Benchmarking with Modelplace.AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://modelplace.ai/&quot;&gt;Model Zoo&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/real-time-style-transfer-in-a-zoom-meeting/&quot;&gt;Real-time style transfer in a zoom meeting&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/style-transfer-zoom&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-openvino-deep-learning-workbench/&quot;&gt;Introduction to OpenVino Deep Learning Workbench&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-OpenVino-Deep-Learning-Workbench&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/running-openvino-models-on-intel-integrated-gpu/&quot;&gt;Running OpenVino Models on Intel Integrated GPU&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Running-OpenVino-Models-on-Intel-Integrated-GPU&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/post-training-quantization-with-openvino-toolkit/&quot;&gt;Post Training Quantization with OpenVino Toolkit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Post-Training-Quantization-with-OpenVino-Toolkit&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-intel-openvino-toolkit/&quot;&gt;Introduction to Intel OpenVINO Toolkit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/human-action-recognition-using-detectron2-and-lstm/&quot;&gt;Human Action Recognition using Detectron2 and LSTM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Human-Action-Recognition-Using-Detectron2-And-Lstm&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/paired-image-to-image-translation-pix2pix/&quot;&gt;Pix2Pix:Image-to-Image Translation in PyTorch &amp;amp; TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Image-to-Image-Translation-with-GAN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/&quot;&gt;Conditional GAN (cGAN) in PyTorch and TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Conditional-GAN-PyTorch-TensorFlow&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deep-convolutional-gan-in-pytorch-and-tensorflow/&quot;&gt;Deep Convolutional GAN in PyTorch and TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Deep-Convolutional-GAN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-generative-adversarial-networks/&quot;&gt;Introduction to Generative Adversarial Networks (GANs)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Intro-to-Generative-Adversarial-Network&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/human-pose-estimation-using-keypoint-rcnn-in-pytorch/&quot;&gt;Human Pose Estimation using Keypoint RCNN in PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Keypoint-RCNN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch&quot;&gt;Non Maximum Suppression: Theory and Implementation in PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Non-Maximum-Suppression&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mrnet-multitask-approach/&quot;&gt;MRNet â€“ The Multi-Task Approach&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/MRnet-MultiTask-Approach&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/generative-and-discriminative-models/&quot;&gt;Generative and Discriminative Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/playing-chromes-t-rex-game-with-facial-gestures/&quot;&gt;Playing Chrome&#39;s T-Rex Game with Facial Gestures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Playing-Chrome-TRex-Game-with-Facial-Gestures&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/variational-autoencoder-in-tensorflow/&quot;&gt;Variational Autoencoder in TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Variational-Autoencoder-TensorFlow&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/autoencoder-in-tensorflow-2-beginners-guide/&quot;&gt;Autoencoder in TensorFlow 2: Beginnerâ€™s Guide&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Autoencoder-in-TensorFlow&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deep-learning-with-opencvs-dnn-module-a-definitive-guide/&quot;&gt;Deep Learning with OpenCV DNN Module: A Definitive Guide&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Deep-Learning-with-OpenCV-DNN-Module&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/depth-perception-using-stereo-camera-python-c/&quot;&gt;Depth perception using stereo camera (Python/C++)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Depth-Perception-Using-Stereo-Camera&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/contour-detection-using-opencv-python-c/&quot;&gt;Contour Detection using OpenCV (Python/C++)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Contour-Detection-using-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/super-resolution-in-opencv/&quot;&gt;Super Resolution in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/Super-Resolution-in-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/improving-illumination-in-night-time-images/&quot;&gt;Improving Illumination in Night Time Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Improving-Illumination-in-Night-Time-Images&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-video-classification-and-human-activity-recognition/&quot;&gt;Video Classification and Human Activity Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/video-classification-and-human-activity-recognition&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/how-to-use-opencv-dnn-module-with-nvidia-gpu-on-windows&quot;&gt;How to use OpenCV DNN Module with Nvidia GPU on Windows&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OpenCV-dnn-gpu-support-Windows&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/opencv-dnn-with-gpu-support/&quot;&gt;How to use OpenCV DNN Module with NVIDIA GPUs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OpenCV-dnn-gpu-support-Linux&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/code-opencv-in-visual-studio/&quot;&gt;Code OpenCV in Visual Studio&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/install-opencv-on-windows/&quot;&gt;Install OpenCV on Windows â€“ C++ / Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Install-OpenCV-Windows-exe&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/face-recognition-with-arcface/&quot;&gt;Face Recognition with ArcFace&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Face-Recognition-with-ArcFace&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/background-subtraction-with-opencv-and-bgs-libraries/&quot;&gt;Background Subtraction with OpenCV and BGS Libraries&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Background-Subtraction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/optical-flow-using-deep-learning-raft/&quot;&gt;RAFT: Optical Flow estimation using Deep Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-Estimation-using-Deep-Learning-RAFT&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/&quot;&gt;Making A Low-Cost Stereo Camera Using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/stereo-camera&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/optical-flow-in-opencv&quot;&gt;Optical Flow in OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-in-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/&quot;&gt;Introduction to Epipolar Geometry and Stereo Vision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/EpipolarGeometryAndStereoVision&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/classification-with-localization/&quot;&gt;Classification With Localization: Convert any keras Classifier to a Detector&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Classification-with-localization-convert-any-keras-classifier-into-a-detector/README.md&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/photoshop-filters-in-opencv/&quot;&gt;Photoshop Filters in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Photoshop-Filters-in-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/tetris-with-opencv-python&quot;&gt;Tetris Game using OpenCV Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Tetris&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-with-opencv-for-android/&quot;&gt;Image Classification with OpenCV for Android&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-Android&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-with-opencv-java&quot;&gt;Image Classification with OpenCV Java&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-with-Java&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/pytorch-to-tensorflow-model-conversion/&quot;&gt;PyTorch to Tensorflow Model Conversion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-TensorFlow-Model-Conversion&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/snake-game-with-opencv-python/&quot;&gt;Snake Game with OpenCV Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SnakeGame&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/stanford-mrnet-challenge-classifying-knee-mris/&quot;&gt;Stanford MRNet Challenge: Classifying Knee MRIs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/MRNet-Single-Model&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/experiment-logging-with-tensorboard-and-wandb&quot;&gt;Experiment Logging with TensorBoard and wandb&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Vision-Experiment-Logging&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/understanding-lens-distortion/&quot;&gt;Understanding Lens Distortion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/UnderstandingLensDistortion&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-matting-with-state-of-the-art-method-f-b-alpha-matting/&quot;&gt;Image Matting with state-of-the-art Method â€œF, B, Alpha Mattingâ€&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FBAMatting&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/bag-of-tricks-for-image-classification-lets-check-if-it-is-working-or-not/&quot;&gt;Bag Of Tricks For Image Classification - Let&#39;s check if it is working or not&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Bag-Of-Tricks-For-Image-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/getting-started-opencv-cuda-module/&quot;&gt;Getting Started with OpenCV CUDA Module&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Getting-Started-OpenCV-CUDA-Module&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/training-a-custom-object-detector-with-dlib-making-gesture-controlled-applications/&quot;&gt;Training a Custom Object Detector with DLIB &amp;amp; Making Gesture Controlled Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Training_a_custom_hand_detector_with_dlib&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/how-to-run-inference-using-tensorrt-c-api/&quot;&gt;How To Run Inference Using TensorRT C++ API&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT-CPP&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/using-facial-landmarks-for-overlaying-faces-with-masks/&quot;&gt;Using Facial Landmarks for Overlaying Faces with Medical Masks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FaceMaskOverlay&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/tensorboard-with-pytorch-lightning&quot;&gt;Tensorboard with PyTorch Lightning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TensorBoard-With-Pytorch-Lightning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/otsu-thresholding-with-opencv/&quot;&gt;Otsu&#39;s Thresholding with OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/otsu-method&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/pytorch-to-coreml-model-conversion/&quot;&gt;PyTorch-to-CoreML-model-conversion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-CoreML-model-conversion&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/playing-rock-paper-scissors-with-ai/&quot;&gt;Playing Rock, Paper, Scissors with AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Playing-rock-paper-scissors-with-AI&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/&quot;&gt;CNN Receptive Field Computation Using Backprop with TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/cnn-fully-convolutional-image-classification-with-tensorflow&quot;&gt;CNN Fully Convolutional Image Classification with TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Fully-Convolutional-Image-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/&quot;&gt;How to convert a model from PyTorch to TensorRT and speed up inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/efficient-image-loading/&quot;&gt;Efficient image loading&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Efficient-image-loading&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/graph-convolutional-networks-model-relations-in-data/&quot;&gt;Graph Convolutional Networks: Model Relations In Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Graph-Convolutional-Networks-Model-Relations-In-Data&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/&quot;&gt;Getting Started with Federated Learning with PyTorch and PySyft&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/creating-a-virtual-pen-and-eraser-with-opencv/&quot;&gt;Creating a Virtual Pen &amp;amp; Eraser&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Creating-a-Virtual-Pen-and-Eraser&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/getting-started-with-pytorch-lightning/&quot;&gt;Getting Started with PyTorch Lightning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Pytorch-Lightning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/multi-label-image-classification-with-pytorch-image-tagging/&quot;&gt;Multi-Label Image Classification with PyTorch: Image Tagging&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification-Image-Tagging&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/Funny-Mirrors-Using-OpenCV/&quot;&gt;Funny Mirrors Using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FunnyMirrors&quot;&gt;code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/t-sne-for-feature-visualization/&quot;&gt;t-SNE for ResNet feature visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TSNE&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/multi-label-image-classification-with-pytorch/&quot;&gt;Multi-Label Image Classification with Pytorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop/&quot;&gt;CNN Receptive Field Computation Using Backprop&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Receptive-Field-With-Backprop&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/&quot;&gt;CNN Receptive Field Computation Using Backprop with TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/augmented-reality-using-aruco-markers-in-opencv-(c++-python)/&quot;&gt;Augmented Reality using AruCo Markers in OpenCV(C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/AugmentedRealityWithArucoMarkers&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/fully-convolutional-image-classification-on-arbitrary-sized-image/&quot;&gt;Fully Convolutional Image Classification on Arbitrary Sized Image&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Fully-Convolutional-Image-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/camera-calibration-using-opencv/&quot;&gt;Camera Calibration using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/CameraCalibration&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/geometry-of-image-formation/&quot;&gt;Geometry of Image Formation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/ensuring-training-reproducibility-in-pytorch&quot;&gt;Ensuring Training Reproducibility in Pytorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/gaze-tracking/&quot;&gt;Gaze Tracking&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/simple-background-estimation-in-videos-using-opencv-c-python/&quot;&gt;Simple Background Estimation in Videos Using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/VideoBackgroundEstimation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/applications-of-foreground-background-separation-with-semantic-segmentation/&quot;&gt;Applications of Foreground-Background separation with Semantic Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/app-seperation-semseg&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/efficientnet-theory-code&quot;&gt;EfficientNet: Theory + Code&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/EfficientNet&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/mask-r-cnn-instance-segmentation-with-pytorch/&quot;&gt;PyTorch for Beginners: Mask R-CNN Instance Segmentation with PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/spmallick/learnopencv/master/PyTorch-Mask-RCNN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch&quot;&gt;PyTorch for Beginners: Faster R-CNN Object Detection with PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-faster-RCNN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/&quot;&gt;PyTorch for Beginners: Semantic Segmentation using torchvision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Segmentation-torchvision&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-using-pre-trained-models-using-pytorch/&quot;&gt;PyTorch for Beginners: Comparison of pre-trained models for Image Classification&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Image-classification-pre-trained-models/Image_Classification_using_pre_trained_models.ipynb&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/pytorch-for-beginners-basics/&quot;&gt;PyTorch for Beginners: Basics&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-for-Beginners/PyTorch_for_Beginners.ipynb&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/pytorch-model-inference-using-onnx-and-caffe2/&quot;&gt;PyTorch Model Inference using ONNX and Caffe2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Inference-for-PyTorch-Models/ONNX-Caffe2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/&quot;&gt;Image Classification Using Transfer Learning in PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Image-Classification-in-PyTorch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/hangman-creating-games-in-opencv/&quot;&gt;Hangman: Creating games in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Hangman&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-inpainting-with-opencv-c-python/&quot;&gt;Image Inpainting with OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Image-Inpainting&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/hough-transform-with-opencv-c-python/&quot;&gt;Hough Transform with OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Hough-Transform&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/&quot;&gt;Xeus-Cling: Run C++ code in Jupyter Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/XeusCling&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/&quot;&gt;Gender &amp;amp; Age Classification using OpenCV Deep Learning ( C++/Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/AgeGender&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/invisibility-cloak-using-color-detection-and-segmentation-with-opencv/&quot;&gt;Invisibility Cloak using Color Detection and Segmentation with OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/InvisibilityCloak&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/fast-image-downloader-for-open-images-v4/&quot;&gt;Fast Image Downloader for Open Images V4 (Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/downloadOpenImages&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-based-text-detection-using-opencv-c-python/&quot;&gt;Deep Learning based Text Detection Using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TextDetectionEAST&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/&quot;&gt;Video Stabilization Using Point Feature Matching in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/VideoStabilization&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/training-yolov3-deep-learning-based-custom-object-detector/&quot;&gt;Training YOLOv3 : Deep Learning based Custom Object Detector&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv3-Training-Snowman-Detector&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/using-openvino-with-opencv/&quot;&gt;Using OpenVINO with OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OpenVINO-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/duplicate-search-on-quora-dataset/&quot;&gt;Duplicate Search on Quora Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Quora-Dataset-Duplicate-Search&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/shape-matching-using-hu-moments-c-python/&quot;&gt;Shape Matching using Hu Moments (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/HuMoments&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-centos-7/&quot;&gt;Install OpenCV 4 on CentOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-centos.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-centos-7/&quot;&gt;Install OpenCV 3.4.4 on CentOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-centos.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-red-hat/&quot;&gt;Install OpenCV 3.4.4 on Red Hat (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-red-hat.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-red-hat/&quot;&gt;Install OpenCV 4 on Red Hat (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-on-red-hat.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-macos/&quot;&gt;Install OpenCV 4 on macOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/InstallScripts/installOpenCV-4-macos.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-raspberry-pi/&quot;&gt;Install OpenCV 3.4.4 on Raspberry Pi&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-raspberry-pi.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-macos/&quot;&gt;Install OpenCV 3.4.4 on macOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-macos.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/opencv-qr-code-scanner-c-and-python/&quot;&gt;OpenCV QR Code Scanner (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/QRCode-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-windows/&quot;&gt;Install OpenCV 3.4.4 on Windows (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-3&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-16-04/&quot;&gt;Install OpenCV 3.4.4 on Ubuntu 16.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-Ubuntu-16-04.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-18-04/&quot;&gt;Install OpenCV 3.4.4 on Ubuntu 18.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-Ubuntu-18-04.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/universal-sentence-encoder&quot;&gt;Universal Sentence Encoder&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/Universal-Sentence-Encoder&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-raspberry-pi/&quot;&gt;Install OpenCV 4 on Raspberry Pi&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-raspberry-pi.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-windows/&quot;&gt;Install OpenCV 4 on Windows (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-4&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/&quot;&gt;Face Detection â€“ Dlib, OpenCV, and Deep Learning ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FaceDetectionComparison&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/hand-keypoint-detection-using-deep-learning-and-opencv/&quot;&gt;Hand Keypoint Detection using Deep Learning and OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/HandPose&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/&quot;&gt;Deep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Mask-RCNN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/&quot;&gt;Install OpenCV 4 on Ubuntu 18.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-on-Ubuntu-18-04.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-ubuntu-16-04/&quot;&gt;Install OpenCV 4 on Ubuntu 16.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-on-Ubuntu-16-04.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/&quot;&gt;Multi-Person Pose Estimation in OpenCV using OpenPose&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OpenPose-Multi-Person&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/heatmap-for-logo-detection-using-opencv-python/&quot;&gt;Heatmap for Logo Detection using OpenCV (Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/heatmap&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/&quot;&gt;Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ObjectDetection-YOLO&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/&quot;&gt;Convex Hull using OpenCV in Python and C++&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ConvexHull&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/multitracker-multiple-object-tracking-using-opencv-c-python/&quot;&gt;MultiTracker : Multiple Object Tracking using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/MultiObjectTracker&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/convolutional-neural-network-based-image-colorization-using-opencv/&quot;&gt;Convolutional Neural Network based Image Colorization using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Colorization&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/svm-using-scikit-learn-in-python/&quot;&gt;SVM using scikit-learn&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/&quot;&gt;GOTURN: Deep Learning based Object Tracking&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/GOTURN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/&quot;&gt;Find the Center of a Blob (Centroid) using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/CenterofBlob&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/support-vector-machines-svm/&quot;&gt;Support Vector Machines (SVM)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/batch-normalization-in-deep-networks/&quot;&gt;Batch Normalization in Deep Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/BatchNormalization&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-character-classification-using-synthetic-dataset/&quot;&gt;Deep Learning based Character Classification using Synthetic Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/CharClassification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-quality-assessment-brisque/&quot;&gt;Image Quality Assessment : BRISQUE&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ImageMetrics&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/understanding-alexnet/&quot;&gt;Understanding AlexNet&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-based-text-recognition-ocr-using-tesseract-and-opencv/&quot;&gt;Deep Learning based Text Recognition (OCR) using Tesseract and OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OCR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/&quot;&gt;Deep Learning based Human Pose Estimation using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OpenPose&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/&quot;&gt;Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/how-to-convert-your-opencv-c-code-into-a-python-module/&quot;&gt;How to convert your OpenCV C++ code into a Python module&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/pymodule&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/cv4faces-best-project-award-2018/&quot;&gt;CV4Faces : Best Project Award 2018&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/facemark-facial-landmark-detection-using-opencv/&quot;&gt;Facemark : Facial Landmark Detection using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FacialLandmarkDetection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/&quot;&gt;Image Alignment (Feature Based) using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ImageAlignment-FeatureBased&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/barcode-and-qr-code-scanner-using-zbar-and-opencv/&quot;&gt;Barcode and QR code Scanner using ZBar and OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/barcode-QRcodeScanner&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/&quot;&gt;Keras Tutorial : Fine-tuning using pre-trained models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Keras-Fine-Tuning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/opencv-transparent-api/&quot;&gt;OpenCV Transparent API&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/face-reconstruction-using-eigenfaces-cpp-python/&quot;&gt;Face Reconstruction using EigenFaces (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ReconstructFaceUsingEigenFaces&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/eigenface-using-opencv-c-python/&quot;&gt;Eigenface using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/EigenFace&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/principal-component-analysis/&quot;&gt;Principal Component Analysis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/&quot;&gt;Keras Tutorial : Transfer Learning using pre-trained models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Keras-Transfer-Learning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/&quot;&gt;Keras Tutorial : Using pre-trained Imagenet models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Keras-ImageNet-Models&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/technical-aspects-of-a-digital-slr/&quot;&gt;Technical Aspects of a Digital SLR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/using-harry-potter-interactive-wand-with-opencv-to-create-magic/&quot;&gt;Using Harry Potter interactive wand with OpenCV to create magic&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-and-dlib-on-windows-python-only/&quot;&gt;Install OpenCV 3 and Dlib on Windows ( Python only )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras&quot;&gt;Image Classification using Convolutional Neural Networks in Keras&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/KerasCNN-CIFAR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/&quot;&gt;Understanding Autoencoders using Tensorflow (Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DenoisingAutoencoder&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/best-project-award-computer-vision-for-faces/&quot;&gt;Best Project Award : Computer Vision for Faces&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/&quot;&gt;Understanding Activation Functions in Deep Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/&quot;&gt;Image Classification using Feedforward Neural Network in Keras&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/kromydas/learnopencv/tree/master/Keras-MLP-MNIST-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/exposure-fusion-using-opencv-cpp-python/&quot;&gt;Exposure Fusion using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ExposureFusion&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/understanding-feedforward-neural-networks/&quot;&gt;Understanding Feedforward Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/high-dynamic-range-hdr-imaging-using-opencv-cpp-python&quot;&gt;High Dynamic Range (HDR) Imaging using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/hdr&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/deep-learning-using-keras-the-basics&quot;&gt;Deep learning using Keras â€“ The Basics&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/kromydas/learnopencv/tree/master/Keras-Linear-Regression&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/selective-search-for-object-detection-cpp-python/&quot;&gt;Selective Search for Object Detection (C++ / Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SelectiveSearch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/&quot;&gt;Installing Deep Learning Frameworks on Ubuntu with CUDA support&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/parallel-pixel-access-in-opencv-using-foreach/&quot;&gt;Parallel Pixel Access in OpenCV using forEach&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/forEach&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/cvui-gui-lib-built-on-top-of-opencv-drawing-primitives/&quot;&gt;cvui: A GUI lib built on top of OpenCV drawing primitives&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/UI-cvui&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-dlib-on-windows/&quot;&gt;Install Dlib on Windows&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-dlib-on-ubuntu/&quot;&gt;Install Dlib on Ubuntu&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-opencv3-on-ubuntu/&quot;&gt;Install OpenCV3 on Ubuntu&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/&quot;&gt;Read, Write and Display a video using OpenCV ( C++/ Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/VideoReadWriteDisplay&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-dlib-on-macos/&quot;&gt;Install Dlib on MacOS&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-opencv3-on-macos/&quot;&gt;Install OpenCV 3 on MacOS&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-opencv3-on-windows/&quot;&gt;Install OpenCV 3 on Windows&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/get-opencv-build-information-getbuildinformation/&quot;&gt;Get OpenCV Build Information ( getBuildInformation )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/color-spaces-in-opencv-cpp-python/&quot;&gt;Color spaces in OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ColorSpaces&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/&quot;&gt;Neural Networks : A 30,000 Feet View for Beginners&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/alpha-blending-using-opencv-cpp-python/&quot;&gt;Alpha Blending using OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/AlphaBlending&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/user-stories-how-readers-of-this-blog-are-applying-their-knowledge-to-build-applications/&quot;&gt;User stories : How readers of this blog are applying their knowledge to build applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/how-to-select-a-bounding-box-roi-in-opencv-cpp-python/&quot;&gt;How to select a bounding box ( ROI ) in OpenCV (C++/Python) ?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/automatic-red-eye-remover-using-opencv-cpp-python/&quot;&gt;Automatic Red Eye Remover using OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/RedEyeRemover&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/&quot;&gt;Bias-Variance Tradeoff in Machine Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/embedded-computer-vision-which-device-should-you-choose/&quot;&gt;Embedded Computer Vision: Which device should you choose?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/object-tracking-using-opencv-cpp-python/&quot;&gt;Object Tracking using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/tracking&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/&quot;&gt;Handwritten Digits Classification : An OpenCV ( C++ / Python ) Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/digits-classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv/&quot;&gt;Training a better Haar and LBP cascade based Eye Detector using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/deep-learning-book-gift-recipients/&quot;&gt;Deep Learning Book Gift Recipients&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/minified-opencv-haar-and-lbp-cascades/&quot;&gt;Minified OpenCV Haar and LBP Cascades&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ninjaEyeDetector&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/deep-learning-book-gift/&quot;&gt;Deep Learning Book Gift&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/histogram-of-oriented-gradients/&quot;&gt;Histogram of Oriented Gradients&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/image-recognition-and-object-detection-part1/&quot;&gt;Image Recognition and Object Detection : Part 1&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/&quot;&gt;Head Pose Estimation using OpenCV and Dlib&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/HeadPose&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/live-cv/&quot;&gt;Live CV : A Computer Vision Coding Application&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/&quot;&gt;Approximate Focal Length for Webcams and Cell Phone Cameras&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/configuring-qt-for-opencv-on-osx/&quot;&gt;Configuring Qt for OpenCV on OSX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/qt-test&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/rotation-matrix-to-euler-angles/&quot;&gt;Rotation Matrix To Euler Angles&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/RotationMatrixToEulerAngles&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/speeding-up-dlib-facial-landmark-detector/&quot;&gt;Speeding up Dlibâ€™s Facial Landmark Detector&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/warp-one-triangle-to-another-using-opencv-c-python/&quot;&gt;Warp one triangle to another using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/WarpTriangle&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/average-face-opencv-c-python-tutorial/&quot;&gt;Average Face : OpenCV ( C++ / Python ) Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FaceAverage&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/face-swap-using-opencv-c-python/&quot;&gt;Face Swap using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FaceSwap&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/face-morph-using-opencv-cpp-python/&quot;&gt;Face Morph Using OpenCV â€” C++ / Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FaceMorph&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/deep-learning-example-using-nvidia-digits-3-on-ec2/&quot;&gt;Deep Learning Example using NVIDIA DIGITS 3 on EC2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/nvidia-digits-3-on-ec2/&quot;&gt;NVIDIA DIGITS 3 on EC2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/homography-examples-using-opencv-python-c/&quot;&gt;Homography Examples using OpenCV ( Python / C ++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Homography&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/filling-holes-in-an-image-using-opencv-python-c/&quot;&gt;Filling holes in an image using OpenCV ( Python / C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Holes&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/how-to-find-frame-rate-or-frames-per-second-fps-in-opencv-python-cpp/&quot;&gt;How to find frame rate or frames per second (fps) in OpenCV ( Python / C++ ) ?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FPS&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/delaunay-triangulation-and-voronoi-diagram-using-opencv-c-python/&quot;&gt;Delaunay Triangulation and Voronoi Diagram using OpenCV ( C++ / Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Delaunay&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/opencv-c-vs-python-vs-matlab-for-computer-vision/&quot;&gt;OpenCV (C++ vs Python) vs MATLAB for Computer Vision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/facial-landmark-detection/&quot;&gt;Facial Landmark Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/why-does-opencv-use-bgr-color-format/&quot;&gt;Why does OpenCV use BGR color format ?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/computer-vision-for-predicting-facial-attractiveness/&quot;&gt;Computer Vision for Predicting Facial Attractiveness&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FacialAttractiveness&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/&quot;&gt;applyColorMap for pseudocoloring in OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Colormap&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/image-alignment-ecc-in-opencv-c-python/&quot;&gt;Image Alignment (ECC) in OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ImageAlignment&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/how-to-find-opencv-version-python-cpp/&quot;&gt;How to find OpenCV version in Python and C++ ?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/baidu-banned-from-ilsvrc-2015/&quot;&gt;Baidu banned from ILSVRC 2015&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/opencv-transparent-api/&quot;&gt;OpenCV Transparent API&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/how-computer-vision-solved-the-greatest-soccer-mystery-of-all-times/&quot;&gt;How Computer Vision Solved the Greatest Soccer Mystery of All Time&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/embedded-vision-summit-2015/&quot;&gt;Embedded Vision Summit 2015&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/read-an-image-in-opencv-python-cpp/&quot;&gt;Read an Image in OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/imread&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/&quot;&gt;Non-Photorealistic Rendering using OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/NonPhotorealisticRendering&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp/&quot;&gt;Seamless Cloning using OpenCV ( Python , C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SeamlessCloning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/opencv-threshold-python-cpp/&quot;&gt;OpenCV Threshold ( Python , C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Threshold&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/blob-detection-using-opencv-python-c/&quot;&gt;Blob Detection Using OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/BlobDetector&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/turn-your-opencv-Code-into-a-web-api-in-under-10-minutes-part-1/&quot;&gt;Turn your OpenCV Code into a Web API in under 10 minutes â€” Part 1&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/how-to-compile-opencv-sample-Code/&quot;&gt;How to compile OpenCV sample Code ?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-opencv-3-on-yosemite-osx-10-10-x/&quot;&gt;Install OpenCV 3 on Yosemite ( OSX 10.10.x )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
  </channel>
</rss>
