<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Jupyter Notebook Weekly Trending</title>
    <description>Weekly Trending of Jupyter Notebook in GitHub</description>
    <pubDate>Wed, 13 Aug 2025 01:44:15 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>rasbt/LLMs-from-scratch</title>
      <link>https://github.com/rasbt/LLMs-from-scratch</link>
      <description>&lt;p&gt;Implement a ChatGPT-like LLM in PyTorch from scratch, step by step&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Build a Large Language Model (From Scratch)&lt;/h1&gt; 
&lt;p&gt;This repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book &lt;a href=&quot;https://amzn.to/4fqvn0D&quot;&gt;Build a Large Language Model (From Scratch)&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href=&quot;https://amzn.to/4fqvn0D&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123&quot; width=&quot;250px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;In &lt;a href=&quot;http://mng.bz/orYv&quot;&gt;&lt;em&gt;Build a Large Language Model (From Scratch)&lt;/em&gt;&lt;/a&gt;, you&#39;ll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I&#39;ll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.&lt;/p&gt; 
&lt;p&gt;The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Link to the official &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;source code repository&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://mng.bz/orYv&quot;&gt;Link to the book at Manning (the publisher&#39;s website)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/gp/product/1633437167&quot;&gt;Link to the book page on Amazon.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ISBN 9781633437166&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;http://mng.bz/orYv#reviews&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com//images/LLMs-from-scratch-images/other/reviews.png&quot; width=&quot;220px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;To download a copy of this repository, click on the &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/archive/refs/heads/main.zip&quot;&gt;Download ZIP&lt;/a&gt; button or execute the following command in your terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone --depth 1 https://github.com/rasbt/LLMs-from-scratch.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;p&gt;(If you downloaded the code bundle from the Manning website, please consider visiting the official code repository on GitHub at &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;https://github.com/rasbt/LLMs-from-scratch&lt;/a&gt; for the latest updates.)&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h1&gt;Table of Contents&lt;/h1&gt; 
&lt;p&gt;Please note that this &lt;code&gt;README.md&lt;/code&gt; file is a Markdown (&lt;code&gt;.md&lt;/code&gt;) file. If you have downloaded this code bundle from the Manning website and are viewing it on your local computer, I recommend using a Markdown editor or previewer for proper viewing. If you haven&#39;t installed a Markdown editor yet, &lt;a href=&quot;https://ghostwriter.kde.org&quot;&gt;Ghostwriter&lt;/a&gt; is a good free option.&lt;/p&gt; 
&lt;p&gt;You can alternatively view this and other files on GitHub at &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;https://github.com/rasbt/LLMs-from-scratch&lt;/a&gt; in your browser, which renders Markdown automatically.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; If you&#39;re seeking guidance on installing Python and Python packages and setting up your code environment, I suggest reading the &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/README.md&quot;&gt;README.md&lt;/a&gt; file located in the &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup&quot;&gt;setup&lt;/a&gt; directory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests Linux&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests Windows&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests macOS&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Chapter Title&lt;/th&gt; 
   &lt;th&gt;Main Code (for Quick Access)&lt;/th&gt; 
   &lt;th&gt;All Code + Supplementary&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup&quot;&gt;Setup recommendations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 1: Understanding Large Language Models&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 2: Working with Text Data&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/ch02.ipynb&quot;&gt;ch02.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/dataloader.ipynb&quot;&gt;dataloader.ipynb&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02&quot;&gt;./ch02&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 3: Coding Attention Mechanisms&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/ch03.ipynb&quot;&gt;ch03.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/multihead-attention.ipynb&quot;&gt;multihead-attention.ipynb&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03&quot;&gt;./ch03&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 4: Implementing a GPT Model from Scratch&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/ch04.ipynb&quot;&gt;ch04.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/gpt.py&quot;&gt;gpt.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04&quot;&gt;./ch04&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 5: Pretraining on Unlabeled Data&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/ch05.ipynb&quot;&gt;ch05.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_train.py&quot;&gt;gpt_train.py&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_generate.py&quot;&gt;gpt_generate.py&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05&quot;&gt;./ch05&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 6: Finetuning for Text Classification&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/ch06.ipynb&quot;&gt;ch06.ipynb&lt;/a&gt; &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/gpt_class_finetune.py&quot;&gt;gpt_class_finetune.py&lt;/a&gt; &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06&quot;&gt;./ch06&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 7: Finetuning to Follow Instructions&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/ch07.ipynb&quot;&gt;ch07.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py&quot;&gt;gpt_instruction_finetuning.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/ollama_evaluate.py&quot;&gt;ollama_evaluate.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07&quot;&gt;./ch07&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix A: Introduction to PyTorch&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/code-part1.ipynb&quot;&gt;code-part1.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/code-part2.ipynb&quot;&gt;code-part2.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/DDP-script.py&quot;&gt;DDP-script.py&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A&quot;&gt;./appendix-A&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix B: References and Further Reading&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix C: Exercise Solutions&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix D: Adding Bells and Whistles to the Training Loop&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-D/01_main-chapter-code/appendix-D.ipynb&quot;&gt;appendix-D.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-D&quot;&gt;./appendix-D&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix E: Parameter-efficient Finetuning with LoRA&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-E/01_main-chapter-code/appendix-E.ipynb&quot;&gt;appendix-E.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-E&quot;&gt;./appendix-E&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;p&gt;The mental model below summarizes the contents covered in this book.&lt;/p&gt; 
&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/mental-model.jpg&quot; width=&quot;650px&quot; /&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;The most important prerequisite is a strong foundation in Python programming. With this knowledge, you will be well prepared to explore the fascinating world of LLMs and understand the concepts and code examples presented in this book.&lt;/p&gt; 
&lt;p&gt;If you have some experience with deep neural networks, you may find certain concepts more familiar, as LLMs are built upon these architectures.&lt;/p&gt; 
&lt;p&gt;This book uses PyTorch to implement the code from scratch without using any external LLM libraries. While proficiency in PyTorch is not a prerequisite, familiarity with PyTorch basics is certainly useful. If you are new to PyTorch, Appendix A provides a concise introduction to PyTorch. Alternatively, you may find my book, &lt;a href=&quot;https://sebastianraschka.com/teaching/pytorch-1h/&quot;&gt;PyTorch in One Hour: From Tensors to Training Neural Networks on Multiple GPUs&lt;/a&gt;, helpful for learning about the essentials.&lt;/p&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;p&gt;The code in the main chapters of this book is designed to run on conventional laptops within a reasonable timeframe and does not require specialized hardware. This approach ensures that a wide audience can engage with the material. Additionally, the code automatically utilizes GPUs if they are available. (Please see the &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/raw/main/setup/README.md&quot;&gt;setup&lt;/a&gt; doc for additional recommendations.)&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Video Course&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/livevideo/master-and-build-large-language-models&quot;&gt;A 17-hour and 15-minute companion video course&lt;/a&gt; where I code through each chapter of the book. The course is organized into chapters and sections that mirror the book&#39;s structure so that it can be used as a standalone alternative to the book or complementary code-along resource.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/livevideo/master-and-build-large-language-models&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/video-screenshot.webp?123&quot; width=&quot;350px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Exercises&lt;/h2&gt; 
&lt;p&gt;Each chapter of the book includes several exercises. The solutions are summarized in Appendix C, and the corresponding code notebooks are available in the main chapter folders of this repository (for example, &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;./ch02/01_main-chapter-code/exercise-solutions.ipynb&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In addition to the code exercises, you can download a free 170-page PDF titled &lt;a href=&quot;https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch&quot;&gt;Test Yourself On Build a Large Language Model (From Scratch)&lt;/a&gt; from the Manning website. It contains approximately 30 quiz questions and solutions per chapter to help you test your understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/test-yourself-cover.jpg?123&quot; width=&quot;150px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Bonus Material&lt;/h2&gt; 
&lt;p&gt;Several folders contain optional materials as a bonus for interested readers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Setup&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/01_optional-python-setup-preferences&quot;&gt;Python Setup Tips&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/02_installing-python-libraries&quot;&gt;Installing Python Packages and Libraries Used In This Book&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/03_optional-docker-environment&quot;&gt;Docker Environment Setup Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 2: Working with text data&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb&quot;&gt;Byte Pair Encoding (BPE) Tokenizer From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/02_bonus_bytepair-encoder&quot;&gt;Comparing Various Byte Pair Encoding (BPE) Implementations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/03_bonus_embedding-vs-matmul&quot;&gt;Understanding the Difference Between Embedding Layers and Linear Layers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/04_bonus_dataloader-intuition&quot;&gt;Dataloader Intuition with Simple Numbers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 3: Coding attention mechanisms&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/02_bonus_efficient-multihead-attention/mha-implementations.ipynb&quot;&gt;Comparing Efficient Multi-Head Attention Implementations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/03_understanding-buffers/understanding-buffers.ipynb&quot;&gt;Understanding PyTorch Buffers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 4: Implementing a GPT model from scratch&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/02_performance-analysis/flops-analysis.ipynb&quot;&gt;FLOPS Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/03_kv-cache&quot;&gt;KV Cache&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 5: Pretraining on unlabeled data:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/02_alternative_weight_loading/&quot;&gt;Alternative Weight Loading Methods&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/03_bonus_pretraining_on_gutenberg&quot;&gt;Pretraining GPT on the Project Gutenberg Dataset&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/04_learning_rate_schedulers&quot;&gt;Adding Bells and Whistles to the Training Loop&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/05_bonus_hparam_tuning&quot;&gt;Optimizing Hyperparameters for Pretraining&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/06_user_interface&quot;&gt;Building a User Interface to Interact With the Pretrained LLM&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/07_gpt_to_llama&quot;&gt;Converting GPT to Llama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/07_gpt_to_llama/standalone-llama32.ipynb&quot;&gt;Llama 3.2 From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/11_qwen3/&quot;&gt;Qwen3 Dense and Mixture-of-Experts (MoE) From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb&quot;&gt;Memory-efficient Model Weight Loading&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/09_extending-tokenizers/extend-tiktoken.ipynb&quot;&gt;Extending the Tiktoken BPE Tokenizer with New Tokens&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/10_llm-training-speed&quot;&gt;PyTorch Performance Tips for Faster LLM Training&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 6: Finetuning for classification&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/02_bonus_additional-experiments&quot;&gt;Additional experiments finetuning different layers and using larger models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/03_bonus_imdb-classification&quot;&gt;Finetuning different models on 50k IMDB movie review dataset&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/04_user_interface&quot;&gt;Building a User Interface to Interact With the GPT-based Spam Classifier&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 7: Finetuning to follow instructions&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/02_dataset-utilities&quot;&gt;Dataset Utilities for Finding Near Duplicates and Creating Passive Voice Entries&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/03_model-evaluation&quot;&gt;Evaluating Instruction Responses Using the OpenAI API and Ollama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/05_dataset-generation/llama3-ollama.ipynb&quot;&gt;Generating a Dataset for Instruction Finetuning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/05_dataset-generation/reflection-gpt4.ipynb&quot;&gt;Improving a Dataset for Instruction Finetuning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/04_preference-tuning-with-dpo/create-preference-data-ollama.ipynb&quot;&gt;Generating a Preference Dataset with Llama 3.1 70B and Ollama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb&quot;&gt;Direct Preference Optimization (DPO) for LLM Alignment&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/06_user_interface&quot;&gt;Building a User Interface to Interact With the Instruction Finetuned GPT Model&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Questions, Feedback, and Contributing to This Repository&lt;/h2&gt; 
&lt;p&gt;I welcome all sorts of feedback, best shared via the &lt;a href=&quot;https://livebook.manning.com/forum?product=raschka&amp;amp;page=1&quot;&gt;Manning Forum&lt;/a&gt; or &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/discussions&quot;&gt;GitHub Discussions&lt;/a&gt;. Likewise, if you have any questions or just want to bounce ideas off others, please don&#39;t hesitate to post these in the forum as well.&lt;/p&gt; 
&lt;p&gt;Please note that since this repository contains the code corresponding to a print book, I currently cannot accept contributions that would extend the contents of the main chapter code, as it would introduce deviations from the physical book. Keeping it consistent helps ensure a smooth experience for everyone.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this book or code useful for your research, please consider citing it.&lt;/p&gt; 
&lt;p&gt;Chicago-style citation:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Raschka, Sebastian. &lt;em&gt;Build A Large Language Model (From Scratch)&lt;/em&gt;. Manning, 2024. ISBN: 978-1633437166.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;BibTeX entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@book{build-llms-from-scratch-book,
  author       = {Sebastian Raschka},
  title        = {Build A Large Language Model (From Scratch)},
  publisher    = {Manning},
  year         = {2024},
  isbn         = {978-1633437166},
  url          = {https://www.manning.com/books/build-a-large-language-model-from-scratch},
  github       = {https://github.com/rasbt/LLMs-from-scratch}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>patchy631/ai-engineering-hub</title>
      <link>https://github.com/patchy631/ai-engineering-hub</link>
      <description>&lt;p&gt;In-depth tutorials on LLMs, RAGs and real-world AI agent applications.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/12800&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/assets/TRENDING-BADGE.png&quot; alt=&quot;Trending Badge&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/assets/ai-eng-hub.gif&quot; alt=&quot;AI Engineering Hub Banner&quot; /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;AI Engineering Hub 🚀&lt;/h1&gt; 
&lt;p&gt;Welcome to the &lt;strong&gt;AI Engineering Hub&lt;/strong&gt;!&lt;/p&gt; 
&lt;h2&gt;🌟 Why This Repo?&lt;/h2&gt; 
&lt;p&gt;AI Engineering is advancing rapidly, and staying at the forefront requires both deep understanding and hands-on experience. Here, you will find:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In-depth tutorials on &lt;strong&gt;LLMs and RAGs&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Real-world &lt;strong&gt;AI agent&lt;/strong&gt; applications&lt;/li&gt; 
 &lt;li&gt;Examples to implement, adapt, and scale in your projects&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Whether you’re a beginner, practitioner, or researcher, this repo provides resources for all skill levels to experiment and succeed in AI engineering.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📬 Stay Updated with Our Newsletter!&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Get a FREE Data Science eBook&lt;/strong&gt; 📖 with 150+ essential lessons in Data Science when you subscribe to our newsletter! Stay in the loop with the latest tutorials, insights, and exclusive resources. &lt;a href=&quot;https://join.dailydoseofds.com&quot;&gt;Subscribe now!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://join.dailydoseofds.com&quot;&gt;&lt;img src=&quot;https://github.com/patchy631/ai-engineering/raw/main/resources/join_ddods.png&quot; alt=&quot;Daily Dose of Data Science Newsletter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📢 Contribute to the AI Engineering Hub!&lt;/h2&gt; 
&lt;p&gt;We welcome contributors! Whether you want to add new tutorials, improve existing code, or report issues, your contributions make this community thrive. Here’s how to get involved:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Fork&lt;/strong&gt; the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch for your contribution.&lt;/li&gt; 
 &lt;li&gt;Submit a &lt;strong&gt;Pull Request&lt;/strong&gt; and describe the improvements.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📜 License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the MIT License - see the &lt;a href=&quot;https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;💬 Connect&lt;/h2&gt; 
&lt;p&gt;For discussions, suggestions, and more, feel free to &lt;a href=&quot;https://github.com/patchy631/ai-engineering/issues&quot;&gt;create an issue&lt;/a&gt; or reach out directly!&lt;/p&gt; 
&lt;p&gt;Happy Coding! 🎉&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>udlbook/udlbook</title>
      <link>https://github.com/udlbook/udlbook</link>
      <description>&lt;p&gt;Understanding Deep Learning - Simon J.D. Prince&lt;/p&gt;&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>chiphuyen/aie-book</title>
      <link>https://github.com/chiphuyen/aie-book</link>
      <description>&lt;p&gt;[WIP] Resources for AI engineers. Also contains supporting materials for the book AI Engineering (Chip Huyen, 2025)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Engineering book and other resources&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;This repo will be updated with more resources in the next few weeks.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/#about-the-book&quot;&gt;About the book AI Engineering&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/ToC.md&quot;&gt;Table of contents&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/chapter-summaries.md&quot;&gt;Chapter summaries&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/study-notes.md&quot;&gt;Study notes&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/resources.md&quot;&gt;AI engineering resources&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/prompt-examples.md&quot;&gt;Prompt examples&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/case-studies.md&quot;&gt;Case studies&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/misalignment.md&quot;&gt;Misalignment AI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/appendix.md&quot;&gt;Appendix&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Fun tools:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/scripts/ai-heatmap.ipynb&quot;&gt;ChatGPT and Claude conversation heatmap generator&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;And more ...&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About the book&lt;/h2&gt; 
&lt;p&gt;The availability of foundation models has transformed AI from a specialized discipline into a powerful development tool everyone can use. This book covers the end-to-end process of adapting foundation models to solve real-world problems, encompassing tried-and-true techniques from other engineering fields and techniques emerging with foundation models.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://amzn.to/49j1cGS&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/assets/aie-cover.png&quot; width=&quot;250&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;https://amzn.to/49j1cGS&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chiphuyen/aie-book/main/assets/aie-cover-back.png&quot; width=&quot;250&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The book is available on:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://amzn.to/49j1cGS&quot;&gt;Amazon&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://oreillymedia.pxf.io/c/5719111/2146021/15173&quot;&gt;O&#39;Reilly&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://amzn.to/3Vq2ryu&quot;&gt;Kindle&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;and most places where technical books are sold.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;This is NOT a tutorial book, so it doesn&#39;t have a lot of code snippets.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;What this book is about&lt;/h2&gt; 
&lt;p&gt;This book provides a framework for adapting foundation models, which include both large language models (LLMs) and large multimodal models (LMMs), to specific applications. It not only outlines various solutions for building an AI application but also raises questions you can ask to evaluate the best solution for your needs. Here are just some of the many questions that this book can help you answer:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Should I build this AI application?&lt;/li&gt; 
 &lt;li&gt;How do I evaluate my application? Can I use AI to evaluate AI outputs?&lt;/li&gt; 
 &lt;li&gt;What causes hallucinations? How do I detect and mitigate hallucinations?&lt;/li&gt; 
 &lt;li&gt;What are the best practices for prompt engineering?&lt;/li&gt; 
 &lt;li&gt;Why does RAG work? What are the strategies for doing RAG?&lt;/li&gt; 
 &lt;li&gt;What’s an agent? How do I build and evaluate an agent?&lt;/li&gt; 
 &lt;li&gt;When to finetune a model? When not to finetune a model?&lt;/li&gt; 
 &lt;li&gt;How much data do I need? How do I validate the quality of my data?&lt;/li&gt; 
 &lt;li&gt;How do I make my model faster, cheaper, and secure?&lt;/li&gt; 
 &lt;li&gt;How do I create a feedback loop to improve my application continually?&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The book will also help you navigate the overwhelming AI landscape: types of models, evaluation benchmarks, and a seemingly infinite number of use cases and application patterns.&lt;/p&gt; 
&lt;p&gt;The content in this book is illustrated using actual case studies, many of which I’ve worked on, backed by ample references and extensively reviewed by experts from a wide range of backgrounds. Even though the book took two years to write, it draws from my experience working with language models and ML systems from the last decade.&lt;/p&gt; 
&lt;p&gt;Like my previous book, &lt;em&gt;&lt;a href=&quot;https://amzn.to/4fXVZH2&quot;&gt;Designing Machine Learning Systems (DMLS)&lt;/a&gt;&lt;/em&gt;, this book focuses on the fundamentals of AI engineering instead of any specific tool or API. Tools become outdated quickly, but fundamentals should last longer.&lt;/p&gt; 
&lt;h3&gt;Reading &lt;em&gt;AI Engineering&lt;/em&gt; (AIE) with &lt;em&gt;Designing Machine Learning Systems&lt;/em&gt; (DMLS)&lt;/h3&gt; 
&lt;p&gt;AIE can be a companion to DMLS. DMLS focuses on building applications on top of traditional ML models, which involves more tabular data annotations, feature engineering, and model training. AIE focuses on building applications on top of foundation models, which involves more prompt engineering, context construction, and parameter-efficient finetuning. Both books are self-contained and modular, so you can read either book independently.&lt;/p&gt; 
&lt;p&gt;Since foundation models are ML models, some concepts are relevant to working with both. If a topic is relevant to AIE but has been discussed extensively in DMLS, it’ll still be covered in this book, but to a lesser extent, with pointers to relevant resources.&lt;/p&gt; 
&lt;p&gt;Note that many topics are covered in DMLS but not in AIE, and vice versa. The first chapter of this book also covers the differences between traditional ML engineering and AI engineering.&lt;/p&gt; 
&lt;p&gt;A real-world system often involves both traditional ML models and foundation models, so knowledge about working with both is often necessary.&lt;/p&gt; 
&lt;h2&gt;Who this book is for&lt;/h2&gt; 
&lt;p&gt;This book is for anyone who wants to leverage foundation models to solve real-world problems. This is a technical book, so the language of this book is geared towards technical roles, including AI engineers, ML engineers, data scientists, engineering managers, and technical product managers. This book is for you if you can relate to one of the following scenarios:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You’re building or optimizing an AI application, whether you’re starting from scratch or looking to move beyond the demo phase into a production-ready stage. You may also be facing issues like hallucinations, security, latency, or costs, and need targeted solutions.&lt;/li&gt; 
 &lt;li&gt;You want to streamline your team’s AI development process, making it more systematic, faster, and reliable.&lt;/li&gt; 
 &lt;li&gt;You want to understand how your organization can leverage foundation models to improve the business’s bottom line and how to build a team to do so.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can also benefit from the book if you belong to one of the following groups:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Tool developers who want to identify underserved areas in AI engineering to position your products in the ecosystem.&lt;/li&gt; 
 &lt;li&gt;Researchers who want to understand better AI use cases.&lt;/li&gt; 
 &lt;li&gt;Job candidates seeking clarity on the skills needed to pursue a career as an AI engineer.&lt;/li&gt; 
 &lt;li&gt;Anyone wanting to better understand AI&#39;s capabilities and limitations, and how it might affect different roles.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;I love getting to the bottom of things, so some sections dive a bit deeper into the technical side. While many early readers like the detail, I know it might not be for everyone. I’ll give you a heads-up before things get too technical. Feel free to skip ahead if it feels a little too in the weeds!&lt;/p&gt; 
&lt;h2&gt;Reviews&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;This book offers a comprehensive, well-structured guide to the essential aspects of building generative AI systems. A must-read for any professional looking to scale AI across the enterprise.&quot;&lt;/em&gt; - Vittorio Cretella, former global CIO at P&amp;amp;G and Mars&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;Chip Huyen gets generative AI. She is a remarkable teacher and writer whose work has been instrumental in helping teams bring AI into production. Drawing on her deep expertise, AI Engineering is a comprehensive and holistic guide to building generative AI applications in production.&quot;&lt;/em&gt; - Luke Metz, co-creator of ChatGPT, ex-research manager @ OpenAI&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;Every AI engineer building real-world applications should read this book. It’s a vital guide to end-to-end AI system design, from model development and evaluation to large-scale deployment and operation.&quot;&lt;/em&gt; - Andrei Lopatenko, Director Search and AI, Neuron7&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;This book serves as an essential guide for building AI products that can scale. Unlike other books that focus on tools or current trends that are constantly changing, Chip delivers timeless foundational knowledge. Whether you&#39;re a product manager or an engineer, this book effectively bridges the collaboration gap between cross-functional teams, making it a must-read for anyone involved in AI development.&quot;&lt;/em&gt; - Aileen Bui, AI Product Operations Manager, Google&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;This is the definitive segue into AI Engineering from one of the greats of ML Engineering! Chip has seen through successful projects and careers at every stage of a company and for the first time ever condensed her expertise for new AI Engineers entering the field.&quot;&lt;/em&gt; - swyx, Curator, AI.Engineer&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;AI Engineering is a practical guide that provides the most up-to-date information on AI development, making it approachable for novice and expert leaders alike. This book is an essential resource for anyone looking to build robust and scalable AI systems.&quot;&lt;/em&gt; - Vicki Reyzelman, Chief AI Solutions Architect, Mave Sparks&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;AI Engineering is a comprehensive guide that serves as an essential reference for both understanding and implementing AI systems in practice.&quot;&lt;/em&gt; - Han Lee, Director - Data Science, Moody&#39;s.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;em&gt;&quot;AI Engineering is an essential guide for anyone building software with Generative AI! It demystifies the technology, highlights the importance of evaluation, and shares what should be done to achieve quality before starting with costly fine-tuning.&quot;&lt;/em&gt; - Rafal Kawala, Senior AI Engineering Director, 16 years of experience working in a Fortune 500 company&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See what people are talking about the book on Twitter &lt;a href=&quot;https://twitter.com/aisysbooks/likes&quot;&gt;@aisysbooks&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Acknowledgments&lt;/h2&gt; 
&lt;p&gt;This book would&#39;ve taken a lot longer to write and missed many important topics if it wasn&#39;t for so many wonderful people who helped me through the process.&lt;/p&gt; 
&lt;p&gt;Because the timeline for the project was tight—two years for a 150,000-word book that covers so much ground—I&#39;m grateful to the technical reviewers who put aside their precious time to review this book so quickly.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://x.com/luke_metz&quot;&gt;Luke Metz&lt;/a&gt; is an amazing soundboard who checked my assumptions and prevented me from going down the wrong path. &lt;a href=&quot;https://www.linkedin.com/in/hanchunglee/&quot;&gt;Han-chung Lee&lt;/a&gt;, always up to date with the latest AI news and community development, pointed me toward resources that I missed. Luke and Han were the first to review my drafts before I sent them to the next round of technical reviewers, and I&#39;m forever indebted to them for tolerating my follies and mistakes.&lt;/p&gt; 
&lt;p&gt;Having led AI innovation at Fortune 500 companies, &lt;a href=&quot;https://www.linkedin.com/in/vittorio-cretella/&quot;&gt;Vittorio Cretella&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/lopatenko/&quot;&gt;Andrei Lopatenko&lt;/a&gt; provided invaluable feedback that combined deep technical expertise with executive insights. &lt;a href=&quot;https://www.linkedin.com/in/vickireyzelman/&quot;&gt;Vicki Reyzelman&lt;/a&gt; helped me ground my content and keep it relevant for readers with a software engineering background.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://eugeneyan.com/&quot;&gt;Eugene Yan&lt;/a&gt;, a dear friend and amazing applied scientist, provided me with technical and emotional support. Shawn Wang (&lt;a href=&quot;https://x.com/swyx&quot;&gt;swyx&lt;/a&gt;), provided an important vibe check that helped me feel more confident about the book. &lt;a href=&quot;https://x.com/bhutanisanyam1&quot;&gt;Sanyam Bhutani&lt;/a&gt; is one of the best learners and most humble souls I know, who not only gave thoughtful written feedback but also recorded videos to explain his feedback.&lt;/p&gt; 
&lt;p&gt;Kyle Krannen is a star deep learning lead who interviewed his colleagues and shared with me an amazing writeup about their finetuning process, which guided the finetuning chapter. &lt;a href=&quot;https://x.com/marksaroufim&quot;&gt;Mark Saroufim&lt;/a&gt;, an inquisitive mind who always has his pulse on the most interesting problems, introduced me to great resources on efficiency. Both Kyle and Mark&#39;s feedback was critical in writing Chapters 7 and 9.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/kittipat-bot-kampa-1b1965/&quot;&gt;Kittipat &quot;Bot&quot; Kampa&lt;/a&gt;, on top of answering my many questions, shared with me a detailed visualization of how he thinks about AI platform. I appreciate &lt;a href=&quot;https://www.linkedin.com/in/denyslinkov/&quot;&gt;Denys Linkov&lt;/a&gt;&#39;s systematic approach to evaluation and platform development. &lt;a href=&quot;https://www.linkedin.com/in/chetantekur/&quot;&gt;Chetan Tekur&lt;/a&gt; gave great examples that helped me structure AI application patterns. I&#39;d also like to thank &lt;a href=&quot;https://www.linkedin.com/in/findalexli/&quot;&gt;Alex (Shengzhi Li) Li&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/hienluu/&quot;&gt;Hien Luu&lt;/a&gt; for their thoughtful feedback on my draft on AI architecture.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/aileenbui/&quot;&gt;Aileen Bui&lt;/a&gt; is a treasure who shared unique feedback and examples from a product manager&#39;s perspective. Thanks &lt;a href=&quot;https://www.linkedin.com/in/todor-markov-4aa38a67/&quot;&gt;Todor Markov&lt;/a&gt; for the actionable advice on the RAG and Agents chapter. Thanks &lt;a href=&quot;https://www.linkedin.com/in/tal-kachman/&quot;&gt;Tal Kachman&lt;/a&gt; for jumping in at the last minute to push the finetuning chapter over the finish line.&lt;/p&gt; 
&lt;p&gt;There are so many wonderful people whose company and conversations gave me ideas that guide the content of this book. I tried my best to include the names of everyone who has helped me here, but due to the inherent faultiness of human memory, I undoubtedly neglected to mention many. If I forgot to include your name, please know that it wasn&#39;t because I don&#39;t appreciate your contribution, and please kindly remind me so that I can rectify as soon as possible!&lt;/p&gt; 
&lt;p&gt;Andrew Francis, Anish Nag, &lt;a href=&quot;https://www.linkedin.com/in/wgalczak/&quot;&gt;Anthony Galczak&lt;/a&gt;, &lt;a href=&quot;https://x.com/abacaj&quot;&gt;Anton Bacaj&lt;/a&gt;, Balázs Galambosi, Charles Frye, Charles Packer, Chris Brousseau, Eric Hartford, Goku Mohandas, Hamel Husain, Harpreet Sahota, Hassan El Mghari, Huu Nguyen, Jeremy Howard, Jesse Silver, John Cook, &lt;a href=&quot;https://www.linkedin.com/in/juan-pablo-bottaro/&quot;&gt;Juan Pablo Bottaro&lt;/a&gt;, Kyle Gallatin, Lance Martin, Lucio Dery, Matt Ross, Maxime Labonne, Miles Brundage, Nathan Lambert, Omar Khattab, &lt;a href=&quot;https://www.linkedin.com/in/xphongvn/&quot;&gt;Phong Nguyen&lt;/a&gt;, Purnendu Mukherjee, Sam Reiswig, Sebastian Raschka, Shahul ES, Sharif Shameem, Soumith Chintala, Teknium, Tim Dettmers, Undi5, Val Andrei Fajardo, Vern Liang, Victor Sanh, Wing Lian, Xiquan Cui, Ying Sheng, and Kristofer.&lt;/p&gt; 
&lt;p&gt;I&#39;d like to thank all early readers who have also reached out with feedback. Douglas Bailley is a super reader who shared so much thoughtful feedback. Nutan Sahoo for suggesting an elegant way to explain perplexity.&lt;/p&gt; 
&lt;p&gt;I learned so much from the online discussions with so many. Thanks to everyone who&#39;s ever answered my questions, commented on my posts, or sent me an email with your thoughts.&lt;/p&gt; 
&lt;p&gt;Of course, the book wouldn&#39;t have been possible without the team at O&#39;Reilly, especially my development editors (Melissa Potter, Corbin Collins, Jill Leonard) and my production editors (Kristen Brown and Elizabeth Kelly). Liz Wheeler is the most discerning editor I&#39;ve ever worked with. Nicole Butterfield is a force who oversaw this book from an idea to a final product.&lt;/p&gt; 
&lt;p&gt;This book, after all, is an accumulation of invaluable lessons I learned throughout my career. I owe these lessons to my extremely competent and patient coworkers and former coworkers. Every person I&#39;ve worked with has taught me something new about bringing ML into the world.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;Chip Huyen, &lt;em&gt;AI Engineering&lt;/em&gt;. O&#39;Reilly Media, 2025.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@book{aiebook2025,  
    address = {USA},  
    author = {Chip Huyen},  
    isbn = {978-1801819312},   
    publisher = {O&#39;Reilly Media},  
    title = {{AI Engineering}},  
    year = {2025}  
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/Data-Science-For-Beginners</title>
      <link>https://github.com/microsoft/Data-Science-For-Beginners</link>
      <description>&lt;p&gt;10 Weeks, 20 Lessons, Data Science for All!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Science for Beginners - A Curriculum&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/codespaces/new?hide_repo_select=true&amp;amp;ref=main&amp;amp;repo=344191198&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in GitHub Codespaces&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/Data-Science-For-Beginners/raw/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/graphs/contributors/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/issues/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/pulls/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/watchers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/network/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/stargazers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/ByRwuEEgH4&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://aka.ms/foundry/forum&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Azure_AI_Foundry_Developer_Forum-blue?style=for-the-badge&amp;amp;logo=github&amp;amp;color=000000&amp;amp;logoColor=fff&quot; alt=&quot;Azure AI Foundry Developer Forum&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. Each lesson includes pre-lesson and post-lesson quizzes, written instructions to complete the lesson, a solution, and an assignment. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Hearty thanks to our authors:&lt;/strong&gt; &lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine Greenaway&lt;/a&gt;, &lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry Soshnikov&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya Narasimhan&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/JalenMcG&quot;&gt;Jalen McGee&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen Looper&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud Levy&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany Souterre&lt;/a&gt;, &lt;a href=&quot;https://www.twitter.com/geektrainer&quot;&gt;Christopher Harrison&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🙏 Special thanks 🙏 to our &lt;a href=&quot;https://studentambassadors.microsoft.com/&quot;&gt;Microsoft Student Ambassador&lt;/a&gt; authors, reviewers and content contributors,&lt;/strong&gt; notably Aaryan Arora, &lt;a href=&quot;https://github.com/AdityaGarg00&quot;&gt;Aditya Garg&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/alondra-sanchez-molina/&quot;&gt;Alondra Sanchez&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/ankitasingh007&quot;&gt;Ankita Singh&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/anupam--mishra/&quot;&gt;Anupam Mishra&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/arpitadas01/&quot;&gt;Arpita Das&lt;/a&gt;, ChhailBihari Dubey, &lt;a href=&quot;https://www.linkedin.com/in/dibrinsofor&quot;&gt;Dibri Nsofor&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/dishita-bhasin-7065281bb&quot;&gt;Dishita Bhasin&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/majd-s/&quot;&gt;Majd Safi&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/max-blum-6036a1186/&quot;&gt;Max Blum&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/miguelmque/&quot;&gt;Miguel Correa&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/iftu119&quot;&gt;Mohamma Iftekher (Iftu) Ebne Jalal&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/nawrin-tabassum&quot;&gt;Nawrin Tabassum&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/raymond-wp/&quot;&gt;Raymond Wangsa Putra&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/rty2423&quot;&gt;Rohit Yadav&lt;/a&gt;, Samridhi Sharma, &lt;a href=&quot;https://www.linkedin.com/mwlite/in/sanya-sinha-13aab1200&quot;&gt;Sanya Sinha&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/sheena-narua-n/&quot;&gt;Sheena Narula&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/tauqeerahmad5201/&quot;&gt;Tauqeer Ahmad&lt;/a&gt;, Yogendrasingh Pawar , &lt;a href=&quot;https://www.linkedin.com/in/vidushi-gupta07/&quot;&gt;Vidushi Gupta&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jasleen-sondhi/&quot;&gt;Jasleen Sondhi&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Title.png&quot; alt=&quot; Sketchnote by (@sketchthedocs) &quot; /&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Announcement - New Curriculum on Generative AI was just released!&lt;/h2&gt; 
&lt;p&gt;We just released a 12 lesson curriculum on generative AI. Come learn things like:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;prompting and prompt engineering&lt;/li&gt; 
 &lt;li&gt;text and image app generation&lt;/li&gt; 
 &lt;li&gt;search apps&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As usual, there&#39;s a lesson, assignments to complete, knowledge checks and challenges.&lt;/p&gt; 
&lt;p&gt;Check it out:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners&quot;&gt;https://aka.ms/genai-beginners&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Are you a student?&lt;/h1&gt; 
&lt;p&gt;Get started with the following resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-gb/learn/student-hub?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Student Hub page&lt;/a&gt; In this page, you will find beginner resources, Student packs and even ways to get a free cert voucher. This is one page you want to bookmark and check from time to time as we switch out content at least monthly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://studentambassadors.microsoft.com?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Microsoft Learn Student Ambassadors&lt;/a&gt; Join a global community of student ambassadors, this could be your way into Microsoft.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Getting Started&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;: we have &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/for-teachers.md&quot;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum. We&#39;d love your feedback &lt;a href=&quot;https://github.com/microsoft/Data-Science-For-Beginners/discussions&quot;&gt;in our discussion forum&lt;/a&gt;!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://aka.ms/student-page&quot;&gt;Students&lt;/a&gt;&lt;/strong&gt;: to use this curriculum on your own, fork the entire repo and complete the exercises on your own, starting with a pre-lecture quiz. Then read the lecture and complete the rest of the activities. Try to create the projects by comprehending the lessons rather than copying the solution code; however, that code is available in the /solutions folders in each project-oriented lesson. Another idea would be to form a study group with friends and go through the content together. For further study, we recommend &lt;a href=&quot;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/qprpajyoy3x0g7?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Microsoft Learn&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Meet the Team&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://youtu.be/8mzavjQSMM4&quot; title=&quot;Promo video&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/ds-for-beginners.gif&quot; alt=&quot;Promo video&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&quot;https://www.linkedin.com/in/mohitjaisal&quot;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;🎥 Click the image above for a video about the project the folks who created it!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Pedagogy&lt;/h2&gt; 
&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is project-based and that it includes frequent quizzes. By the end of this series, students will have learned basic principles of data science, including ethical concepts, data preparation, different ways of working with data, data visualization, data analysis, real-world use cases of data science, and more.&lt;/p&gt; 
&lt;p&gt;In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 10 week cycle.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Find our &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CODE_OF_CONDUCT.md&quot;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CONTRIBUTING.md&quot;&gt;Contributing&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&quot;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Each lesson includes:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optional sketchnote&lt;/li&gt; 
 &lt;li&gt;Optional supplemental video&lt;/li&gt; 
 &lt;li&gt;Pre-lesson warmup quiz&lt;/li&gt; 
 &lt;li&gt;Written lesson&lt;/li&gt; 
 &lt;li&gt;For project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; 
 &lt;li&gt;Knowledge checks&lt;/li&gt; 
 &lt;li&gt;A challenge&lt;/li&gt; 
 &lt;li&gt;Supplemental reading&lt;/li&gt; 
 &lt;li&gt;Assignment&lt;/li&gt; 
 &lt;li&gt;Post-lesson quiz&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained in the Quiz-App folder, for 40 total quizzes of three questions each. They are linked from within the lessons, but the quiz app can be run locally or deployed to Azure; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Roadmap.png&quot; alt=&quot; Sketchnote by (@sketchthedocs) &quot; /&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science For Beginners: Roadmap - &lt;em&gt;Sketchnote by &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Lesson Number&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Topic&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Lesson Grouping&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Learning Objectives&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Linked Lesson&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Author&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;01&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Defining Data Science&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Learn the basic concepts behind data science and how it’s related to artificial intelligence, machine learning, and big data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/01-defining-data-science/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/beZ7Mb_oz9I&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;02&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science Ethics&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Ethics Concepts, Challenges &amp;amp; Frameworks.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/02-ethics/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;03&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Defining Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;How data is classified and its common sources.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/03-defining-data/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;04&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to Statistics &amp;amp; Probability&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;The mathematical techniques of probability and statistics to understand data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/04-stats-and-probability/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/Z5Zy85g4Yjw&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;05&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with Relational Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to relational data and the basics of exploring and analyzing relational data with the Structured Query Language, also known as SQL (pronounced “see-quell”).&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/geektrainer&quot;&gt;Christopher&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;06&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with NoSQL Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to non-relational data, its various types and the basics of exploring and analyzing document databases.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/06-non-relational/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;07&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with Python&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Basics of using Python for data exploration with libraries such as Pandas. Foundational understanding of Python programming is recommended.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/07-python/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/dZjWOGbsN4Y&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;08&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Preparation&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Topics on data techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/08-data-preparation/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;09&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Quantities&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Learn how to use Matplotlib to visualize bird data 🦆&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/09-visualization-quantities/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;10&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Distributions of Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing observations and trends within an interval.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/10-visualization-distributions/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;11&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Proportions&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing discrete and grouped percentages.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/11-visualization-proportions/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;12&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Relationships&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing connections and correlations between sets of data and their variables.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/12-visualization-relationships/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;13&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Meaningful Visualizations&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Techniques and guidance for making your visualizations valuable for effective problem solving and insights.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/13-meaningful-visualizations/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to the Data Science lifecycle&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to the data science lifecycle and its first step of acquiring and extracting data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/14-Introduction/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;15&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Analyzing&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This phase of the data science lifecycle focuses on techniques to analyze data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/15-analyzing/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;16&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Communication&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This phase of the data science lifecycle focuses on presenting the insights from the data in a way that makes it easier for decision makers to understand.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/16-communication/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/JalenMcG&quot;&gt;Jalen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;17&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This series of lessons introduces data science in the cloud and its benefits.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/17-Introduction/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;18&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Training models using Low Code tools.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/18-Low-Code/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;19&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Deploying models with Azure Machine Learning Studio.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/19-Azure/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;20&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Wild&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/README.md&quot;&gt;In the Wild&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data science driven projects in the real world.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/20-Real-World-Examples/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;GitHub Codespaces&lt;/h2&gt; 
&lt;p&gt;Follow these steps to open this sample in a Codespace:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Click the Code drop-down menu and select the Open with Codespaces option.&lt;/li&gt; 
 &lt;li&gt;Select + New codespace at the bottom on the pane. For more info, check out the &lt;a href=&quot;https://docs.github.com/en/codespaces/developing-in-codespaces/creating-a-codespace-for-a-repository#creating-a-codespace&quot;&gt;GitHub documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;VSCode Remote - Containers&lt;/h2&gt; 
&lt;p&gt;Follow these steps to open this repo in a container using your local machine and VSCode using the VS Code Remote - Containers extension:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If this is your first time using a development container, please ensure your system meets the pre-reqs (i.e. have Docker installed) in &lt;a href=&quot;https://code.visualstudio.com/docs/devcontainers/containers#_getting-started&quot;&gt;the getting started documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To use this repository, you can either open the repository in an isolated Docker volume:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Under the hood, this will use the Remote-Containers: &lt;strong&gt;Clone Repository in Container Volume...&lt;/strong&gt; command to clone the source code in a Docker volume instead of the local filesystem. &lt;a href=&quot;https://docs.docker.com/storage/volumes/&quot;&gt;Volumes&lt;/a&gt; are the preferred mechanism for persisting container data.&lt;/p&gt; 
&lt;p&gt;Or open a locally cloned or downloaded version of the repository:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Clone this repository to your local filesystem.&lt;/li&gt; 
 &lt;li&gt;Press F1 and select the &lt;strong&gt;Remote-Containers: Open Folder in Container...&lt;/strong&gt; command.&lt;/li&gt; 
 &lt;li&gt;Select the cloned copy of this folder, wait for the container to start, and try things out.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Offline access&lt;/h2&gt; 
&lt;p&gt;You can run this documentation offline by using &lt;a href=&quot;https://docsify.js.org/#/&quot;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&quot;https://docsify.js.org/#/quickstart&quot;&gt;install Docsify&lt;/a&gt; on your local machine, then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note, notebooks will not be rendered via Docsify, so when you need to run a notebook, do that separately in VS Code running a Python kernel.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Help Wanted!&lt;/h2&gt; 
&lt;p&gt;If you would like to translate all or part of the curriculum, please follow our &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&quot;&gt;Translations&lt;/a&gt; guide.&lt;/p&gt; 
&lt;h2&gt;Other Curricula&lt;/h2&gt; 
&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-beginners&quot;&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet&quot;&gt;Generative AI for Beginners .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/generative-ai-with-javascript&quot;&gt;Generative AI with JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genaijava&quot;&gt;Generative AI with Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming&quot;&gt;Mastering GitHub Copilot for Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>huggingface/smol-course</title>
      <link>https://github.com/huggingface/smol-course</link>
      <description>&lt;p&gt;A course on aligning smol models.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/banner.png&quot; alt=&quot;smolcourse image&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;a smol course&lt;/h1&gt; 
&lt;p&gt;This is a practical course on aligning language models for your specific use case. It&#39;s a handy way to get started with aligning language models, because everything runs on most local machines. There are minimal GPU requirements and no paid services. The course is based on the &lt;a href=&quot;https://github.com/huggingface/smollm/tree/main&quot;&gt;SmolLM2&lt;/a&gt; series of models, but you can transfer the skills you learn here to larger models or other small language models.&lt;/p&gt; 
&lt;a href=&quot;http://hf.co/join/discord&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Discord-7289DA?&amp;amp;logo=discord&amp;amp;logoColor=white&quot; /&gt; &lt;/a&gt; 
&lt;div style=&quot;background: linear-gradient(to right, #e0f7fa, #e1bee7, orange); padding: 20px; border-radius: 5px; margin-bottom: 20px; color: purple;&quot;&gt; 
 &lt;h2&gt;Participation is open, free, and now!&lt;/h2&gt; 
 &lt;p&gt;This course is open and peer reviewed. To get involved with the course &lt;strong&gt;open a pull request&lt;/strong&gt; and submit your work for review. Here are the steps:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Fork the repo &lt;a href=&quot;https://github.com/huggingface/smol-course/fork&quot;&gt;here&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Read the material, make changes, do the exercises, add your own examples.&lt;/li&gt; 
  &lt;li&gt;Open a PR on the december_2024 branch&lt;/li&gt; 
  &lt;li&gt;Get it reviewed and merged&lt;/li&gt; 
 &lt;/ol&gt; 
 &lt;p&gt;This should help you learn and to build a community-driven course that is always improving.&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;We can discuss the process in this &lt;a href=&quot;https://github.com/huggingface/smol-course/discussions/2#discussion-7602932&quot;&gt;discussion thread&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Course Outline&lt;/h2&gt; 
&lt;p&gt;This course provides a practical, hands-on approach to working with small language models, from initial training through to production deployment.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Module&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
   &lt;th&gt;Release Date&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/1_instruction_tuning&quot;&gt;Instruction Tuning&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Learn supervised fine-tuning, chat templating, and basic instruction following&lt;/td&gt; 
   &lt;td&gt;✅ Ready&lt;/td&gt; 
   &lt;td&gt;Dec 3, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/2_preference_alignment&quot;&gt;Preference Alignment&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Explore DPO and ORPO techniques for aligning models with human preferences&lt;/td&gt; 
   &lt;td&gt;✅ Ready&lt;/td&gt; 
   &lt;td&gt;Dec 6, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/3_parameter_efficient_finetuning&quot;&gt;Parameter-efficient Fine-tuning&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Learn LoRA, prompt tuning, and efficient adaptation methods&lt;/td&gt; 
   &lt;td&gt;✅ Ready&lt;/td&gt; 
   &lt;td&gt;Dec 9, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/4_evaluation&quot;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Use automatic benchmarks and create custom domain evaluations&lt;/td&gt; 
   &lt;td&gt;✅ Ready&lt;/td&gt; 
   &lt;td&gt;Dec 13, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/5_vision_language_models&quot;&gt;Vision-language Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Adapt multimodal models for vision-language tasks&lt;/td&gt; 
   &lt;td&gt;✅ Ready&lt;/td&gt; 
   &lt;td&gt;Dec 16, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/6_synthetic_datasets&quot;&gt;Synthetic Datasets&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Create and validate synthetic datasets for training&lt;/td&gt; 
   &lt;td&gt;✅ Ready&lt;/td&gt; 
   &lt;td&gt;Dec 20, 2024&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/7_inference&quot;&gt;Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Infer with models efficiently&lt;/td&gt; 
   &lt;td&gt;✅ Ready&lt;/td&gt; 
   &lt;td&gt;Jan 8, 2025&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/huggingface/smol-course/main/8_agents&quot;&gt;Agents&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Build your own agentic AI&lt;/td&gt; 
   &lt;td&gt;✅ Ready&lt;/td&gt; 
   &lt;td&gt;Jan 13, 2025&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Why Small Language Models?&lt;/h2&gt; 
&lt;p&gt;While large language models have shown impressive capabilities, they often require significant computational resources and can be overkill for focused applications. Small language models offer several advantages for domain-specific applications:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;: Require significantly less computational resources to train and deploy&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customization&lt;/strong&gt;: Easier to fine-tune and adapt to specific domains&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Control&lt;/strong&gt;: Better understanding and control of model behavior&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cost&lt;/strong&gt;: Lower operational costs for training and inference&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Privacy&lt;/strong&gt;: Can be run locally without sending data to external APIs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Green Technology&lt;/strong&gt;: Advocates efficient usage of resources with reduced carbon footprint&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easier Academic Research Development&lt;/strong&gt;: Provides an easy starter for academic research with cutting-edge LLMs with less logistical constraints&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before starting, ensure you have the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Basic understanding of machine learning and natural language processing.&lt;/li&gt; 
 &lt;li&gt;Familiarity with Python, PyTorch, and the &lt;code&gt;transformers&lt;/code&gt; library.&lt;/li&gt; 
 &lt;li&gt;Access to a pre-trained language model and a labeled dataset.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We maintain the course as a package so you can install dependencies easily via a package manager. We recommend &lt;a href=&quot;https://github.com/astral-sh/uv&quot;&gt;uv&lt;/a&gt; for this purpose, but you could use alternatives like &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;pdm&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Using &lt;code&gt;uv&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;With &lt;code&gt;uv&lt;/code&gt; installed, you can install the course like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv venv --python 3.11.0
uv sync
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using &lt;code&gt;pip&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;All the examples run in the same &lt;strong&gt;python 3.11&lt;/strong&gt; environment, so you should create an environment and install dependencies like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# python -m venv .venv
# source .venv/bin/activate
pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Google Colab&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;From Google Colab&lt;/strong&gt; you will need to install dependencies flexibly based on the hardware you&#39;re using. Like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install transformers trl datasets huggingface_hub
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-cookbook</title>
      <link>https://github.com/openai/openai-cookbook</link>
      <description>&lt;p&gt;Examples and guides for using the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;a href=&quot;https://cookbook.openai.com&quot; target=&quot;_blank&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;/images/openai-cookbook-white.png&quot; style=&quot;max-width: 100%; width: 400px; margin-bottom: 20px&quot; /&gt; 
  &lt;img alt=&quot;OpenAI Cookbook Logo&quot; src=&quot;https://raw.githubusercontent.com/openai/openai-cookbook/main/images/openai-cookbook.png&quot; width=&quot;400px&quot; /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h3&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;✨ Navigate at &lt;a href=&quot;https://cookbook.openai.com&quot;&gt;cookbook.openai.com&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Example code and guides for accomplishing common tasks with the &lt;a href=&quot;https://platform.openai.com/docs/introduction&quot;&gt;OpenAI API&lt;/a&gt;. To run these examples, you&#39;ll need an OpenAI account and associated API key (&lt;a href=&quot;https://platform.openai.com/signup&quot;&gt;create a free account here&lt;/a&gt;). Set an environment variable called &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; with your API key. Alternatively, in most IDEs such as Visual Studio Code, you can create an &lt;code&gt;.env&lt;/code&gt; file at the root of your repo containing &lt;code&gt;OPENAI_API_KEY=&amp;lt;your API key&amp;gt;&lt;/code&gt;, which will be picked up by the notebooks.&lt;/p&gt; 
&lt;p&gt;Most code examples are written in Python, though the concepts can be applied in any language.&lt;/p&gt; 
&lt;p&gt;For other useful tools, guides and courses, check out these &lt;a href=&quot;https://cookbook.openai.com/related_resources&quot;&gt;related resources from around the web&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/generative-ai-for-beginners</title>
      <link>https://github.com/microsoft/generative-ai-for-beginners</link>
      <description>&lt;p&gt;21 Lessons, Get Started Building with Generative AI 🔗 https://microsoft.github.io/generative-ai-for-beginners/&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/images/repo-thumbnailv4-fixed.png?WT.mc_id=academic-105485-koreyst&quot; alt=&quot;Generative AI For Beginners&quot; /&gt;&lt;/p&gt; 
&lt;h3&gt;21 Lessons teaching everything you need to know to start building Generative AI applications&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-For-Beginners/raw/master/LICENSE?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/issues/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/pulls/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/watchers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/network/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/stargazers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://dcbadge.limes.pink/api/server/ByRwuEEgH4&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;🌐 Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fr/README.md&quot;&gt;French&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/es/README.md&quot;&gt;Spanish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/de/README.md&quot;&gt;German&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ru/README.md&quot;&gt;Russian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ar/README.md&quot;&gt;Arabic&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fa/README.md&quot;&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ur/README.md&quot;&gt;Urdu&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/zh/README.md&quot;&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/mo/README.md&quot;&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hk/README.md&quot;&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tw/README.md&quot;&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ja/README.md&quot;&gt;Japanese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ko/README.md&quot;&gt;Korean&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hi/README.md&quot;&gt;Hindi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/bn/README.md&quot;&gt;Bengali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/mr/README.md&quot;&gt;Marathi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ne/README.md&quot;&gt;Nepali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pa/README.md&quot;&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pt/README.md&quot;&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/br/README.md&quot;&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/it/README.md&quot;&gt;Italian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pl/README.md&quot;&gt;Polish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tr/README.md&quot;&gt;Turkish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/el/README.md&quot;&gt;Greek&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/th/README.md&quot;&gt;Thai&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sv/README.md&quot;&gt;Swedish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/da/README.md&quot;&gt;Danish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/no/README.md&quot;&gt;Norwegian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fi/README.md&quot;&gt;Finnish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/nl/README.md&quot;&gt;Dutch&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/he/README.md&quot;&gt;Hebrew&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/vi/README.md&quot;&gt;Vietnamese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/id/README.md&quot;&gt;Indonesian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ms/README.md&quot;&gt;Malay&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tl/README.md&quot;&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sw/README.md&quot;&gt;Swahili&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hu/README.md&quot;&gt;Hungarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/cs/README.md&quot;&gt;Czech&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sk/README.md&quot;&gt;Slovak&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ro/README.md&quot;&gt;Romanian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/bg/README.md&quot;&gt;Bulgarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sr/README.md&quot;&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hr/README.md&quot;&gt;Croatian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sl/README.md&quot;&gt;Slovenian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/uk/README.md&quot;&gt;Ukrainian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/my/README.md&quot;&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Generative AI for Beginners (Version 3) - A Course&lt;/h1&gt; 
&lt;p&gt;Learn the fundamentals of building Generative AI applications with our 21-lesson comprehensive course by Microsoft Cloud Advocates.&lt;/p&gt; 
&lt;h2&gt;🌱 Getting Started&lt;/h2&gt; 
&lt;p&gt;This course has 21 lessons. Each lesson covers its own topic so start wherever you like!&lt;/p&gt; 
&lt;p&gt;Lessons are labeled either &quot;Learn&quot; lessons explaining a Generative AI concept or &quot;Build&quot; lessons that explain a concept and code examples in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt; when possible.&lt;/p&gt; 
&lt;p&gt;For .NET Developers checkout &lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners (.NET Edition)&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Each lesson also includes a &quot;Keep Learning&quot; section with additional learning tools.&lt;/p&gt; 
&lt;h2&gt;What You Need&lt;/h2&gt; 
&lt;h3&gt;To run the code of this course, you can use either:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/azure-open-ai?WT.mc_id=academic-105485-koreyst&quot;&gt;Azure OpenAI Service&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;aoai-assignment&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/gh-models?WT.mc_id=academic-105485-koreyst&quot;&gt;GitHub Marketplace Model Catalog&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;githubmodels&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/open-ai?WT.mc_id=academic-105485-koreyst&quot;&gt;OpenAI API&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;oai-assignment&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Basic knowledge of Python or TypeScript is helpful - *For absolute beginners check out these &lt;a href=&quot;https://aka.ms/genai-beginners/python?WT.mc_id=academic-105485-koreyst&quot;&gt;Python&lt;/a&gt; and &lt;a href=&quot;https://aka.ms/genai-beginners/typescript?WT.mc_id=academic-105485-koreyst&quot;&gt;TypeScript&lt;/a&gt; courses&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;A GitHub account to &lt;a href=&quot;https://aka.ms/genai-beginners/github?WT.mc_id=academic-105485-koreyst&quot;&gt;fork this entire repo&lt;/a&gt; to your own GitHub account&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We have created a &lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/00-course-setup/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Course Setup&lt;/a&gt;&lt;/strong&gt; lesson to help you with setting up your development environment.&lt;/p&gt; 
&lt;p&gt;Don&#39;t forget to &lt;a href=&quot;https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst&quot;&gt;star (🌟) this repo&lt;/a&gt; to find it easier later.&lt;/p&gt; 
&lt;h2&gt;🧠 Ready to Deploy?&lt;/h2&gt; 
&lt;p&gt;If you are looking for more advanced code samples, check out our &lt;a href=&quot;https://aka.ms/genai-beg-code?WT.mc_id=academic-105485-koreyst&quot;&gt;collection of Generative AI Code Samples&lt;/a&gt; in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;🗣️ Meet Other Learners, Get Support&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href=&quot;https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst&quot;&gt;official Azure AI Foundry Discord server&lt;/a&gt; to meet and network with other learners taking this course and get support.&lt;/p&gt; 
&lt;p&gt;Ask questions or share product feedback in our &lt;a href=&quot;https://aka.ms/azureaifoundry/forum&quot;&gt;Azure AI Foundry Developer Forum&lt;/a&gt; on Github.&lt;/p&gt; 
&lt;h2&gt;🚀 Building a Startup?&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href=&quot;https://www.microsoft.com/startups&quot;&gt;Microsoft for Startups&lt;/a&gt; to find out how to get started building with Azure credits today.&lt;/p&gt; 
&lt;h2&gt;🙏 Want to help?&lt;/h2&gt; 
&lt;p&gt;Do you have suggestions or found spelling or code errors? &lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst&quot;&gt;Raise an issue&lt;/a&gt; or &lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners/pulls?WT.mc_id=academic-105485-koreyst&quot;&gt;Create a pull request&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📂 Each lesson includes:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A short video introduction to the topic&lt;/li&gt; 
 &lt;li&gt;A written lesson located in the README&lt;/li&gt; 
 &lt;li&gt;Python and TypeScript code samples supporting Azure OpenAI and OpenAI API&lt;/li&gt; 
 &lt;li&gt;Links to extra resources to continue your learning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🗃️ Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Lesson Link&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Extra Learning&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;00&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/00-course-setup/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Course Setup&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to Setup Your Development Environment&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;01&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/01-introduction-to-genai/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Introduction to Generative AI and LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; Understanding what Generative AI is and how Large Language Models (LLMs) work.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson-1-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;02&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/02-exploring-and-comparing-different-llms/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Exploring and comparing different LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to select the right model for your use case&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;03&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Using Generative AI Responsibly&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to build Generative AI Applications responsibly&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;04&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Understanding Prompt Engineering Fundamentals&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; Hands-on Prompt Engineering Best Practices&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson4-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;05&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Creating Advanced Prompts&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to apply prompt engineering techniques that improve the outcome of your prompts.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson5-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;06&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Text Generation Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A text generation app using Azure OpenAI / OpenAI API&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson6-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;07&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/07-building-chat-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Chat Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; Techniques for efficiently building and integrating chat applications.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lessons7-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;08&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Search Apps Vector Databases&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A search application that uses Embeddings to search for data.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson8-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;09&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/09-building-image-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Image Generation Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An image generation application&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson9-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/10-building-low-code-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Low Code AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A Generative AI application using Low Code tools&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson10-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/11-integrating-with-function-calling/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Integrating External Applications with Function Calling&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; What is function calling and its use cases for applications&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson11-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Designing UX for AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to apply UX design principles when developing Generative AI Applications&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson12-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/13-securing-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Securing Your Generative AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The threats and risks to AI systems and methods to secure these systems.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;The Generative AI Application Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The tools and metrics to manage the LLM Lifecycle and LLMOps&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson14-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/15-rag-and-vector-databases/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Retrieval Augmented Generation (RAG) and Vector Databases&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using a RAG Framework to retrieve embeddings from a Vector Databases&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/16-open-source-models/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Open Source Models and Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using open source models available on Hugging Face&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson16-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/17-ai-agents/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;AI Agents&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using an AI Agent Framework&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson17-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/18-fine-tuning/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Fine-Tuning LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The what, why and how of fine-tuning LLMs&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/19-slm/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with SLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The benefits of building with Small Language Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/20-mistral/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with Mistral Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The features and differences of the Mistral Family Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/21-meta/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with Meta Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The features and differences of the Meta Family Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;🌟 Special thanks&lt;/h3&gt; 
&lt;p&gt;Special thanks to &lt;a href=&quot;https://www.linkedin.com/in/john0isaac/&quot;&gt;&lt;strong&gt;John Aziz&lt;/strong&gt;&lt;/a&gt; for creating all of the GitHub Actions and workflows&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/bernhard-merkle-738b73/&quot;&gt;&lt;strong&gt;Bernhard Merkle&lt;/strong&gt;&lt;/a&gt; for making key contributions to each lesson to improve the learner and code experience.&lt;/p&gt; 
&lt;h2&gt;🎒 Other Courses&lt;/h2&gt; 
&lt;p&gt;Our team produces other courses! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;strong&gt;NEW&lt;/strong&gt; Model Context Protocol for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;AI Agents for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners using .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-js-course?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners using JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst&quot;&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>NirDiamant/RAG_Techniques</title>
      <link>https://github.com/NirDiamant/RAG_Techniques</link>
      <description>&lt;p&gt;This repository showcases various advanced techniques for Retrieval-Augmented Generation (RAG) systems. RAG systems combine information retrieval with generative models to provide accurate and contextually rich responses.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;http://makeapullrequest.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.linkedin.com/in/nir-diamant-759323134/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/LinkedIn-Connect-blue&quot; alt=&quot;LinkedIn&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/NirDiamantAI&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/NirDiamantAI?label=Follow%20@NirDiamantAI&amp;amp;style=social&quot; alt=&quot;Twitter&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.reddit.com/r/EducationalAI/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Reddit-Join%20our%20subreddit-FF4500?style=flat-square&amp;amp;logo=reddit&amp;amp;logoColor=white&quot; alt=&quot;Reddit&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/cA6Aa4uyDX&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20our%20community-7289da?style=flat-square&amp;amp;logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/sponsors/NirDiamant&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Sponsor&amp;amp;message=%E2%9D%A4&amp;amp;logo=GitHub&amp;amp;color=ff69b4&quot; alt=&quot;Sponsor&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;🌟 &lt;strong&gt;Support This Project:&lt;/strong&gt; Your sponsorship fuels innovation in RAG technologies. &lt;strong&gt;&lt;a href=&quot;https://github.com/sponsors/NirDiamant&quot;&gt;Become a sponsor&lt;/a&gt;&lt;/strong&gt; to help maintain and expand this valuable resource!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Sponsors ❤️&lt;/h2&gt; 
&lt;p&gt;We gratefully acknowledge the organizations and individuals who have made significant contributions to this project.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Company Sponsors&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://zilliz.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/images/ziliz_logo.png&quot; style=&quot;border-radius: 12px; margin-right: 24px; vertical-align: middle;&quot; height=&quot;96&quot; alt=&quot;Zilliz: Key Collaborator&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Individual Sponsors&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/sponsors/Eisenh&quot;&gt;&lt;img src=&quot;https://github.com/Eisenh.png&quot; style=&quot;border-radius: 50%;&quot; width=&quot;64&quot; height=&quot;64&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Advanced RAG Techniques: Elevating Your Retrieval-Augmented Generation Systems 🚀&lt;/h1&gt; 
&lt;p&gt;Welcome to one of the most comprehensive and dynamic collections of Retrieval-Augmented Generation (RAG) tutorials available today. This repository serves as a hub for cutting-edge techniques aimed at enhancing the accuracy, efficiency, and contextual richness of RAG systems.&lt;/p&gt; 
&lt;h2&gt;📫 Stay Updated!&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;table&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot;&gt;🚀&lt;br /&gt;&lt;b&gt;Cutting-edge&lt;br /&gt;Updates&lt;/b&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;💡&lt;br /&gt;&lt;b&gt;Expert&lt;br /&gt;Insights&lt;/b&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot;&gt;🎯&lt;br /&gt;&lt;b&gt;Top 0.1%&lt;br /&gt;Content&lt;/b&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;p&gt;&lt;a href=&quot;https://diamantai.substack.com/?r=336pe4&amp;amp;utm_campaign=pub-share-checklist&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/images/subscribe-button.svg?sanitize=true&quot; alt=&quot;Subscribe to DiamantAI Newsletter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Join over 20,000 of AI enthusiasts getting unique cutting-edge insights and free tutorials!&lt;/em&gt; &lt;em&gt;&lt;strong&gt;Plus, subscribers get exclusive early access and special 33% discounts to my book and the upcoming RAG Techniques course!&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://diamantai.substack.com/?r=336pe4&amp;amp;utm_campaign=pub-share-checklist&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/images/substack_image.png&quot; alt=&quot;DiamantAI&#39;s newsletter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Retrieval-Augmented Generation (RAG) is revolutionizing the way we combine information retrieval with generative AI. This repository showcases a curated collection of advanced techniques designed to supercharge your RAG systems, enabling them to deliver more accurate, contextually relevant, and comprehensive responses.&lt;/p&gt; 
&lt;p&gt;Our goal is to provide a valuable resource for researchers and practitioners looking to push the boundaries of what&#39;s possible with RAG. By fostering a collaborative environment, we aim to accelerate innovation in this exciting field.&lt;/p&gt; 
&lt;h2&gt;Related Projects&lt;/h2&gt; 
&lt;p&gt;🚀 Level up with my &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/agents-towards-production&quot;&gt;Agents Towards Production&lt;/a&gt;&lt;/strong&gt; repository. It delivers horizontal, code-first tutorials that cover every tool and step in the lifecycle of building production-grade GenAI agents, guiding you from spark to scale with proven patterns and reusable blueprints for real-world launches, making it the smartest place to start if you&#39;re serious about shipping agents to production.&lt;/p&gt; 
&lt;p&gt;🤖 Explore my &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/GenAI_Agents&quot;&gt;GenAI Agents Repository&lt;/a&gt;&lt;/strong&gt; to discover a variety of AI agent implementations and tutorials, showcasing how different AI technologies can be combined to create powerful, interactive systems.&lt;/p&gt; 
&lt;p&gt;🖋️ Check out my &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/Prompt_Engineering&quot;&gt;Prompt Engineering Techniques guide&lt;/a&gt;&lt;/strong&gt; for a comprehensive collection of prompting strategies, from basic concepts to advanced techniques, enhancing your ability to interact effectively with AI language models.&lt;/p&gt; 
&lt;h2&gt;A Community-Driven Knowledge Hub&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;This repository grows stronger with your contributions!&lt;/strong&gt; Join our vibrant communities - the central hubs for shaping and advancing this project together 🤝&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.reddit.com/r/EducationalAI/&quot;&gt;Educational AI Subreddit&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://discord.gg/cA6Aa4uyDX&quot;&gt;RAG Techniques Discord Community&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Whether you&#39;re an expert or just starting out, your insights can shape the future of RAG. Join us to propose ideas, get feedback, and collaborate on innovative techniques. For contribution guidelines, please refer to our &lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques/raw/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/strong&gt; file. Let&#39;s advance RAG technology together!&lt;/p&gt; 
&lt;p&gt;🔗 For discussions on GenAI, RAG, or custom agents, or to explore knowledge-sharing opportunities, feel free to &lt;strong&gt;&lt;a href=&quot;https://www.linkedin.com/in/nir-diamant-759323134/&quot;&gt;connect on LinkedIn&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;🧠 State-of-the-art RAG enhancements&lt;/li&gt; 
 &lt;li&gt;📚 Comprehensive documentation for each technique&lt;/li&gt; 
 &lt;li&gt;🛠️ Practical implementation guidelines&lt;/li&gt; 
 &lt;li&gt;🌟 Regular updates with the latest advancements&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Advanced Techniques&lt;/h2&gt; 
&lt;p&gt;Explore our extensive list of cutting-edge RAG techniques:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;Category&lt;/th&gt; 
   &lt;th&gt;Technique&lt;/th&gt; 
   &lt;th&gt;View&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;⭐ Key Collaboration&lt;/td&gt; 
   &lt;td&gt;Graph RAG with Milvus Vector DB&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/graphrag_with_milvus_vectordb.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graphrag_with_milvus_vectordb.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;Foundational 🌱&lt;/td&gt; 
   &lt;td&gt;Basic RAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/simple_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;Foundational 🌱&lt;/td&gt; 
   &lt;td&gt;RAG with CSV Files&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/simple_csv_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_csv_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;Foundational 🌱&lt;/td&gt; 
   &lt;td&gt;Reliable RAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/reliable_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reliable_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;Foundational 🌱&lt;/td&gt; 
   &lt;td&gt;Optimizing Chunk Sizes&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/choose_chunk_size.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/choose_chunk_size.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;Foundational 🌱&lt;/td&gt; 
   &lt;td&gt;Proposition Chunking&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/proposition_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/proposition_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;Query Enhancement 🔍&lt;/td&gt; 
   &lt;td&gt;Query Transformations&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/query_transformations.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/query_transformations.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;8&lt;/td&gt; 
   &lt;td&gt;Query Enhancement 🔍&lt;/td&gt; 
   &lt;td&gt;HyDE (Hypothetical Document Embedding)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/HyDe_Hypothetical_Document_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/HyDe_Hypothetical_Document_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;9&lt;/td&gt; 
   &lt;td&gt;Query Enhancement 🔍&lt;/td&gt; 
   &lt;td&gt;HyPE (Hypothetical Prompt Embedding)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/HyPE_Hypothetical_Prompt_Embeddings.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/HyPE_Hypothetical_Prompt_Embeddings.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;Context Enrichment 📚&lt;/td&gt; 
   &lt;td&gt;Contextual Chunk Headers&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/contextual_chunk_headers.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_chunk_headers.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;Context Enrichment 📚&lt;/td&gt; 
   &lt;td&gt;Relevant Segment Extraction&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/relevant_segment_extraction.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/relevant_segment_extraction.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;Context Enrichment 📚&lt;/td&gt; 
   &lt;td&gt;Context Window Enhancement&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/context_enrichment_window_around_chunk.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/context_enrichment_window_around_chunk.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;Context Enrichment 📚&lt;/td&gt; 
   &lt;td&gt;Semantic Chunking&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/semantic_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/semantic_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;Context Enrichment 📚&lt;/td&gt; 
   &lt;td&gt;Contextual Compression&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/contextual_compression.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_compression.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;Context Enrichment 📚&lt;/td&gt; 
   &lt;td&gt;Document Augmentation&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/document_augmentation.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/document_augmentation.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval 🚀&lt;/td&gt; 
   &lt;td&gt;Fusion Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/fusion_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval 🚀&lt;/td&gt; 
   &lt;td&gt;Reranking&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/reranking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval 🚀&lt;/td&gt; 
   &lt;td&gt;Multi-faceted Filtering&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/multi_faceted_filtering.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_faceted_filtering.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval 🚀&lt;/td&gt; 
   &lt;td&gt;Hierarchical Indices&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/hierarchical_indices.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/hierarchical_indices.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval 🚀&lt;/td&gt; 
   &lt;td&gt;Ensemble Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/ensemble_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/ensemble_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval 🚀&lt;/td&gt; 
   &lt;td&gt;Dartboard Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/dartboard.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/dartboard.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;22&lt;/td&gt; 
   &lt;td&gt;Advanced Retrieval 🚀&lt;/td&gt; 
   &lt;td&gt;Multi-modal RAG with Captioning&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/multi_model_rag_with_captioning.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_model_rag_with_captioning.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;23&lt;/td&gt; 
   &lt;td&gt;Iterative Techniques 🔁&lt;/td&gt; 
   &lt;td&gt;Retrieval with Feedback Loop&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/retrieval_with_feedback_loop.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/retrieval_with_feedback_loop.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;24&lt;/td&gt; 
   &lt;td&gt;Iterative Techniques 🔁&lt;/td&gt; 
   &lt;td&gt;Adaptive Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/adaptive_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/adaptive_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;25&lt;/td&gt; 
   &lt;td&gt;Iterative Retrieval 🔄&lt;/td&gt; 
   &lt;td&gt;Iterative Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/iterative_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/iterative_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;26&lt;/td&gt; 
   &lt;td&gt;Evaluation 📊&lt;/td&gt; 
   &lt;td&gt;DeepEval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/evaluation/evaluation_deep_eval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_deep_eval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;27&lt;/td&gt; 
   &lt;td&gt;Evaluation 📊&lt;/td&gt; 
   &lt;td&gt;GroUSE&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/evaluation/evaluation_grouse.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_grouse.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;28&lt;/td&gt; 
   &lt;td&gt;Explainability 🔬&lt;/td&gt; 
   &lt;td&gt;Explainable Retrieval&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/explainable_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/explainable_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;29&lt;/td&gt; 
   &lt;td&gt;Advanced Architecture 🏗️&lt;/td&gt; 
   &lt;td&gt;Graph RAG with LangChain&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/graph_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graph_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;30&lt;/td&gt; 
   &lt;td&gt;Advanced Architecture 🏗️&lt;/td&gt; 
   &lt;td&gt;Microsoft GraphRAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/Microsoft_GraphRag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/Microsoft_GraphRag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;31&lt;/td&gt; 
   &lt;td&gt;Advanced Architecture 🏗️&lt;/td&gt; 
   &lt;td&gt;RAPTOR&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/raptor.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/raptor.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;32&lt;/td&gt; 
   &lt;td&gt;Advanced Architecture 🏗️&lt;/td&gt; 
   &lt;td&gt;Self-RAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/self_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/self_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;33&lt;/td&gt; 
   &lt;td&gt;Advanced Architecture 🏗️&lt;/td&gt; 
   &lt;td&gt;Corrective RAG (CRAG)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/crag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/crag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;34&lt;/td&gt; 
   &lt;td&gt;Special Technique 🌟&lt;/td&gt; 
   &lt;td&gt;Sophisticated Controllable Agent&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NirDiamant/Controllable-RAG-Agent&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;🌱 Foundational RAG Techniques&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Simple RAG 🌱&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_rag_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques/raw/main/all_rag_techniques_runnable_scripts/simple_rag.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Introducing basic RAG techniques ideal for newcomers.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Start with basic retrieval queries and integrate incremental learning mechanisms.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Simple RAG using a CSV file 🧩&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_csv_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_csv_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_csv_rag_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/simple_csv_rag_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Introducing basic RAG using CSV files.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;This uses CSV files to create basic retrieval and integrates with openai to create question and answering system.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Reliable RAG 🏷️&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reliable_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reliable_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Enhances the Simple RAG by adding validation and refinement to ensure the accuracy and relevance of retrieved information.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Check for retrieved document relevancy and highlight the segment of docs used for answering.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Choose Chunk Size 📏&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/choose_chunk_size.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/choose_chunk_size.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/choose_chunk_size.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Selecting an appropriate fixed size for text chunks to balance context preservation and retrieval efficiency.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Experiment with different chunk sizes to find the optimal balance between preserving context and maintaining retrieval speed for your specific use case.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Proposition Chunking ⛓️‍💥&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/proposition_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/proposition_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Breaking down the text into concise, complete, meaningful sentences allowing for better control and handling of specific queries (especially extracting knowledge).&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;💪 &lt;strong&gt;Proposition Generation:&lt;/strong&gt; The LLM is used in conjunction with a custom prompt to generate factual statements from the document chunks.&lt;/li&gt; 
   &lt;li&gt;✅ &lt;strong&gt;Quality Checking:&lt;/strong&gt; The generated propositions are passed through a grading system that evaluates accuracy, clarity, completeness, and conciseness.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/the-propositions-method-enhancing?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&quot;&gt;The Propositions Method: Enhancing Information Retrieval for AI Systems&lt;/a&gt;&lt;/strong&gt; - A comprehensive blog post exploring the benefits and implementation of proposition chunking in RAG systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🔍 Query Enhancement&lt;/h3&gt; 
&lt;ol start=&quot;6&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Query Transformations 🔄&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/query_transformations.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/query_transformations.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/query_transformations.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Modifying and expanding queries to improve retrieval effectiveness.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;✍️ &lt;strong&gt;Query Rewriting:&lt;/strong&gt; Reformulate queries to improve retrieval.&lt;/li&gt; 
   &lt;li&gt;🔙 &lt;strong&gt;Step-back Prompting:&lt;/strong&gt; Generate broader queries for better context retrieval.&lt;/li&gt; 
   &lt;li&gt;🧩 &lt;strong&gt;Sub-query Decomposition:&lt;/strong&gt; Break complex queries into simpler sub-queries.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Hypothetical Questions (HyDE Approach) ❓&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/all_rag_techniques/HyDe_Hypothetical_Document_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/HyDe_Hypothetical_Document_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/HyDe_Hypothetical_Document_Embedding.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Generating hypothetical questions to improve alignment between queries and data.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Create hypothetical questions that point to relevant locations in the data, enhancing query-data matching.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/hyde-exploring-hypothetical-document?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&quot;&gt;HyDE: Exploring Hypothetical Document Embeddings for AI Retrieval&lt;/a&gt;&lt;/strong&gt; - A short blog post explaining this method clearly.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;📚 Context and Content Enrichment&lt;/h3&gt; 
&lt;ol start=&quot;8&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Hypothetical Prompt Embeddings (HyPE) ❓🚀&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/HyPE_Hypothetical_Prompt_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/HyPE_Hypothetical_Prompt_Embedding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/HyPE_Hypothetical_Prompt_Embeddings.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;HyPE (Hypothetical Prompt Embeddings) is an enhancement to traditional RAG retrieval that &lt;strong&gt;precomputes hypothetical prompts at the indexing stage&lt;/strong&gt;, but inseting the chunk in their place. This transforms retrieval into a &lt;strong&gt;question-question matching task&lt;/strong&gt;. This avoids the need for runtime synthetic answer generation, reducing inference-time computational overhead while &lt;strong&gt;improving retrieval alignment&lt;/strong&gt;.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;📖 &lt;strong&gt;Precomputed Questions:&lt;/strong&gt; Instead of embedding document chunks, HyPE &lt;strong&gt;generates multiple hypothetical queries per chunk&lt;/strong&gt; at indexing time.&lt;/li&gt; 
   &lt;li&gt;🔍 &lt;strong&gt;Question-Question Matching:&lt;/strong&gt; User queries are matched against stored hypothetical questions, leading to &lt;strong&gt;better retrieval alignment&lt;/strong&gt;.&lt;/li&gt; 
   &lt;li&gt;⚡ &lt;strong&gt;No Runtime Overhead:&lt;/strong&gt; Unlike HyDE, HyPE does &lt;strong&gt;not require LLM calls at query time&lt;/strong&gt;, making retrieval &lt;strong&gt;faster and cheaper&lt;/strong&gt;.&lt;/li&gt; 
   &lt;li&gt;📈 &lt;strong&gt;Higher Precision &amp;amp; Recall:&lt;/strong&gt; Improves retrieval &lt;strong&gt;context precision by up to 42 percentage points&lt;/strong&gt; and &lt;strong&gt;claim recall by up to 45 percentage points&lt;/strong&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5139335&quot;&gt;Preprint: Hypothetical Prompt Embeddings (HyPE)&lt;/a&gt;&lt;/strong&gt; - Research paper detailing the method, evaluation, and benchmarks.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Contextual Chunk Headers &lt;span&gt;🏷&lt;/span&gt;&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_chunk_headers.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_chunk_headers.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Contextual chunk headers (CCH) is a method of creating document-level and section-level context, and prepending those chunk headers to the chunks prior to embedding them.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Create a chunk header that includes context about the document and/or section of the document, and prepend that to each chunk in order to improve the retrieval accuracy.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/D-Star-AI/dsRAG&quot;&gt;dsRAG&lt;/a&gt;&lt;/strong&gt;: open-source retrieval engine that implements this technique (and a few other advanced RAG techniques)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Relevant Segment Extraction 🧩&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/relevant_segment_extraction.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/relevant_segment_extraction.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Relevant segment extraction (RSE) is a method of dynamically constructing multi-chunk segments of text that are relevant to a given query.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Perform a retrieval post-processing step that analyzes the most relevant chunks and identifies longer multi-chunk segments to provide more complete context to the LLM.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Context Enrichment Techniques 📝&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/context_enrichment_window_around_chunk.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/context_enrichment_window_around_chunk.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/context_enrichment_window_around_chunk_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/context_enrichment_window_around_chunk_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/context_enrichment_window_around_chunk.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Overview 🔎&lt;/h4&gt; 
&lt;p&gt;Enhancing retrieval accuracy by embedding individual sentences and extending context to neighboring sentences.&lt;/p&gt; 
&lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
&lt;p&gt;Retrieve the most relevant sentence while also accessing the sentences before and after it in the original text.&lt;/p&gt; 
&lt;ol start=&quot;12&quot;&gt; 
 &lt;li&gt;Semantic Chunking 🧠&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/semantic_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/semantic_chunking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques/raw/main/all_rag_techniques_runnable_scripts/semantic_chunking.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Overview 🔎&lt;/h4&gt; 
&lt;p&gt;Dividing documents based on semantic coherence rather than fixed sizes.&lt;/p&gt; 
&lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
&lt;p&gt;Use NLP techniques to identify topic boundaries or coherent sections within documents for more meaningful retrieval units.&lt;/p&gt; 
&lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/semantic-chunking-improving-ai-information?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&quot;&gt;Semantic Chunking: Improving AI Information Retrieval&lt;/a&gt;&lt;/strong&gt; - A comprehensive blog post exploring the benefits and implementation of semantic chunking in RAG systems.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start=&quot;13&quot;&gt; 
 &lt;li&gt;Contextual Compression 🗜️&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_compression.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/contextual_compression.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/contextual_compression.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Overview 🔎&lt;/h4&gt; 
&lt;p&gt;Compressing retrieved information while preserving query-relevant content.&lt;/p&gt; 
&lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
&lt;p&gt;Use an LLM to compress or summarize retrieved chunks, preserving key information relevant to the query.&lt;/p&gt; 
&lt;ol start=&quot;14&quot;&gt; 
 &lt;li&gt;Document Augmentation through Question Generation for Enhanced Retrieval&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/document_augmentation.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/document_augmentation.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/document_augmentation.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Overview 🔎&lt;/h4&gt; 
&lt;p&gt;This implementation demonstrates a text augmentation technique that leverages additional question generation to improve document retrieval within a vector database. By generating and incorporating various questions related to each text fragment, the system enhances the standard retrieval process, thus increasing the likelihood of finding relevant documents that can be utilized as context for generative question answering.&lt;/p&gt; 
&lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
&lt;p&gt;Use an LLM to augment text dataset with all possible questions that can be asked to each document.&lt;/p&gt; 
&lt;h3&gt;🚀 Advanced Retrieval Methods&lt;/h3&gt; 
&lt;ol start=&quot;15&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Fusion Retrieval 🔗&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/fusion_retrieval_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/fusion_retrieval.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Optimizing search results by combining different retrieval methods.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Combine keyword-based search with vector-based search for more comprehensive and accurate retrieval.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Intelligent Reranking 📈&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;LlamaIndex&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/reranking_with_llamaindex.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/reranking.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Applying advanced scoring mechanisms to improve the relevance ranking of retrieved results.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;🧠 &lt;strong&gt;LLM-based Scoring:&lt;/strong&gt; Use a language model to score the relevance of each retrieved chunk.&lt;/li&gt; 
   &lt;li&gt;🔀 &lt;strong&gt;Cross-Encoder Models:&lt;/strong&gt; Re-encode both the query and retrieved documents jointly for similarity scoring.&lt;/li&gt; 
   &lt;li&gt;🏆 &lt;strong&gt;Metadata-enhanced Ranking:&lt;/strong&gt; Incorporate metadata into the scoring process for more nuanced ranking.&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/relevance-revolution-how-re-ranking?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&quot;&gt;Relevance Revolution: How Re-ranking Transforms RAG Systems&lt;/a&gt;&lt;/strong&gt; - A comprehensive blog post exploring the power of re-ranking in enhancing RAG system performance.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multi-faceted Filtering 🔍&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Applying various filtering techniques to refine and improve the quality of retrieved results.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;🏷️ &lt;strong&gt;Metadata Filtering:&lt;/strong&gt; Apply filters based on attributes like date, source, author, or document type.&lt;/li&gt; 
   &lt;li&gt;📊 &lt;strong&gt;Similarity Thresholds:&lt;/strong&gt; Set thresholds for relevance scores to keep only the most pertinent results.&lt;/li&gt; 
   &lt;li&gt;📄 &lt;strong&gt;Content Filtering:&lt;/strong&gt; Remove results that don&#39;t match specific content criteria or essential keywords.&lt;/li&gt; 
   &lt;li&gt;🌈 &lt;strong&gt;Diversity Filtering:&lt;/strong&gt; Ensure result diversity by filtering out near-duplicate entries.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Hierarchical Indices 🗂️&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/hierarchical_indices.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/hierarchical_indices.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/hierarchical_indices.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Creating a multi-tiered system for efficient information navigation and retrieval.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Implement a two-tiered system for document summaries and detailed chunks, both containing metadata pointing to the same location in the data.&lt;/p&gt; &lt;h4&gt;Additional Resources 📚&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://open.substack.com/pub/diamantai/p/hierarchical-indices-enhancing-rag?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&quot;&gt;Hierarchical Indices: Enhancing RAG Systems&lt;/a&gt;&lt;/strong&gt; - A comprehensive blog post exploring the power of hierarchical indices in enhancing RAG system performance.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ensemble Retrieval 🎭&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Combining multiple retrieval models or techniques for more robust and accurate results.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Apply different embedding models or retrieval algorithms and use voting or weighting mechanisms to determine the final set of retrieved documents.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Dartboard Retrieval 🎯&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/dartboard.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/dartboard.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Optimizing over Relevant Information Gain in Retrieval&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Combine both relevance and diversity into a single scoring function and directly optimize for it.&lt;/li&gt; 
   &lt;li&gt;POC showing plain simple RAG underperforming when the database is dense, and the dartboard retrieval outperforming it.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Multi-modal Retrieval 📽️&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Extending RAG capabilities to handle diverse data types for richer responses.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Multi-model RAG with Multimedia Captioning&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_model_rag_with_captioning.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_model_rag_with_captioning.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; - Caption and store all the other multimedia data like pdfs, ppts, etc., with text data in vector store and retrieve them together.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Multi-model RAG with Colpali&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_model_rag_with_colpali.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/multi_model_rag_with_colpali.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; - Instead of captioning convert all the data into image, then find the most relevant images and pass them to a vision large language model.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🔁 Iterative and Adaptive Techniques&lt;/h3&gt; 
&lt;ol start=&quot;22&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Retrieval with Feedback Loops 🔁&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/retrieval_with_feedback_loop.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/retrieval_with_feedback_loop.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/retrieval_with_feedback_loop.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Implementing mechanisms to learn from user interactions and improve future retrievals.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Collect and utilize user feedback on the relevance and quality of retrieved documents and generated responses to fine-tune retrieval and ranking models.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Adaptive Retrieval 🎯&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/adaptive_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/adaptive_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/adaptive_retrieval.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Dynamically adjusting retrieval strategies based on query types and user contexts.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Classify queries into different categories and use tailored retrieval strategies for each, considering user context and preferences.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Iterative Retrieval 🔄&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Performing multiple rounds of retrieval to refine and enhance result quality.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Use the LLM to analyze initial results and generate follow-up queries to fill in gaps or clarify information.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;📊 Evaluation&lt;/h3&gt; 
&lt;ol start=&quot;25&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepEval Evaluation&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_deep_eval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_deep_eval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; | Comprehensive RAG system evaluation |&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Performing evaluations Retrieval-Augmented Generation systems, by covering several metrics and creating test cases.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Use the &lt;code&gt;deepeval&lt;/code&gt; library to conduct test cases on correctness, faithfulness and contextual relevancy of RAG systems.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GroUSE Evaluation&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_grouse.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/evaluation/evaluation_grouse.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; | Contextually-grounded LLM evaluation |&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Evaluate the final stage of Retrieval-Augmented Generation using metrics of the GroUSE framework and meta-evaluate your custom LLM judge on GroUSE unit tests.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Use the &lt;code&gt;grouse&lt;/code&gt; package to evaluate contextually-grounded LLM generations with GPT-4 on the 6 metrics of the GroUSE framework and use unit tests to evaluate a custom Llama 3.1 405B evaluator.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🔬 Explainability and Transparency&lt;/h3&gt; 
&lt;ol start=&quot;27&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Explainable Retrieval 🔍&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/explainable_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/explainable_retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/explainable_retrieval.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Providing transparency in the retrieval process to enhance user trust and system refinement.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Explain why certain pieces of information were retrieved and how they relate to the query.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;🏗️ Advanced Architectures&lt;/h3&gt; 
&lt;ol start=&quot;28&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Graph RAG with Milvus Vector Database 🔍&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Graph RAG with Milvus&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graphrag_with_milvus_vectordb.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graphrag_with_milvus_vectordb.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A simple yet powerful approach to implement Graph RAG using Milvus vector databases. This technique significantly improves performance on complex multi-hop questions by combining relationship-based retrieval with vector search and reranking.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Store both text passages and relationship triplets (subject-predicate-object) in separate Milvus collections&lt;/li&gt; 
   &lt;li&gt;Perform multi-way retrieval by querying both collections&lt;/li&gt; 
   &lt;li&gt;Use an LLM to rerank retrieved relationships based on their relevance to the query&lt;/li&gt; 
   &lt;li&gt;Retrieve the final passages based on the most relevant relationships&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Knowledge Graph Integration (Graph RAG) 🕸️&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graph_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/graph_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/graph_rag.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Incorporating structured data from knowledge graphs to enrich context and improve retrieval.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Retrieve entities and their relationships from a knowledge graph relevant to the query, combining this structured data with unstructured text for more informative responses.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GraphRag (Microsoft) 🎯&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;GraphRag&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/Microsoft_GraphRag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/Microsoft_GraphRag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Microsoft GraphRAG (Open Source) is an advanced RAG system that integrates knowledge graphs to improve the performance of LLMs&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;• Analyze an input corpus by extracting entities, relationships from text units. generates summaries of each community and its constituents from the bottom-up.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval 🌳&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/raptor.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/raptor.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/raptor.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;Implementing a recursive approach to process and organize retrieved information in a tree structure.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;Use abstractive summarization to recursively process and summarize retrieved documents, organizing the information in a tree structure for hierarchical context.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Self RAG 🔁&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/self_rag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/self_rag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/self_rag.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A dynamic approach that combines retrieval-based and generation-based methods, adaptively deciding whether to use retrieved information and how to best utilize it in generating responses.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;• Implement a multi-step process including retrieval decision, document retrieval, relevance evaluation, response generation, support assessment, and utility evaluation to produce accurate, relevant, and useful outputs.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Corrective RAG 🔧&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;LangChain&lt;/strong&gt;: &lt;a href=&quot;https://github.com/NirDiamant/RAG_TECHNIQUES/raw/main/https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/crag.ipynb&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-View-blue&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/crag.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; height=&quot;20&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/all_rag_techniques_runnable_scripts/crag.py&quot;&gt;Runnable Script&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;A sophisticated RAG approach that dynamically evaluates and corrects the retrieval process, combining vector databases, web search, and language models for highly accurate and context-aware responses.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;• Integrate Retrieval Evaluator, Knowledge Refinement, Web Search Query Rewriter, and Response Generator components to create a system that adapts its information sourcing strategy based on relevance scores and combines multiple sources when necessary.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🌟 Special Advanced Technique 🌟&lt;/h2&gt; 
&lt;ol start=&quot;34&quot;&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/NirDiamant/Controllable-RAG-Agent&quot;&gt;Sophisticated Controllable Agent for Complex RAG Tasks 🤖&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;h4&gt;Overview 🔎&lt;/h4&gt; &lt;p&gt;An advanced RAG solution designed to tackle complex questions that simple semantic similarity-based retrieval cannot solve. This approach uses a sophisticated deterministic graph as the &quot;brain&quot; 🧠 of a highly controllable autonomous agent, capable of answering non-trivial questions from your own data.&lt;/p&gt; &lt;h4&gt;Implementation 🛠️&lt;/h4&gt; &lt;p&gt;• Implement a multi-step process involving question anonymization, high-level planning, task breakdown, adaptive information retrieval and question answering, continuous re-planning, and rigorous answer verification to ensure grounded and accurate responses.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To begin implementing these advanced RAG techniques in your projects:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone this repository: &lt;pre&gt;&lt;code&gt;git clone https://github.com/NirDiamant/RAG_Techniques.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Navigate to the technique you&#39;re interested in: &lt;pre&gt;&lt;code&gt;cd all_rag_techniques/technique-name
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Follow the detailed implementation guide in each technique&#39;s directory.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions from the community! If you have a new technique or improvement to suggest:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Fork the repository&lt;/li&gt; 
 &lt;li&gt;Create your feature branch: &lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Commit your changes: &lt;code&gt;git commit -m &#39;Add some AmazingFeature&#39;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Push to the branch: &lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Open a pull request&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/NirDiamant/RAG_Techniques/graphs/contributors&quot;&gt;&lt;img src=&quot;https://contrib.rocks/image?repo=NirDiamant/RAG_Techniques&quot; alt=&quot;Contributors&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under a custom non-commercial license - see the &lt;a href=&quot;https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;⭐️ If you find this repository helpful, please consider giving it a star!&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=main-readme&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;Keywords: RAG, Retrieval-Augmented Generation, NLP, AI, Machine Learning, Information Retrieval, Natural Language Processing, LLM, Embeddings, Semantic Search&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>maxim5/cs229-2018-autumn</title>
      <link>https://github.com/maxim5/cs229-2018-autumn</link>
      <description>&lt;p&gt;All notes and materials for the CS229: Machine Learning course by Stanford University&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CS229 Autumn 2018&lt;/h1&gt; 
&lt;p&gt;All lecture notes, slides and assignments for &lt;a href=&quot;http://cs229.stanford.edu/&quot;&gt;CS229: Machine Learning&lt;/a&gt; course by Stanford University.&lt;/p&gt; 
&lt;p&gt;The videos of all lectures are available &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&quot;&gt;on YouTube&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Useful links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/maxim5/cs229-2019-summer&quot;&gt;CS229 Summer 2019 edition&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>google-deepmind/deepmind-research</title>
      <link>https://github.com/google-deepmind/deepmind-research</link>
      <description>&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DeepMind Research&lt;/h1&gt; 
&lt;p&gt;This repository contains implementations and illustrative code to accompany DeepMind publications. Along with publishing papers to accompany research conducted at DeepMind, we release open-source &lt;a href=&quot;https://deepmind.com/research/open-source/open-source-environments/&quot;&gt;environments&lt;/a&gt;, &lt;a href=&quot;https://deepmind.com/research/open-source/open-source-datasets/&quot;&gt;data sets&lt;/a&gt;, and &lt;a href=&quot;https://deepmind.com/research/open-source/open-source-code/&quot;&gt;code&lt;/a&gt; to enable the broader research community to engage with our work and build upon it, with the ultimate goal of accelerating scientific progress to benefit society. For example, you can build on our implementations of the &lt;a href=&quot;https://github.com/deepmind/dqn&quot;&gt;Deep Q-Network&lt;/a&gt; or &lt;a href=&quot;https://github.com/deepmind/dnc&quot;&gt;Differential Neural Computer&lt;/a&gt;, or experiment in the same environments we use for our research, such as &lt;a href=&quot;https://github.com/deepmind/lab&quot;&gt;DeepMind Lab&lt;/a&gt; or &lt;a href=&quot;https://github.com/deepmind/pysc2&quot;&gt;StarCraft II&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you enjoy building tools, environments, software libraries, and other infrastructure of the kind listed below, you can view open positions to work in related areas on our &lt;a href=&quot;https://deepmind.com/careers/&quot;&gt;careers page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For a full list of our publications, please see &lt;a href=&quot;https://deepmind.com/research/publications/&quot;&gt;https://deepmind.com/research/publications/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Projects&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/fusion_tcv&quot;&gt;Magnetic control of tokamak plasmas through deep reinforcement learning&lt;/a&gt;, Nature 2022&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/density_functional_approximation_dm21&quot;&gt;Pushing the Frontiers of Density Functionals by Solving the Fractional Electron Problem&lt;/a&gt;, Science 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/pitfalls_static_language_models&quot;&gt;Mind the Gap: Assessing Temporal Generalization in Neural Language Models&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/tandem_dqn&quot;&gt;The Difficulty of Passive Learning in Deep Reinforcement Learning&lt;/a&gt;, NeurIPS 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/nowcasting&quot;&gt;Skilful precipitation nowcasting using deep generative models of radar&lt;/a&gt;, Nature 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/cadl&quot;&gt;Compute-Aided Design as Language&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/continual_learning&quot;&gt;Encoders and ensembles for continual learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/hierarchical_transformer_memory&quot;&gt;Towards mental time travel: a hierarchical memory for reinforcement learning agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/perceiver&quot;&gt;Perceiver IO: A General Architecture for Structured Inputs &amp;amp; Outputs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/neural_mip_solving&quot;&gt;Solving Mixed Integer Programs Using Neural Networks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/noisy_label&quot;&gt;A Realistic Simulation Framework for Learning with Label Noise&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/rapid_task_solving&quot;&gt;Rapid Task-Solving in Novel Environments&lt;/a&gt;, ICLR 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/wikigraphs&quot;&gt;WikiGraphs: A Wikipedia - Knowledge Graph Paired Dataset&lt;/a&gt;, TextGraphs 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/box_arrangement&quot;&gt;Behavior Priors for Efficient Reinforcement Learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/meshgraphnets&quot;&gt;Learning Mesh-Based Simulation with Graph Networks&lt;/a&gt;, ICLR 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/ogb_lsc&quot;&gt;Open Graph Benchmark - Large-Scale Challenge (OGB-LSC)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/synthetic_returns&quot;&gt;Synthetic Returns for Long-Term Credit Assignment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/galaxy_mergers&quot;&gt;A Deep Learning Approach for Characterizing Major Galaxy Mergers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/kfac_ferminet_alpha&quot;&gt;Better, Faster Fermionic Neural Networks&lt;/a&gt; (KFAC implementation)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/object_attention_for_reasoning&quot;&gt;Object-based attention for spatio-temporal reasoning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/enformer&quot;&gt;Effective gene expression prediction from sequence by integrating long-range interactions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/satore&quot;&gt;Satore: First-order logic saturation with atom rewriting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/nfnets&quot;&gt;Characterizing signal propagation to close the performance gap in unnormalized ResNets&lt;/a&gt;, ICLR 2021&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/adversarial_robustness&quot;&gt;Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/cmtouch&quot;&gt;Learning rich touch representations through cross-modal self-supervision&lt;/a&gt;, CoRL 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/functional_regularisation_for_continual_learning&quot;&gt;Functional Regularisation for Continual Learning&lt;/a&gt;, ICLR 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/avae&quot;&gt;The Autoencoding Variational Autoencoder&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/mmv&quot;&gt;Self-Supervised MultiModal Versatile Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/ode_gan&quot;&gt;ODE-GAN: Training GANs by Solving Ordinary Differential Equations&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/causal_reasoning&quot;&gt;Algorithms for Causal Reasoning in Probability Trees&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/gated_linear_networks&quot;&gt;Gated Linear Networks&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/himo&quot;&gt;Value-driven Hindsight Modelling&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/learned_free_energy_estimation&quot;&gt;Targeted free energy estimation via learned mappings&lt;/a&gt;, Journal of Chemical Physics 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/learning_to_simulate&quot;&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/a&gt;, ICML 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/physics_planning_games&quot;&gt;Physically Embedded Planning Problems&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/polygen&quot;&gt;PolyGen: PolyGen: An Autoregressive Generative Model of 3D Meshes&lt;/a&gt;, ICML 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/byol&quot;&gt;Bootstrap Your Own Latent&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/catch_carry&quot;&gt;Catch &amp;amp; Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks&lt;/a&gt;, SIGGRAPH 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/memo&quot;&gt;MEMO: A Deep Network For Flexible Combination Of Episodic Memories&lt;/a&gt;, ICLR 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/rl_unplugged&quot;&gt;RL Unplugged: Benchmarks for Offline Reinforcement Learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/geomancer&quot;&gt;Disentangling by Subspace Diffusion (GEOMANCER)&lt;/a&gt;, NeurIPS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/affordances_theory&quot;&gt;What can I do here? A theory of affordances in reinforcement learning&lt;/a&gt;, ICML 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/sketchy&quot;&gt;Scaling data-driven robotics with reward sketching and batch reinforcement learning&lt;/a&gt;, RSS 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/counterfactual_fairness&quot;&gt;Path-Specific Counterfactual Fairness&lt;/a&gt;, AAAI 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/option_keyboard&quot;&gt;The Option Keyboard: Combining Skills in Reinforcement Learning&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/visr&quot;&gt;VISR - Fast Task Inference with Variational Intrinsic Successor Features&lt;/a&gt;, ICLR 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/glassy_dynamics&quot;&gt;Unveiling the predictive power of static structure in glassy systems&lt;/a&gt;, Nature Physics 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/iodine&quot;&gt;Multi-Object Representation Learning with Iterative Variational Inference (IODINE)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/alphafold_casp13&quot;&gt;AlphaFold CASP13&lt;/a&gt;, Nature 2020&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/unrestricted_advx&quot;&gt;Unrestricted Adversarial Challenge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/hierarchical_probabilistic_unet&quot;&gt;Hierarchical Probabilistic U-Net (HPU-Net)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/scratchgan&quot;&gt;Training Language GANs from Scratch&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/tvt&quot;&gt;Temporal Value Transport&lt;/a&gt;, Nature Communications 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/curl&quot;&gt;Continual Unsupervised Representation Learning (CURL)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/transporter&quot;&gt;Unsupervised Learning of Object Keypoints (Transporter)&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/bigbigan&quot;&gt;BigBiGAN&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/cs_gan&quot;&gt;Deep Compressed Sensing&lt;/a&gt;, ICML 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/side_effects_penalties&quot;&gt;Side Effects Penalties&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/PrediNet&quot;&gt;PrediNet Architecture and Relations Game Datasets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/unsupervised_adversarial_training&quot;&gt;Unsupervised Adversarial Training&lt;/a&gt;, NeurIPS 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/graph_matching_networks&quot;&gt;Graph Matching Networks for Learning the Similarity of Graph Structured Objects&lt;/a&gt;, ICML 2019&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/regal&quot;&gt;REGAL: Transfer Learning for Fast Optimization of Computation Graphs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/ensemble_loss_landscape&quot;&gt;Deep Ensembles: A Loss Landscape Perspective&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/powerpropagation&quot;&gt;Powerpropagation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-deepmind/deepmind-research/master/physics_inspired_models&quot;&gt;Physics Inspired Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/langchain</title>
      <link>https://github.com/langchain-ai/langchain</link>
      <description>&lt;p&gt;🦜🔗 Build context-aware reasoning applications&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;docs/static/img/logo-dark.svg&quot; /&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;docs/static/img/logo-light.svg&quot; /&gt; 
 &lt;img alt=&quot;LangChain Logo&quot; src=&quot;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/logo-dark.svg?sanitize=true&quot; width=&quot;80%&quot; /&gt; 
&lt;/picture&gt; 
&lt;div&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square&quot; alt=&quot;Release Notes&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml&quot;&gt;&lt;img src=&quot;https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml/badge.svg?sanitize=true&quot; alt=&quot;CI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/langchain-core?style=flat-square&quot; alt=&quot;PyPI - License&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypistats.org/packages/langchain-core&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/dm/langchain-core?style=flat-square&quot; alt=&quot;PyPI - Downloads&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://star-history.com/#langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/langchain-ai/langchain?style=flat-square&quot; alt=&quot;GitHub star chart&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/langchain-ai/langchain/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/langchain-ai/langchain?style=flat-square&quot; alt=&quot;Open Issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&amp;amp;style=flat-square&quot; alt=&quot;Open in Dev Containers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codespaces.new/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in Github Codespace&quot; title=&quot;Open in Github Codespace&quot; width=&quot;150&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/langchainai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&amp;amp;label=Follow%20%40LangChainAI&quot; alt=&quot;Twitter&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codspeed.io/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://codspeed.io/badge.json&quot; alt=&quot;CodSpeed Badge&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JS/TS library? Check out &lt;a href=&quot;https://github.com/langchain-ai/langchainjs&quot;&gt;LangChain.js&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development — all while future-proofing decisions as the underlying technology evolves.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U langchain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To learn more about LangChain, check out &lt;a href=&quot;https://python.langchain.com/docs/introduction/&quot;&gt;the docs&lt;/a&gt;. If you’re looking for more advanced customization or agent orchestration, check out &lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt;, our framework for building controllable agent workflows.&lt;/p&gt; 
&lt;h2&gt;Why use LangChain?&lt;/h2&gt; 
&lt;p&gt;LangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.&lt;/p&gt; 
&lt;p&gt;Use LangChain for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time data augmentation&lt;/strong&gt;. Easily connect LLMs to diverse data sources and external / internal systems, drawing from LangChain’s vast library of integrations with model providers, tools, vector stores, retrievers, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model interoperability&lt;/strong&gt;. Swap models in and out as your engineering team experiments to find the best choice for your application’s needs. As the industry frontier evolves, adapt quickly — LangChain’s abstractions keep you moving without losing momentum.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;LangChain’s ecosystem&lt;/h2&gt; 
&lt;p&gt;While the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.&lt;/p&gt; 
&lt;p&gt;To improve your LLM application development, pair LangChain with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.langchain.com/langsmith&quot;&gt;LangSmith&lt;/a&gt; - Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt; - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows — and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/&quot;&gt;LangGraph Platform&lt;/a&gt; - Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/&quot;&gt;LangGraph Studio&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/tutorials/&quot;&gt;Tutorials&lt;/a&gt;: Simple walkthroughs with guided examples on getting started with LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/how_to/&quot;&gt;How-to Guides&lt;/a&gt;: Quick, actionable code snippets for topics such as tool calling, RAG use cases, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/concepts/&quot;&gt;Conceptual Guides&lt;/a&gt;: Explanations of key concepts behind the LangChain framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://forum.langchain.com/&quot;&gt;LangChain Forum&lt;/a&gt;: Connect with the community and share all of your technical questions, ideas, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/api_reference/&quot;&gt;API Reference&lt;/a&gt;: Detailed reference on navigating base packages and integrations for LangChain.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>unslothai/notebooks</title>
      <link>https://github.com/unslothai/notebooks</link>
      <description>&lt;p&gt;100+ Fine-tuning LLM Notebooks on Google Colab, Kaggle, and more.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://unsloth.ai&quot;&gt;
   &lt;picture&gt; 
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png&quot; /&gt; 
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot; /&gt; 
    &lt;img alt=&quot;unsloth logo&quot; src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png&quot; height=&quot;110&quot; style=&quot;max-width: 100%;&quot; /&gt; 
   &lt;/picture&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png&quot; height=&quot;48&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/unsloth&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png&quot; height=&quot;48&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://docs.unsloth.ai&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png&quot; height=&quot;48&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;📒 Fine-tuning Notebooks&lt;/h2&gt; 
&lt;p&gt;Below are our notebooks for Google Colab categorized by model. You can view our &lt;a href=&quot;https://github.com/unslothai/notebooks/#-kaggle-notebooks&quot;&gt;Kaggle notebooks here&lt;/a&gt;.&lt;br /&gt;Use our guided notebooks to prep data, train, evaluate, and save your model. View our main &lt;a href=&quot;https://github.com/unslothai/unsloth&quot;&gt;GitHub repo here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Main Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Multimodal&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open in Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_%2814B%29-Reasoning-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3-Base (4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_%284B%29-GRPO.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma 3 (4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_%284B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%281B_and_3B%29-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi-4 (14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 Vision (11B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%2811B%29-Vision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.1 (8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_%288B%29-Alpaca.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3 (7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_%287B%29-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;DeepSeek-R1-0528-Qwen3 (8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/DeepSeek_R1_0528_Qwen3_(8B)_GRPO.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 (3B) by Meta&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Synthetic Data&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta_Synthetic_Data_Llama3_2_%283B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Sesame-CSM (1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_%281B%29-TTS.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Text-to-Speech (TTS) Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Sesame-CSM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_%281B%29-TTS.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Orpheus-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_%283B%29-TTS.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Spark-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Spark_TTS_%280_5B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Oute-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Oute_TTS_%281B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Oute-TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Oute_TTS_%281B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llasa TTS (1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_%281B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llasa TTS (3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_%283B%29.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Whisper-Large-V3&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;STT&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Whisper.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Vision (Multimodal) Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama 3.2 (11B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_%2811B%29-Vision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5 VL (7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_VL_%287B%29-Vision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Pixtral (12B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_%2812B%29-Vision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;BERT Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;ModernBERT-large&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/timothelaborie/text_classification_scripts/blob/main/bert_classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Specific use-case Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Usecase&lt;/th&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text Classification&lt;/td&gt; 
   &lt;td&gt;Llama 3.1 (8B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/timothelaborie/text_classification_scripts/blob/main/unsloth_classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tool Calling&lt;/td&gt; 
   &lt;td&gt;Qwen2.5-Coder (1.5B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(1.5B)-Tool_Calling.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multiple Datasets&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1njCCbE1YVal9xC83hjdo2hiGItpY_D6t?usp=sharing&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;KTO&lt;/td&gt; 
   &lt;td&gt;Qwen2.5-Instruct (1.5B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/drive/1MRgGtLWuZX4ypSfGguFgC-IblTvO2ivM?usp=sharing&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference Chat UI&lt;/td&gt; 
   &lt;td&gt;LLaMa 3.2 Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;LLaMa 3.2 (1B and 3B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ChatML&lt;/td&gt; 
   &lt;td&gt;Mistral (7B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/drive/15F1xyn8497_dUbxZP4zWmPZ3PJx1Oymv?usp=sharing&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text Completion&lt;/td&gt; 
   &lt;td&gt;Mistral (7B)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- 🛑 🚨 DO NOT EDIT MANUALLY THIS SECTION UNTIL `end of notebook links`!! 🛑 🚨 --&gt; 
&lt;!-- 🛑 🚨 THIS SECTION IS GENERATED BY `update_all_notebooks.py` AUTOMATICALLY 🛑 🚨  --&gt; 
&lt;!-- START OF EDITING --&gt; 
&lt;h3&gt;GRPO Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi 4&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Meta Synthetic Data Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta-Synthetic-Data-Llama3.1_(8B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Meta Synthetic Data Llama3 2&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta_Synthetic_Data_Llama3_2_(3B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(1B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(3B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;DeepSeek R1 0528 Qwen3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/DeepSeek_R1_0528_Qwen3_(8B)_GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;GRPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-GRPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;GPT-OSS Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;gpt oss&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;GPT OSS BNB&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_BNB_(20B)-Inference.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;GPT OSS MXFP4&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Gemma Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CodeGemma&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/CodeGemma_(7B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(2B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(2B)-Inference.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Multimodal&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Audio&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Audio.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma2&lt;/strong&gt; &lt;strong&gt;(9B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Gemma2&lt;/strong&gt; &lt;strong&gt;(2B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(2B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Linear Attention Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Liquid LFM2&lt;/strong&gt; &lt;strong&gt;(1.2B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Liquid_LFM2_(1.2B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Falcon H1&lt;/strong&gt; &lt;strong&gt;(0.5B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Falcon_H1_(0.5B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Llama Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(11B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(1B and 3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;RAFT&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B)-RAFT.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Inference.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llasa TTS&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_(3B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;ORPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-ORPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Ollama&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;TinyLlama&lt;/strong&gt; &lt;strong&gt;(1.1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/TinyLlama_(1.1B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Llasa TTS&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_(1B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Mistral Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral Small&lt;/strong&gt; &lt;strong&gt;(22B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Small_(22B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral Nemo&lt;/strong&gt; &lt;strong&gt;(12B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_Nemo_(12B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Pixtral&lt;/strong&gt; &lt;strong&gt;(12B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Pixtral_(12B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Text Completion&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Zephyr&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;DPO&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;CPT&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-CPT.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Orpheus Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Orpheus&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Oute Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Oute TTS&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Oute_TTS_(1B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Phi Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi 4&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi 3.5 Mini&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Phi 3 Medium&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3_Medium-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Qwen Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5 Coder&lt;/strong&gt; &lt;strong&gt;(1.5B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Tool Calling&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(1.5B)-Tool_Calling.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5 Coder&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_Coder_(14B)-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2.5 VL&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_VL_(7B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2 VL&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Vision&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Qwen2&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alpaca&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_(7B)-Alpaca.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Spark Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Spark TTS&lt;/strong&gt; &lt;strong&gt;(0 5B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Spark_TTS_(0_5B).ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Whisper Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Whisper.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Other Notebooks&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Notebook Link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Magistral&lt;/strong&gt; &lt;strong&gt;(24B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning Conversational&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Magistral_(24B)-Reasoning-Conversational.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Sesame CSM&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;TTS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_(1B)-TTS.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Unsloth&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Studio&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CodeForces cot Finetune for Reasoning on CodeForces&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Reasoning&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/CodeForces-cot-Finetune_for_Reasoning_on_CodeForces.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;📒 Kaggle Notebooks&lt;/h1&gt; 
&lt;details&gt; 
 &lt;summary&gt; Click for all our Kaggle notebooks categorized by model: &lt;/summary&gt; 
 &lt;h3&gt;GRPO Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Phi 4&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_4_(14B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Meta Synthetic Data Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Meta-Synthetic-Data-Llama3.1_(8B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.1_(8B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3_(1B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Meta Synthetic Data Llama3 2&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Meta_Synthetic_Data_Llama3_2_(3B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(4B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2.5&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_(3B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;DeepSeek R1 0528 Qwen3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-DeepSeek_R1_0528_Qwen3_(8B)_GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;GRPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-GRPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;GPT-OSS Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;GPT OSS BNB&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Inference&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-GPT_OSS_BNB_(20B)-Inference.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;gpt oss&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-gpt-oss-(20B)-Fine-tuning.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;GPT OSS MXFP4&lt;/strong&gt; &lt;strong&gt;(20B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Inference&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-GPT_OSS_MXFP4_(20B)-Inference.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Gemma Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;CodeGemma&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-CodeGemma_(7B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3_(4B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Audio&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3N_(4B)-Audio.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(2B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Inference&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3N_(2B)-Inference.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3N_(4B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3_(4B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma3N&lt;/strong&gt; &lt;strong&gt;(4B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Multimodal&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma3N_(4B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma2&lt;/strong&gt; &lt;strong&gt;(2B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma2_(2B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Gemma2&lt;/strong&gt; &lt;strong&gt;(9B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Gemma2_(9B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Linear Attention Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Liquid LFM2&lt;/strong&gt; &lt;strong&gt;(1.2B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Liquid_LFM2_(1.2B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Falcon H1&lt;/strong&gt; &lt;strong&gt;(0.5B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Falcon_H1_(0.5B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Llama Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(1B and 3B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.2_(1B_and_3B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(11B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.2_(11B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.2&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;RAFT&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.2_(1B)-RAFT.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Inference&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.1_(8B)-Inference.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3.1&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3.1_(8B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llasa TTS&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llasa_TTS_(3B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Ollama&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-Ollama.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;ORPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-ORPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llama3&lt;/strong&gt; &lt;strong&gt;(8B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llama3_(8B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;TinyLlama&lt;/strong&gt; &lt;strong&gt;(1.1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-TinyLlama_(1.1B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Llasa TTS&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Llasa_TTS_(1B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Mistral Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral Small&lt;/strong&gt; &lt;strong&gt;(22B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_Small_(22B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral Nemo&lt;/strong&gt; &lt;strong&gt;(12B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_Nemo_(12B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Pixtral&lt;/strong&gt; &lt;strong&gt;(12B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Pixtral_(12B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Text Completion&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_(7B)-Text_Completion.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Zephyr&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;DPO&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Zephyr_(7B)-DPO.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;CPT&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-CPT.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Mistral v0.3&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Mistral_v0.3_(7B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Orpheus Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Orpheus&lt;/strong&gt; &lt;strong&gt;(3B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Orpheus_(3B)-TTS.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Oute Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Oute TTS&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Oute_TTS_(1B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Phi Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Phi 4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_4-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Phi 3.5 Mini&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_3.5_Mini-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Phi 3 Medium&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Phi_3_Medium-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Qwen Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(14B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(14B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen3&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Reasoning Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen3_(14B)-Reasoning-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2.5 Coder&lt;/strong&gt; &lt;strong&gt;(14B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_Coder_(14B)-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2.5&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_(7B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2.5 Coder&lt;/strong&gt; &lt;strong&gt;(1.5B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Tool Calling&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_Coder_(1.5B)-Tool_Calling.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2.5 VL&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2.5_VL_(7B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2 VL&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Vision&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2_VL_(7B)-Vision.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Qwen2&lt;/strong&gt; &lt;strong&gt;(7B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Alpaca&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Qwen2_(7B)-Alpaca.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Spark Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Spark TTS&lt;/strong&gt; &lt;strong&gt;(0 5B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Spark_TTS_(0_5B).ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Whisper Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Whisper&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Whisper.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h3&gt;Other Notebooks&lt;/h3&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Model&lt;/th&gt; 
    &lt;th&gt;Type&lt;/th&gt; 
    &lt;th&gt;Notebook Link&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Magistral&lt;/strong&gt; &lt;strong&gt;(24B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Reasoning Conversational&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Magistral_(24B)-Reasoning-Conversational.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Sesame CSM&lt;/strong&gt; &lt;strong&gt;(1B)&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;TTS&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Sesame_CSM_(1B)-TTS.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;CodeForces cot Finetune for Reasoning on CodeForces&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Reasoning&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-CodeForces-cot-Finetune_for_Reasoning_on_CodeForces.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;strong&gt;Unsloth&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;Studio&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.kaggle.com/notebooks/welcome?src=https://github.com/unslothai/notebooks/raw/main/nb/Kaggle-Unsloth_Studio.ipynb&amp;amp;accelerator=nvidiaTeslaT4&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;!-- End of Notebook Links --&gt; 
&lt;h1&gt;✨ Contributing to Notebooks&lt;/h1&gt; 
&lt;p&gt;If you&#39;d like to contribute to our notebooks, here&#39;s a guide to get you started:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Find the Template:&lt;/strong&gt; We&#39;ve provided a template notebook called &lt;code&gt;Template_Notebook.ipynb&lt;/code&gt; in the root directory of this project. This template contains the basic structure and formatting guidelines for all notebooks in this collection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create Your Notebook:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Make a copy of &lt;code&gt;Template_Notebook.ipynb&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Rename the copied file to follow this naming convention: 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;LLM Notebooks:&lt;/strong&gt; &lt;code&gt;&amp;lt;Model Name&amp;gt;-&amp;lt;Type&amp;gt;.ipynb&lt;/code&gt; (e.g., &lt;code&gt;Mistral_v0.3_(7B)-Alpaca.ipynb&lt;/code&gt;)&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Vision Notebooks:&lt;/strong&gt; &lt;code&gt;&amp;lt;Model Name&amp;gt;-Vision.ipynb&lt;/code&gt; (e.g., &lt;code&gt;Llava_v1.6_(7B)-Vision.ipynb&lt;/code&gt;)&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Example of &lt;code&gt;&amp;lt;Type&amp;gt;&lt;/code&gt;:&lt;/strong&gt; &lt;code&gt;Alpaca&lt;/code&gt;, &lt;code&gt;Conversational&lt;/code&gt;, &lt;code&gt;CPT&lt;/code&gt;, &lt;code&gt;DPO&lt;/code&gt;, &lt;code&gt;ORPO&lt;/code&gt;, &lt;code&gt;Text_Completion&lt;/code&gt;, &lt;code&gt;CSV&lt;/code&gt;, &lt;code&gt;Inference&lt;/code&gt;, &lt;code&gt;Unsloth_Studio&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;!-- *   Modify the content of your notebook, adding your code, explanations, and any other relevant information. Make sure to follow the structure and guidelines from the template. --&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Place in &lt;code&gt;original_template&lt;/code&gt;:&lt;/strong&gt; Once your notebook is ready, move it to the &lt;code&gt;original_template&lt;/code&gt; directory.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Update Notebooks:&lt;/strong&gt; Run the following command in your terminal: &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python update_all_notebooks.py
&lt;/code&gt;&lt;/pre&gt; This script will automatically: 
  &lt;ul&gt; 
   &lt;li&gt;Copy your notebook from &lt;code&gt;original_template&lt;/code&gt; to the &lt;code&gt;notebooks&lt;/code&gt; directory.&lt;/li&gt; 
   &lt;li&gt;Update the notebook&#39;s internal sections (like Installation, News) to ensure consistency.&lt;/li&gt; 
   &lt;li&gt;Add your notebook to the appropriate list in this &lt;code&gt;README.md&lt;/code&gt; file.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create a Pull Request:&lt;/strong&gt; After that, just create a pull request (PR) to merge your changes, making it available for everyone! 
  &lt;ul&gt; 
   &lt;li&gt;We appreciate your contributions and look forward to reviewing your notebooks!&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt;</description>
    </item>
    
    <item>
      <title>yandexdataschool/nlp_course</title>
      <link>https://github.com/yandexdataschool/nlp_course</link>
      <description>&lt;p&gt;YSDA course in Natural Language Processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;YSDA Natural Language Processing course&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;This is the 2024 version. For previous year&#39; course materials, go to &lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/tree/2023&quot;&gt;this branch&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Lecture and seminar materials for each week are in ./week* folders, see README.md for materials and instructions&lt;/li&gt; 
 &lt;li&gt;Any technical issues, ideas, bugs in course materials, contribution ideas - add an &lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/issues&quot;&gt;issue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Installing libraries and troubleshooting: &lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/issues/1&quot;&gt;this thread&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Syllabus&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week01_embeddings&quot;&gt;&lt;strong&gt;week01&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Word Embeddings&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Word embeddings. Distributional semantics. Count-based (pre-neural) methods. Word2Vec: learn vectors. GloVe: count, then learn. Evaluation: intrinsic vs extrinsic. Analysis and Interpretability. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_word_emb&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Seminar: Playing with word and sentence embeddings&lt;/li&gt; 
   &lt;li&gt;Homework: Embedding-based machine translation system&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week02_classification&quot;&gt;&lt;strong&gt;week02&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Text Classification&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Text classification: introduction and datasets. General framework: feature extractor + classifier. Classical approaches: Naive Bayes, MaxEnt (Logistic Regression), SVM. Neural Networks: General View, Convolutional Models, Recurrent Models. Practical Tips: Data Augmentation. Analysis and Interpretability. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_text_clf&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Seminar: Text classification with convolutional NNs.&lt;/li&gt; 
   &lt;li&gt;Homework: Statistical &amp;amp; neural text classification.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week03_lm&quot;&gt;&lt;strong&gt;week03&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Language Modeling&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Language Modeling: what does it mean? Left-to-right framework. N-gram language models. Neural Language Models: General View, Recurrent Models, Convolutional Models. Evaluation. Practical Tips: Weight Tying. Analysis and Interpretability. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_lang_models&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Seminar: Build a N-gram language model from scratch&lt;/li&gt; 
   &lt;li&gt;Homework: Neural LMs &amp;amp; smoothing in count-based models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week04_seq2seq&quot;&gt;&lt;strong&gt;week04&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Seq2seq and Attention&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Seq2seq Basics: Encoder-Decoder framework, Training, Simple Models, Inference (e.g., beam search). Attention: general, score functions, models. Transformer: self-attention, masked self-attention, multi-head attention; model architecture. Subword Segmentation (BPE). Analysis and Interpretability: functions of attention heads; probing for linguistic structure. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_seq2seq_attn&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Seminar: Basic sequence to sequence model&lt;/li&gt; 
   &lt;li&gt;Homework: Machine translation with attention&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week05_transfer&quot;&gt;&lt;strong&gt;week05&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Transfer Learning&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: What is Transfer Learning? Great idea 1: From Words to Words-in-Context (CoVe, ELMo). Great idea 2: From Replacing Embeddings to Replacing Models (GPT, BERT). (A Bit of) Adaptors. Analysis and Interpretability. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_transfer&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Homework: fine-tuning a pre-trained BERT model&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week06_llm&quot;&gt;&lt;strong&gt;week06&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;LLMs and Prompting&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Scaling laws. Emergent abilities. Prompting (aka &quot;in-context learning&quot;): techiques that work; questioning whether model &quot;understands&quot; prompts. Hypotheses for why and how in-context learning works. Analysis and Interpretability.&lt;/li&gt; 
   &lt;li&gt;Homework: manual prompt engneering and chain-of-thought reasoning&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week07_peft&quot;&gt;&lt;strong&gt;week07&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Transformer architecture and training&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: training tips for transformers; the evolution of transformer architecture from Vaswani et al (2017) to modern LLMs; parameter-efficient fine-tuning (PEFT)&lt;/li&gt; 
   &lt;li&gt;Homework: fine-tuning a large language model with PEFT algorithms&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week08_rlhf&quot;&gt;&lt;strong&gt;week08&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Reinforcement Learning from Human Feedback&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: model alignment, RLHF, case study of InstructGPT and ChatGPT&lt;/li&gt; 
   &lt;li&gt;Homework: fine-tune your own language model with RL (using HuggingFace &lt;code&gt;trl&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week_extra/domain_adaptation&quot;&gt;&lt;strong&gt;week09 (extra)&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Domain Adaptation in NLP&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: why do domain adaptation? Methods: reweighting, proxy labels, adversarial domain adaptation&lt;/li&gt; 
   &lt;li&gt;Optional homework: implement domain adaptation when fine-tuning BERT models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week10_efficiency&quot;&gt;&lt;strong&gt;week10&lt;/strong&gt;_&lt;/a&gt; &lt;strong&gt;Efficient Inference in NLP&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: how NLP models are deployed, a survey of compression and acceleration: quantization, sparsification, ACT &amp;amp; more&lt;/li&gt; 
   &lt;li&gt;Practice: implement RTN and GPTQ for 4-bit LLM quantization&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2024/week_extra/retrieval&quot;&gt;&lt;strong&gt;week11 (extra)&lt;/strong&gt;_&lt;/a&gt; &lt;strong&gt;Retrieval Augmented Language Models&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Guest lecture: retrieval in LMs, token-level retrieval (KNNLM &amp;amp; more), RAG, RETRO, tools: langchain , HF Agents, open problems&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributors &amp;amp; course staff&lt;/h1&gt; 
&lt;p&gt;Course materials and teaching performed by&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://lena-voita.github.io&quot;&gt;Elena Voita&lt;/a&gt; - course author&lt;/li&gt; 
 &lt;li&gt;[Mikhail Diskin] [Ignat Romanov] [Ruslan Svirschevski] - lectures&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.hse.ru/org/persons/831207784/?_gl=1%2a1hz2yht%2a_ga%2aMTg3MTM2ODIwMS4xNjk4NTEyODg5%2a_ga_D145P1R4PL%2aMTY5ODUxMjg4OC4xLjAuMTY5ODUxMjg4OC42MC4wLjA.&quot;&gt;Valentina Broner&lt;/a&gt; - course admin for on-campus students&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kovarsky&quot;&gt;Boris Kovarsky&lt;/a&gt;, &lt;a href=&quot;https://github.com/drt7&quot;&gt;David Talbot&lt;/a&gt;, &lt;a href=&quot;https://github.com/esgv&quot;&gt;Sergey Gubanov&lt;/a&gt;, &lt;a href=&quot;https://github.com/justheuristic&quot;&gt;Just Heuristic&lt;/a&gt; - help build course materials and/or held some classes&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/graphs/contributors&quot;&gt;30+ volunteers&lt;/a&gt; who contributed and refined the notebooks and course materials. Without their help, the course would not be what it is today&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://lk.yandexdataschool.ru/courses/2023-autumn/7.1171-avtomaticheskaia-obrabotka-tekstov/&quot;&gt;A mighty host of TAs&lt;/a&gt; who stoically grade hundreds of homework submissions from on-campus students each year&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/self-llm</title>
      <link>https://github.com/datawhalechina/self-llm</link>
      <description>&lt;p&gt;《开源大模型食用指南》针对中国宝宝量身打造的基于Linux环境快速微调（全参数/Lora）、部署国内外开源大模型（LLM）/多模态大模型（MLLM）教程&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/images/head-img.png&quot; /&gt; 
 &lt;h1&gt;开源大模型食用指南&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;中文 | &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/README_en.md&quot;&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;  本项目是一个围绕开源大模型、针对国内初学者、基于 Linux 平台的中国宝宝专属大模型教程，针对各类开源大模型提供包括环境配置、本地部署、高效微调等技能在内的全流程指导，简化开源大模型的部署、使用和应用流程，让更多的普通学生、研究者更好地使用开源大模型，帮助开源、自由的大模型更快融入到普通学习者的生活中。&lt;/p&gt; 
&lt;p&gt;  本项目的主要内容包括：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;基于 Linux 平台的开源 LLM 环境配置指南，针对不同模型要求提供不同的详细环境配置步骤；&lt;/li&gt; 
 &lt;li&gt;针对国内外主流开源 LLM 的部署使用教程，包括 LLaMA、ChatGLM、InternLM 等；&lt;/li&gt; 
 &lt;li&gt;开源 LLM 的部署应用指导，包括命令行调用、在线 Demo 部署、LangChain 框架集成等；&lt;/li&gt; 
 &lt;li&gt;开源 LLM 的全量微调、高效微调方法，包括分布式全量微调、LoRA、ptuning 等。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;  &lt;strong&gt;项目的主要内容就是教程，让更多的学生和未来的从业者了解和熟悉开源大模型的食用方法！任何人都可以提出issue或是提交PR，共同构建维护这个项目。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;  想要深度参与的同学可以联系我们，我们会将你加入到项目的维护者中。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;  &lt;em&gt;&lt;strong&gt;学习建议：本项目的学习建议是，先学习环境配置，然后再学习模型的部署使用，最后再学习微调。因为环境配置是基础，模型的部署使用是基础，微调是进阶。初学者可以选择Qwen1.5，InternLM2，MiniCPM等模型优先学习。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;  &lt;strong&gt;进阶学习推荐&lt;/strong&gt; ：如果您在学习完本项目后，希望更深入地理解大语言模型的核心原理，并渴望亲手从零开始训练属于自己的大模型，我们强烈推荐关注 Datawhale 的另一个开源项目—— &lt;a href=&quot;https://github.com/datawhalechina/happy-llm&quot;&gt;Happy-LLM 从零开始的大语言模型原理与实践教程&lt;/a&gt; 。该项目将带您深入探索大模型的底层机制，掌握完整的训练流程。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：如果有同学希望了解大模型的模型构成，以及从零手写RAG、Agent和Eval等任务，可以学习Datawhale的另一个项目&lt;a href=&quot;https://github.com/datawhalechina/tiny-universe&quot;&gt;Tiny-Universe&lt;/a&gt;，大模型是当下深度学习领域的热点，但现有的大部分大模型教程只在于教给大家如何调用api完成大模型的应用，而很少有人能够从原理层面讲清楚模型结构、RAG、Agent 以及 Eval。所以该仓库会提供全部手写，不采用调用api的形式，完成大模型的 RAG 、 Agent 、Eval 任务。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：考虑到有同学希望在学习本项目之前，希望学习大模型的理论部分，如果想要进一步深入学习 LLM 的理论基础，并在理论的基础上进一步认识、应用 LLM，可以参考 Datawhale 的 &lt;a href=&quot;https://github.com/datawhalechina/so-large-lm.git&quot;&gt;so-large-llm&lt;/a&gt;课程。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：如果有同学在学习本课程之后，想要自己动手开发大模型应用。同学们可以参考 Datawhale 的 &lt;a href=&quot;https://github.com/datawhalechina/llm-universe&quot;&gt;动手学大模型应用开发&lt;/a&gt; 课程，该项目是一个面向小白开发者的大模型应用开发教程，旨在基于阿里云服务器，结合个人知识库助手项目，向同学们完整的呈现大模型应用开发流程。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;项目意义&lt;/h2&gt; 
&lt;p&gt;  什么是大模型？&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;大模型（LLM）狭义上指基于深度学习算法进行训练的自然语言处理（NLP）模型，主要应用于自然语言理解和生成等领域，广义上还包括机器视觉（CV）大模型、多模态大模型和科学计算大模型等。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;  百模大战正值火热，开源 LLM 层出不穷。如今国内外已经涌现了众多优秀开源 LLM，国外如 LLaMA、Alpaca，国内如 ChatGLM、BaiChuan、InternLM（书生·浦语）等。开源 LLM 支持用户本地部署、私域微调，每一个人都可以在开源 LLM 的基础上打造专属于自己的独特大模型。&lt;/p&gt; 
&lt;p&gt;  然而，当前普通学生和用户想要使用这些大模型，需要具备一定的技术能力，才能完成模型的部署和使用。对于层出不穷又各有特色的开源 LLM，想要快速掌握一个开源 LLM 的应用方法，是一项比较有挑战的任务。&lt;/p&gt; 
&lt;p&gt;  本项目旨在首先基于核心贡献者的经验，实现国内外主流开源 LLM 的部署、使用与微调教程；在实现主流 LLM 的相关部分之后，我们希望充分聚集共创者，一起丰富这个开源 LLM 的世界，打造更多、更全面特色 LLM 的教程。星火点点，汇聚成海。&lt;/p&gt; 
&lt;p&gt;  &lt;em&gt;&lt;strong&gt;我们希望成为 LLM 与普罗大众的阶梯，以自由、平等的开源精神，拥抱更恢弘而辽阔的 LLM 世界。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;项目受众&lt;/h2&gt; 
&lt;p&gt;  本项目适合以下学习者：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;想要使用或体验 LLM，但无条件获得或使用相关 API；&lt;/li&gt; 
 &lt;li&gt;希望长期、低成本、大量应用 LLM；&lt;/li&gt; 
 &lt;li&gt;对开源 LLM 感兴趣，想要亲自上手开源 LLM；&lt;/li&gt; 
 &lt;li&gt;NLP 在学，希望进一步学习 LLM；&lt;/li&gt; 
 &lt;li&gt;希望结合开源 LLM，打造领域特色的私域 LLM；&lt;/li&gt; 
 &lt;li&gt;以及最广大、最普通的学生群体。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;项目规划及进展&lt;/h2&gt; 
&lt;p&gt;   本项目拟围绕开源 LLM 应用全流程组织，包括环境配置及使用、部署应用、微调等，每个部分覆盖主流及特点开源 LLM：&lt;/p&gt; 
&lt;h3&gt;Example 系列&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/Chat-%E5%AC%9B%E5%AC%9B/readme.md&quot;&gt;Chat-嬛嬛&lt;/a&gt;： Chat-甄嬛是利用《甄嬛传》剧本中所有关于甄嬛的台词和语句，基于LLM进行LoRA微调得到的模仿甄嬛语气的聊天语言模型。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/Tianji-%E5%A4%A9%E6%9C%BA/readme.md&quot;&gt;Tianji-天机&lt;/a&gt;：天机是一款基于人情世故社交场景，涵盖提示词工程 、智能体制作、 数据获取与模型微调、RAG 数据清洗与使用等全流程的大语言模型系统应用教程。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/AMchat-%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/readme.md&quot;&gt;AMChat&lt;/a&gt;: AM (Advanced Mathematics) chat 是一个集成了数学知识和高等数学习题及其解答的大语言模型。该模型使用 Math 和高等数学习题及其解析融合的数据集，基于 InternLM2-Math-7B 模型，通过 xtuner 微调，专门设计用于解答高等数学问题。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/%E6%95%B0%E5%AD%97%E7%94%9F%E5%91%BD/readme.md&quot;&gt;数字生命&lt;/a&gt;: 本项目将以我为原型，利用特制的数据集对大语言模型进行微调，致力于创造一个能够真正反映我的个性特征的AI数字人——包括但不限于我的语气、表达方式和思维模式等等，因此无论是日常聊天还是分享心情，它都以一种既熟悉又舒适的方式交流，仿佛我在他们身边一样。整个流程是可迁移复制的，亮点是数据集的制作。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;已支持模型&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/openai/gpt-oss-20b&quot;&gt;gpt-oss-20b&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; gpt-oss-20b vllm 部署调用&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; gpt-oss-20b EvalScope 并发评测&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; gpt-oss-20b lmstudio 本地部署调用&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; gpt-oss-20b Lora 微调及 SwanLab 可视化记录&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; gpt-oss-20b DPO 微调及 SwanLab 可视化记录&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zai-org/GLM-4.1V-Thinking&quot;&gt;GLM-4.1-Thinking&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/01-GLM-4%201V-Thinking%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4.1V-Thinking vLLM 部署调用&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/02-GLM-4%201V-Thinking%20Gradio%E9%83%A8%E7%BD%B2.md&quot;&gt;GLM-4.1V-Thinking Gradio部署&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/03-GLM-4%201V-Thinking%20LoRA%20%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;GLM-4.1V-Thinking Lora 微调及 SwanLab 可视化记录&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/GLM4.1V-Thinking-lora&quot;&gt;GLM-4.1V-Thinking Docker 镜像&lt;/a&gt; @林恒宇&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zai-org/GLM-4.5&quot;&gt;GLM-4.5-Air&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/01-GLM-4.5-Air-vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4.5-Air vLLM 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/02-GLM-4.5-Air%20EvalScope%20%E5%B9%B6%E5%8F%91%E6%B5%8B%E8%AF%95.md&quot;&gt;GLM-4.5-Air EvalScope 智商情商 &amp;amp;&amp;amp; 并发评测&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/03-GLM-4.5-Air-Lora%20%E5%8F%8A%20Swanlab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4.5-Air Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.compshare.cn/images/lUQhKDCeCdZW?referral_code=ELukJdQS3vvCwYIfgsQf2C&amp;amp;ytag=GPU_yy_github_selfllm&quot;&gt;GLM-4.5-Air Ucloud Docker 镜像&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/baidu/ERNIE-4.5-0.3B-PT&quot;&gt;ERNIE-4.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ERNIE-4.5/01-ERNIE-4.5-0.3B-PT%20Lora%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;ERNIE-4.5-0.3B-PT Lora 微调及 SwanLab 可视化记录&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/ERNIE-4.5-lora&quot;&gt;ERNIE-4.5-0.3B-PT Lora Docker 镜像&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Tencent-Hunyuan/Hunyuan-A13B&quot;&gt;Hunyuan-A13B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/01-Hunyuan-A13B-Instruct%20%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20Blog.md&quot;&gt;Hunyuan-A13B-Instruct 模型架构解析 Blog&lt;/a&gt; @卓堂越&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/03-Hunyuan-A13B-Instruct-SGLang%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Hunyuan-A13B-Instruct SGLang 部署调用&lt;/a&gt; @fancy&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/05-Hunyuan-A13B-Instruct-LoRA%E5%8F%8ASwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Hunyuan-A13B-Instruct Lora SwanLab 可视化微调&lt;/a&gt; @谢好冉&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Hunyuan-A13B-Instruct-lora&quot;&gt;Hunyuan-A13B-Instruct Lora Docker 镜像&lt;/a&gt; @谢好冉&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3&quot;&gt;Qwen3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/01-Qwen3-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90-Blog.md&quot;&gt;Qwen3 模型结构解析 Blog&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/02-Qwen3-8B-vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen3-8B vllm 部署调用&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/03-Qwen3-7B-Instruct%20Windows%20LMStudio%20%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen3-8B Windows LMStudio 部署调用&lt;/a&gt; @王熠明&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/04-Qwen3-8B%20EvalScope%E6%99%BA%E5%95%86%E6%83%85%E5%95%86%E8%AF%84%E6%B5%8B.md&quot;&gt;Qwen3-8B Evalscope 智商情商评测&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/05-Qwen3-8B-LoRA%E5%8F%8ASwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-8B Lora 微调及SwanLab 可视化记录&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/06-Qwen3-30B-A3B%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-30B-A3B 微调及SwanLab 可视化记录&lt;/a&gt; @高立业&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/07-Qwen3-Think-%E8%A7%A3%E5%AF%86-Blog.md&quot;&gt;Qwen3 Think 解密 Blog&lt;/a&gt; @樊奇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Qwen3&quot;&gt;Qwen3-8B Docker 镜像&lt;/a&gt; @高立业&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models//Qwen3/08-Qwen3_0_6B%E7%9A%84%E5%B0%8F%E6%A8%A1%E5%9E%8B%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8.md&quot;&gt;Qwen3-0.6B 的小模型有什么用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/09-Qwen3-1.7B-%E5%8C%BB%E5%AD%A6%E6%8E%A8%E7%90%86%E5%BC%8F%E5%AF%B9%E8%AF%9D%E5%BE%AE%E8%B0%83%20%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-1.7B 医学推理式对话微调 及 SwanLab 可视化记录&lt;/a&gt; @林泽毅&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/10-Qwen3-8B%20GRPO%E5%BE%AE%E8%B0%83%E5%8F%8A%E9%80%9A%E8%BF%87swanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;Qwen3-8B GRPO微调及通过swanlab可视化&lt;/a&gt; @郭宣伯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/MoonshotAI/Kimi-VL&quot;&gt;Kimi&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Kimi-VL/02-Kimi-VL-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB.md&quot;&gt;Kimi-VL-A3B 技术报告解读&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Kimi-VL/01-Kimi-VL-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B.md&quot;&gt;Kimi-VL-A3B-Thinking WebDemo 部署（网页对话助手）&lt;/a&gt; @姜舒凡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;&gt;Llama4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama4/01-Llama4-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B/01-Llama4-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B.md&quot;&gt;Llama4 对话助手&lt;/a&gt; @姜舒凡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/manycore-research/SpatialLM&quot;&gt;SpatialLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/SpatialLM/readme.md&quot;&gt;SpatialLM 3D点云理解与目标检测模型部署&lt;/a&gt; @王泽宇&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/tencent/Hunyuan3D-2&quot;&gt;Hunyuan3D-2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/01-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2.md&quot;&gt;Hunyuan3D-2 系列模型部署&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/02-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8.md&quot;&gt;Hunyuan3D-2 系列模型代码调用&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/03-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8BGradio%E9%83%A8%E7%BD%B2.md&quot;&gt;Hunyuan3D-2 系列模型Gradio部署&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/04-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8BAPI%20Server.md&quot;&gt;Hunyuan3D-2 系列模型API Server&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Hunyuan3D-2&quot;&gt;Hunyuan3D-2 Docker 镜像&lt;/a&gt; @林恒宇&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-3-4b-it&quot;&gt;Gemma3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/01-gemma-3-4b-it%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gemma-3-4b-it FastApi 部署调用&lt;/a&gt; @杜森&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/03-gemma-3-4b-it-ollama%20+%20open-webui%E9%83%A8%E7%BD%B2.md&quot;&gt;gemma-3-4b-it ollama + open-webui部署&lt;/a&gt; @孙超&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/04-Gemma3-4b%20%20evalscope%E6%99%BA%E5%95%86%E6%83%85%E5%95%86%E8%AF%84%E6%B5%8B.md&quot;&gt;gemma-3-4b-it evalscope 智商情商评测&lt;/a&gt; @张龙斐&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/05-gemma-3-4b-it%20LoRA.md&quot;&gt;gemma-3-4b-it Lora 微调&lt;/a&gt; @荞麦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/self-llm-gemma3&quot;&gt;gemma-3-4b-it Docker 镜像&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/6-gemma3-4B-itGRPO%E5%BE%AE%E8%B0%83%E5%8F%8A%E9%80%9A%E8%BF%87swanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;gemma-3-4b-it GRPO微调及通过swanlab可视化&lt;/a&gt; @郭宣伯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;&gt;DeepSeek-R1-Distill&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/01-DeepSeek-R1-Distill-Qwen-7B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B FastApi 部署调用&lt;/a&gt; @骆秀韬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/02-DeepSeek-R1-Distill-Qwen-7B%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B Langchain 接入&lt;/a&gt; @骆秀韬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/03-DeepSeek-R1-Distill-Qwen-7B%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B WebDemo 部署&lt;/a&gt; @骆秀韬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/04-DeepSeek-R1-Distill-Qwen-7B%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B vLLM 部署调用&lt;/a&gt; @骆秀韬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/05-DeepSeek-R1-0528-Qwen3-8B-GRPO%E5%8F%8Aswanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;DeepSeek-R1-0528-Qwen3-8B-GRPO及swanlab可视化&lt;/a&gt; @郭宣伯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM-o&quot;&gt;MiniCPM-o-2_6&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/01MiniCPM-o%202%206%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8%20.md&quot;&gt;minicpm-o-2.6 FastApi 部署调用&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/02minicpm-o-2.6WebDemo_streamlit.py&quot;&gt;minicpm-o-2.6 WebDemo 部署&lt;/a&gt; @程宏&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/03-MiniCPM-o-2.6%20%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%AD%E9%9F%B3%E8%83%BD%E5%8A%9B.md&quot;&gt;minicpm-o-2.6 多模态语音能力&lt;/a&gt; @邓恺俊&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/04-MiniCPM-0-2.6%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;minicpm-o-2.6 可视化 LaTeX_OCR Lora 微调&lt;/a&gt; @林泽毅&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM&quot;&gt;InternLM3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/01-InternLM3-8B-Instruct%20FastAPI.md&quot;&gt;internlm3-8b-instruct FastApi 部署调用&lt;/a&gt; @苏向标&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/02-internlm3-8b-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;internlm3-8b-instruct Langchian接入&lt;/a&gt; @赵文恺&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/03-InternLM3-8B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;internlm3-8b-instruct WebDemo 部署&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/04-InternLM3-8B-Instruct%20LoRA.md&quot;&gt;internlm3-8b-instruct Lora 微调&lt;/a&gt; @程宏&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/05-internlm3-8b-instruct%20%E4%B8%8Eo1%20.md&quot;&gt;internlm3-8b-instruct o1-like推理链实现&lt;/a&gt; @陈睿&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/microsoft/phi-4&quot;&gt;phi4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/01-Phi-4%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;phi4 FastApi 部署调用&lt;/a&gt; @杜森&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/02-Phi-4-Langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;phi4 langchain 接入&lt;/a&gt; @小罗&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/03-Phi-4%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;phi4 WebDemo 部署&lt;/a&gt; @杜森&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/04-Phi-4-Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;phi4 Lora 微调&lt;/a&gt; @郑远婧&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/05-Phi-4-Lora%20%E5%BE%AE%E8%B0%83%20%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB.md&quot;&gt;phi4 Lora 微调 NER任务 SwanLab 可视化记录版&lt;/a&gt; @林泽毅&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/06-Phi-4-GRPO%E5%8F%8Aswanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;phi4 GRPO微调及通过swanlab可视化&lt;/a&gt; @郭宣伯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Coder&quot;&gt;Qwen2.5-Coder&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/01-Qwen2.5-Coder-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-Coder-7B-Instruct FastApi部署调用&lt;/a&gt; @赵文恺&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/02-Qwen2.5-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2.5-Coder-7B-Instruct Langchian接入&lt;/a&gt; @杨晨旭&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/03-Qwen2.5-Coder-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2.5-Coder-7B-Instruct WebDemo 部署&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/04-Qwen2.5-Coder-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-Coder-7B-Instruct vLLM 部署&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/Qwen2.5-Coder-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2.5-Coder-7B-Instruct Lora 微调&lt;/a&gt; @荞麦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/05-Qwen2.5-Coder-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2.5-Coder-7B-Instruct Lora 微调 SwanLab 可视化记录版&lt;/a&gt; @杨卓&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2-VL&quot;&gt;Qwen2-vl&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/01-Qwen2-VL-2B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-vl-2B FastApi 部署调用&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/02-Qwen2-VL-2B-Instruct%20Web%20Demo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2-vl-2B WebDemo 部署&lt;/a&gt; @赵伟&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/03-Qwen2-VL-2B-Instruct%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-vl-2B vLLM 部署&lt;/a&gt; @荞麦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/04-Qwen2-VL-2B%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2-vl-2B Lora 微调&lt;/a&gt; @李柯辰&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/05-Qwen2-VL-2B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2-vl-2B Lora 微调 SwanLab 可视化记录版&lt;/a&gt; @林泽毅&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/06-Qwen2-VL-2B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%E6%A1%88%E4%BE%8B%20-%20LaTexOCR.md&quot;&gt;Qwen2-vl-2B Lora 微调案例 - LaTexOCR&lt;/a&gt; @林泽毅&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5&quot;&gt;Qwen2.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/01-Qwen2.5-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-7B-Instruct FastApi 部署调用&lt;/a&gt; @娄天奥&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/02-Qwen2.5-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2.5-7B-Instruct langchain 接入&lt;/a&gt; @娄天奥&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/03-Qwen2.5-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-7B-Instruct vLLM 部署调用&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/04-Qwen2_5-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2.5-7B-Instruct WebDemo 部署&lt;/a&gt; @高立业&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/05-Qwen2.5-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2.5-7B-Instruct Lora 微调&lt;/a&gt; @左春生&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/06-Qwen2.5-7B-Instruct%20o1-like%20%E6%8E%A8%E7%90%86%E9%93%BE%E5%AE%9E%E7%8E%B0.md&quot;&gt;Qwen2.5-7B-Instruct o1-like 推理链实现&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/07-Qwen2.5-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2.5-7B-Instruct Lora 微调 SwanLab 可视化记录版&lt;/a&gt; @林泽毅&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://machinelearning.apple.com/research/openelm&quot;&gt;Apple OpenELM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/OpenELM/01-OpenELM-3B-Instruct%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;OpenELM-3B-Instruct FastApi 部署调用&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/OpenELM/02-OpenELM-3B-Instruct%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;OpenELM-3B-Instruct Lora 微调&lt;/a&gt; @王泽宇&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct&quot;&gt;Llama3_1-8B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/01-Llama3_1-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Llama3_1-8B-Instruct FastApi 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/02-Llama3_1-8B-Instruct%20langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;Llama3_1-8B-Instruct langchain 接入&lt;/a&gt; @张晋&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/03-Llama3_1-8B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Llama3_1-8B-Instruct WebDemo 部署&lt;/a&gt; @张晋&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/04-Llama3_1-8B--Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Llama3_1-8B-Instruct Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/%E5%8A%A8%E6%89%8B%E8%BD%AC%E6%8D%A2GGUF%E6%A8%A1%E5%9E%8B%E5%B9%B6%E4%BD%BF%E7%94%A8Ollama%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2.md&quot;&gt;动手转换GGUF模型并使用Ollama本地部署&lt;/a&gt; @Gaoboy&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-2-9b-it&quot;&gt;Gemma-2-9b-it&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/01-Gemma-2-9b-it%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Gemma-2-9b-it FastApi 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/02-Gemma-2-9b-it%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Gemma-2-9b-it langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/03-Gemma-2-9b-it%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;Gemma-2-9b-it WebDemo 部署&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/04-Gemma-2-9b-it%20peft%20lora%E5%BE%AE%E8%B0%83.md&quot;&gt;Gemma-2-9b-it Peft Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/IEIT-Yuan/Yuan-2.0&quot;&gt;Yuan2.0&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/01-Yuan2.0-2B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-2B FastApi 部署调用&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/02-Yuan2.0-2B%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Yuan2.0-2B Langchain 接入&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/03-Yuan2.0-2B%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Yuan2.0-2B WebDemo部署&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/04-Yuan2.0-2B%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-2B vLLM部署调用&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/05-Yuan2.0-2B%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;Yuan2.0-2B Lora微调&lt;/a&gt; @张帆&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/IEIT-Yuan/Yuan2.0-M32&quot;&gt;Yuan2.0-M32&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/01-Yuan2.0-M32%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-M32 FastApi 部署调用&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/02-Yuan2.0-M32%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Yuan2.0-M32 Langchain 接入&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/03-Yuan2.0-M32%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Yuan2.0-M32 WebDemo部署&lt;/a&gt; @张帆&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-Coder-V2&quot;&gt;DeepSeek-Coder-V2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/01-DeepSeek-Coder-V2-Lite-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct FastApi 部署调用&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/02-DeepSeek-Coder-V2-Lite-Instruct%20%E6%8E%A5%E5%85%A5%20LangChain.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct langchain 接入&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/03-DeepSeek-Coder-V2-Lite-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct WebDemo 部署&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/04-DeepSeek-Coder-V2-Lite-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct Lora 微调&lt;/a&gt; @余洋&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/bilibili/Index-1.9B&quot;&gt;哔哩哔哩 Index-1.9B&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/01-Index-1.9B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Index-1.9B-Chat FastApi 部署调用&lt;/a&gt; @邓恺俊&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/02-Index-1.9B-Chat%20%E6%8E%A5%E5%85%A5%20LangChain.md&quot;&gt;Index-1.9B-Chat langchain 接入&lt;/a&gt; @张友东&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/03-Index-1.9B-chat%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Index-1.9B-Chat WebDemo 部署&lt;/a&gt; @程宏&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/04-Index-1.9B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Index-1.9B-Chat Lora 微调&lt;/a&gt; @姜舒凡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2&quot;&gt;Qwen2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/01-Qwen2-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-7B-Instruct FastApi 部署调用&lt;/a&gt; @康婧淇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/02-Qwen2-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2-7B-Instruct langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/03-Qwen2-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2-7B-Instruct WebDemo 部署&lt;/a&gt; @三水&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/04-Qwen2-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-7B-Instruct vLLM 部署调用&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/05-Qwen2-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2-7B-Instruct Lora 微调&lt;/a&gt; @散步&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/THUDM/GLM-4.git&quot;&gt;GLM-4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/01-GLM-4-9B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4-9B-chat FastApi 部署调用&lt;/a&gt; @张友东&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/02-GLM-4-9B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;GLM-4-9B-chat langchain 接入&lt;/a&gt; @谭逸珂&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/03-GLM-4-9B-Chat%20WebDemo.md&quot;&gt;GLM-4-9B-chat WebDemo 部署&lt;/a&gt; @何至轩&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/04-GLM-4-9B-Chat%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4-9B-chat vLLM 部署&lt;/a&gt; @王熠明&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/05-GLM-4-9B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4-9B-chat Lora 微调&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/05-GLM-4-9B-chat-hf%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4-9B-chat-hf Lora 微调&lt;/a&gt; @付志远&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen1.5.git&quot;&gt;Qwen 1.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/01-Qwen1.5-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen1.5-7B-chat FastApi 部署调用&lt;/a&gt; @颜鑫&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/02-Qwen1.5-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Qwen1.5-7B-chat langchain 接入&lt;/a&gt; @颜鑫&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/03-Qwen1.5-7B-Chat%20WebDemo.md&quot;&gt;Qwen1.5-7B-chat WebDemo 部署&lt;/a&gt; @颜鑫&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/04-Qwen1.5-7B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen1.5-7B-chat Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/05-Qwen1.5-7B-Chat-GPTQ-Int4%20%20WebDemo.md&quot;&gt;Qwen1.5-72B-chat-GPTQ-Int4 部署环境&lt;/a&gt; @byx020119&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/06-Qwen1.5-MoE-A2.7B.md&quot;&gt;Qwen1.5-MoE-chat Transformers 部署调用&lt;/a&gt; @丁悦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/07-Qwen1.5-7B-Chat%20vLLM%20%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen1.5-7B-chat vLLM推理部署&lt;/a&gt; @高立业&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/08-Qwen1.5-7B-chat%20LoRA%E5%BE%AE%E8%B0%83%E6%8E%A5%E5%85%A5%E5%AE%9E%E9%AA%8C%E7%AE%A1%E7%90%86.md&quot;&gt;Qwen1.5-7B-chat Lora 微调 接入SwanLab实验管理平台&lt;/a&gt; @黄柏特&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-7b-it&quot;&gt;谷歌-Gemma&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/01-Gemma-2B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gemma-2b-it FastApi 部署调用 &lt;/a&gt; @东东&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/02-Gemma-2B-Instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;gemma-2b-it langchain 接入 &lt;/a&gt; @东东&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/03-Gemma-2B-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;gemma-2b-it WebDemo 部署 &lt;/a&gt; @东东&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/04-Gemma-2B-Instruct%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;gemma-2b-it Peft Lora 微调 &lt;/a&gt; @东东&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct&quot;&gt;phi-3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/01-Phi-3-mini-4k-instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Phi-3-mini-4k-instruct FastApi 部署调用&lt;/a&gt; @郑皓桦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/02-Phi-3-mini-4k-instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Phi-3-mini-4k-instruct langchain 接入&lt;/a&gt; @郑皓桦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/03-Phi-3-mini-4k-instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Phi-3-mini-4k-instruct WebDemo 部署&lt;/a&gt; @丁悦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/04-Phi-3-mini-4k-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Phi-3-mini-4k-instruct Lora 微调&lt;/a&gt; @丁悦&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/thu-coai/CharacterGLM-6B&quot;&gt;CharacterGLM-6B&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/01-CharacterGLM-6B%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;CharacterGLM-6B Transformers 部署调用&lt;/a&gt; @孙健壮&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/02-CharacterGLM-6B%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;CharacterGLM-6B FastApi 部署调用&lt;/a&gt; @孙健壮&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/03-CharacterGLM-6B-chat.md&quot;&gt;CharacterGLM-6B webdemo 部署&lt;/a&gt; @孙健壮&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/04-CharacterGLM-6B%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;CharacterGLM-6B Lora 微调&lt;/a&gt; @孙健壮&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/meta-llama/llama3.git&quot;&gt;LLaMA3-8B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/01-LLaMA3-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;LLaMA3-8B-Instruct FastApi 部署调用&lt;/a&gt; @高立业&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/02-LLaMA3-8B-Instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;LLaMA3-8B-Instruct langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/03-LLaMA3-8B-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;LLaMA3-8B-Instruct WebDemo 部署&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/04-LLaMA3-8B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;LLaMA3-8B-Instruct Lora 微调&lt;/a&gt; @高立业&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://modelscope.cn/models/xverse/XVERSE-7B-Chat/summary&quot;&gt;XVERSE-7B-Chat&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/01-XVERSE-7B-chat%20Transformers%E6%8E%A8%E7%90%86.md&quot;&gt;XVERSE-7B-Chat transformers 部署调用&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/02-XVERSE-7B-chat%20FastAPI%E9%83%A8%E7%BD%B2.md&quot;&gt;XVERSE-7B-Chat FastApi 部署调用&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/03-XVERSE-7B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;XVERSE-7B-Chat langchain 接入&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/04-XVERSE-7B-chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;XVERSE-7B-Chat WebDemo 部署&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/05-XVERSE-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;XVERSE-7B-Chat Lora 微调&lt;/a&gt; @郭志航&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenNLPLab/TransnormerLLM.git&quot;&gt;TransNormerLLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/01-TransNormer-7B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;TransNormerLLM-7B-Chat FastApi 部署调用&lt;/a&gt; @王茂霖&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/02-TransNormer-7B%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;TransNormerLLM-7B-Chat langchain 接入&lt;/a&gt; @王茂霖&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/03-TransNormer-7B%20WebDemo.md&quot;&gt;TransNormerLLM-7B-Chat WebDemo 部署&lt;/a&gt; @王茂霖&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/04-TrasnNormer-7B%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;TransNormerLLM-7B-Chat Lora 微调&lt;/a&gt; @王茂霖&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/vivo-ai-lab/BlueLM.git&quot;&gt;BlueLM Vivo 蓝心大模型&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/01-BlueLM-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2.md&quot;&gt;BlueLM-7B-Chat FatApi 部署调用&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/02-BlueLM-7B-Chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;BlueLM-7B-Chat langchain 接入&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/03-BlueLM-7B-Chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;BlueLM-7B-Chat WebDemo 部署&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/04-BlueLM-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;BlueLM-7B-Chat Lora 微调&lt;/a&gt; @郭志航&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM&quot;&gt;InternLM2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/01-InternLM2-7B-chat%20FastAPI%E9%83%A8%E7%BD%B2.md&quot;&gt;InternLM2-7B-chat FastApi 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/02-InternLM2-7B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;InternLM2-7B-chat langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/03-InternLM2-7B-chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;InternLM2-7B-chat WebDemo 部署&lt;/a&gt; @郑皓桦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/04-InternLM2-7B-chat%20Xtuner%20Qlora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;InternLM2-7B-chat Xtuner Qlora 微调&lt;/a&gt; @郑皓桦&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-LLM&quot;&gt;DeepSeek 深度求索&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/01-DeepSeek-7B-chat%20FastApi.md&quot;&gt;DeepSeek-7B-chat FastApi 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/02-DeepSeek-7B-chat%20langchain.md&quot;&gt;DeepSeek-7B-chat langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/03-DeepSeek-7B-chat%20WebDemo.md&quot;&gt;DeepSeek-7B-chat WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/04-DeepSeek-7B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-7B-chat Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/05-DeepSeek-7B-chat%204bits%E9%87%8F%E5%8C%96%20Qlora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-7B-chat 4bits量化 Qlora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/06-DeepSeek-MoE-16b-chat%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-MoE-16b-chat Transformers 部署调用&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/06-DeepSeek-MoE-16b-chat%20FastApi.md&quot;&gt;DeepSeek-MoE-16b-chat FastApi 部署调用&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/07-deepseek_fine_tune.ipynb&quot;&gt;DeepSeek-coder-6.7b finetune colab&lt;/a&gt; @Swiftie&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/08-deepseek_web_demo.ipynb&quot;&gt;Deepseek-coder-6.7b webdemo colab&lt;/a&gt; @Swiftie&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM.git&quot;&gt;MiniCPM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20transformers%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;MiniCPM-2B-chat transformers 部署调用&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;MiniCPM-2B-chat FastApi 部署调用&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;MiniCPM-2B-chat langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;MiniCPM-2B-chat webdemo 部署&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20Lora%20&amp;amp;&amp;amp;%20Full%20%E5%BE%AE%E8%B0%83.md&quot;&gt;MiniCPM-2B-chat Lora &amp;amp;&amp;amp; Full 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; 官方友情链接：&lt;a href=&quot;https://modelbest.feishu.cn/wiki/D2tFw8Pcsi5CIzkaHNacLK64npg&quot;&gt;面壁小钢炮MiniCPM教程&lt;/a&gt; @OpenBMB&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; 官方友情链接：&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM-CookBook&quot;&gt;MiniCPM-Cookbook&lt;/a&gt; @OpenBMB&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen-Audio.git&quot;&gt;Qwen-Audio&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen-Audio/01-Qwen-Audio-chat%20FastApi.md&quot;&gt;Qwen-Audio FastApi 部署调用&lt;/a&gt; @陈思州&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen-Audio/02-Qwen-Audio-chat%20WebDemo.md&quot;&gt;Qwen-Audio WebDemo&lt;/a&gt; @陈思州&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen.git&quot;&gt;Qwen&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/01-Qwen-7B-Chat%20Transformers%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen-7B-chat Transformers 部署调用&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/02-Qwen-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen-7B-chat FastApi 部署调用&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/03-Qwen-7B-Chat%20WebDemo.md&quot;&gt;Qwen-7B-chat WebDemo&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/04-Qwen-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/05-Qwen-7B-Chat%20Ptuning%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat ptuning 微调&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/06-Qwen-7B-chat%20%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat 全量微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/07-Qwen-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Qwen-7B-Chat 接入langchain搭建知识库助手&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/08-Qwen-7B-Chat%20Lora%20%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat 低精度训练&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/09-Qwen-1_8B-chat%20CPU%20%E9%83%A8%E7%BD%B2%20.md&quot;&gt;Qwen-1_8B-chat CPU 部署&lt;/a&gt; @散步&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/01-ai/Yi.git&quot;&gt;Yi 零一万物&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/01-Yi-6B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yi-6B-chat FastApi 部署调用&lt;/a&gt; @李柯辰&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/02-Yi-6B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Yi-6B-chat langchain接入&lt;/a&gt; @李柯辰&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/03-Yi-6B-chat%20WebDemo.md&quot;&gt;Yi-6B-chat WebDemo&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/04-Yi-6B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Yi-6B-chat Lora 微调&lt;/a&gt; @李娇娇&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.baichuan-ai.com/home&quot;&gt;Baichuan 百川智能&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/BaiChuan/01-Baichuan2-7B-chat%2BFastApi%2B%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Baichuan2-7B-chat FastApi 部署调用&lt;/a&gt; @惠佳豪&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/02-Baichuan-7B-chat%2BWebDemo.md&quot;&gt;Baichuan2-7B-chat WebDemo&lt;/a&gt; @惠佳豪&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/03-Baichuan2-7B-chat%E6%8E%A5%E5%85%A5LangChain%E6%A1%86%E6%9E%B6.md&quot;&gt;Baichuan2-7B-chat 接入 LangChain 框架&lt;/a&gt; @惠佳豪&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/04-Baichuan2-7B-chat%2Blora%2B%E5%BE%AE%E8%B0%83.md&quot;&gt;Baichuan2-7B-chat Lora 微调&lt;/a&gt; @惠佳豪&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM.git&quot;&gt;InternLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/01-InternLM-Chat-7B%20Transformers%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;InternLM-Chat-7B Transformers 部署调用&lt;/a&gt; @小罗&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/02-internLM-Chat-7B%20FastApi.md&quot;&gt;InternLM-Chat-7B FastApi 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/03-InternLM-Chat-7B.md&quot;&gt;InternLM-Chat-7B WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/04-Lagent+InternLM-Chat-7B-V1.1.md&quot;&gt;Lagent+InternLM-Chat-7B-V1.1 WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/05-%E6%B5%A6%E8%AF%AD%E7%81%B5%E7%AC%94%E5%9B%BE%E6%96%87%E7%90%86%E8%A7%A3&amp;amp;%E5%88%9B%E4%BD%9C.md&quot;&gt;浦语灵笔图文理解&amp;amp;创作 WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/06-InternLM%E6%8E%A5%E5%85%A5LangChain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;InternLM-Chat-7B 接入 LangChain 框架&lt;/a&gt; @Logan Zou&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://hf-mirror.com/FlagAlpha/Atom-7B-Chat&quot;&gt;Atom (llama2)&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/01-Atom-7B-chat-WebDemo.md&quot;&gt;Atom-7B-chat WebDemo&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/02-Atom-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Atom-7B-chat Lora 微调&lt;/a&gt; @Logan Zou&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/03-Atom-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Atom-7B-Chat 接入langchain搭建知识库助手&lt;/a&gt; @陈思州&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/04-Atom-7B-chat%20%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83.md&quot;&gt;Atom-7B-chat 全量微调&lt;/a&gt; @Logan Zou&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/THUDM/ChatGLM3.git&quot;&gt;ChatGLM3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/01-ChatGLM3-6B%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;ChatGLM3-6B Transformers 部署调用&lt;/a&gt; @丁悦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/02-ChatGLM3-6B%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;ChatGLM3-6B FastApi 部署调用&lt;/a&gt; @丁悦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/03-ChatGLM3-6B-chat.md&quot;&gt;ChatGLM3-6B chat WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/04-ChatGLM3-6B-Code-Interpreter.md&quot;&gt;ChatGLM3-6B Code Interpreter WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/05-ChatGLM3-6B%E6%8E%A5%E5%85%A5LangChain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;ChatGLM3-6B 接入 LangChain 框架&lt;/a&gt; @Logan Zou&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/06-ChatGLM3-6B-Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;ChatGLM3-6B Lora 微调&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;通用环境配置&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/01-pip%E3%80%81conda%E6%8D%A2%E6%BA%90.md&quot;&gt;pip、conda 换源&lt;/a&gt; @不要葱姜蒜&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/02-AutoDL%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3.md&quot;&gt;AutoDL 开放端口&lt;/a&gt; @不要葱姜蒜&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;模型下载&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;hugging face&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;hugging face&lt;/a&gt; 镜像下载 @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;modelscope&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;git-lfs&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;Openxlab&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Issue &amp;amp;&amp;amp; PR&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;Issue 提交&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;PR 提交&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;fork更新&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;致谢&lt;/h2&gt; 
&lt;h3&gt;核心贡献者&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KMnO4-zx&quot;&gt;宋志学(不要葱姜蒜)-项目负责人&lt;/a&gt; （Datawhale成员-中国矿业大学(北京)）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/logan-zou&quot;&gt;邹雨衡-项目负责人&lt;/a&gt; （Datawhale成员-对外经济贸易大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Tsumugii24&quot;&gt;姜舒凡&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Hongru0306&quot;&gt;肖鸿儒&lt;/a&gt; （Datawhale成员-同济大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/acwwt&quot;&gt;郭志航&lt;/a&gt;（内容创作者）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Zeyi-Lin&quot;&gt;林泽毅&lt;/a&gt;（内容创作者-SwanLab产品负责人）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhangfanTJU&quot;&gt;张帆&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/moyitech&quot;&gt;王泽宇&lt;/a&gt;（内容创作者-太原理工大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Aphasia0515&quot;&gt;李娇娇&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/0-yy-0&quot;&gt;高立业&lt;/a&gt;（内容创作者-DataWhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dingyue772&quot;&gt;丁悦&lt;/a&gt; （Datawhale-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LINHYYY&quot;&gt;林恒宇&lt;/a&gt;（内容创作者-清华大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/L4HeyXiao&quot;&gt;惠佳豪&lt;/a&gt; （Datawhale-宣传大使）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mlw67&quot;&gt;王茂霖&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Caleb-Sun-jz&quot;&gt;孙健壮&lt;/a&gt;（内容创作者-对外经济贸易大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LucaChen&quot;&gt;东东&lt;/a&gt;（内容创作者-谷歌开发者机器学习技术专家）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yeyeyeyeeeee&quot;&gt;荞麦&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Kailigithub&quot;&gt;Kailigithub&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/BaiYu96&quot;&gt;郑皓桦&lt;/a&gt; （内容创作者）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Joe-2002&quot;&gt;李柯辰&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/chg0901&quot;&gt;程宏&lt;/a&gt;（内容创作者-Datawhale意向成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anine09&quot;&gt;骆秀韬&lt;/a&gt;（内容创作者-Datawhale成员-似然实验室）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Twosugar666&quot;&gt;郭宣伯&lt;/a&gt;（内容创作者-北京航空航天大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ilovexsir&quot;&gt;谢好冉&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jjyaoao&quot;&gt;陈思州&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sanbuphy&quot;&gt;散步&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/thomas-yanxin&quot;&gt;颜鑫&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/study520ai520&quot;&gt;杜森&lt;/a&gt;（内容创作者-Datawhale成员-南阳理工学院）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cswangxiaowei&quot;&gt;Swiftie&lt;/a&gt; （小米NLP算法工程师）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KashiwaByte&quot;&gt;黄柏特&lt;/a&gt;（内容创作者-西安电子科技大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AXYZdong&quot;&gt;张友东&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/YangYu-NUAA&quot;&gt;余洋&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Jin-Zhang-Yaoguang&quot;&gt;张晋&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lta155&quot;&gt;娄天奥&lt;/a&gt;（内容创作者-中国科学院大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LinChentang&quot;&gt;左春生&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/little1d&quot;&gt;杨卓&lt;/a&gt;（内容创作者-西安电子科技大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lyj11111111&quot;&gt;小罗&lt;/a&gt; （内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Kedreamix&quot;&gt;邓恺俊&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/XiLinky&quot;&gt;赵文恺&lt;/a&gt;（内容创作者-太原理工大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/comfzy&quot;&gt;付志远&lt;/a&gt;（内容创作者-海南大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/isaacahahah&quot;&gt;郑远婧&lt;/a&gt;（内容创作者-鲸英助教-福州大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Bald0Wang&quot;&gt;王熠明&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LikeGiver&quot;&gt;谭逸珂&lt;/a&gt;（内容创作者-对外经济贸易大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pod2c&quot;&gt;何至轩&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jodie-kang&quot;&gt;康婧淇&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sssanssss&quot;&gt;三水&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langlibai66&quot;&gt;杨晨旭&lt;/a&gt;（内容创作者-太原理工大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/2710932616&quot;&gt;赵伟&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gzhuuser&quot;&gt;苏向标&lt;/a&gt;（内容创作者-广州大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/riannyway&quot;&gt;陈睿&lt;/a&gt;（内容创作者-西交利物浦大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Feimike09&quot;&gt;张龙斐&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anarchysaiko&quot;&gt;孙超&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fanqiNO1&quot;&gt;樊奇&lt;/a&gt;（内容创作者-上海交通大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nusakom&quot;&gt;卓堂越&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fancyboi999&quot;&gt;fancy&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：排名根据贡献程度排序&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;其他&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;特别感谢&lt;a href=&quot;https://github.com/Sm1les&quot;&gt;@Sm1les&lt;/a&gt;对本项目的帮助与支持&lt;/li&gt; 
 &lt;li&gt;部分lora代码和讲解参考仓库：&lt;a href=&quot;https://github.com/zyds/transformers-code.git&quot;&gt;https://github.com/zyds/transformers-code.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;如果有任何想法可以联系我们 DataWhale 也欢迎大家多多提出 issue&lt;/li&gt; 
 &lt;li&gt;特别感谢以下为教程做出贡献的同学！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-top: 30px;&quot;&gt; 
 &lt;a href=&quot;https://github.com/datawhalechina/self-llm/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/self-llm&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Star History&lt;/h3&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-top: 30px;&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/images/star-history-202572.png&quot; /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>anthropics/anthropic-cookbook</title>
      <link>https://github.com/anthropics/anthropic-cookbook</link>
      <description>&lt;p&gt;A collection of notebooks/recipes showcasing some fun and effective ways of using Claude.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Anthropic Cookbook&lt;/h1&gt; 
&lt;p&gt;The Anthropic Cookbook provides code and guides designed to help developers build with Claude, offering copy-able code snippets that you can easily integrate into your own projects.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;To make the most of the examples in this cookbook, you&#39;ll need an Anthropic API key (sign up for free &lt;a href=&quot;https://www.anthropic.com&quot;&gt;here&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;While the code examples are primarily written in Python, the concepts can be adapted to any programming language that supports interaction with the Anthropic API.&lt;/p&gt; 
&lt;p&gt;If you&#39;re new to working with the Anthropic API, we recommend starting with our &lt;a href=&quot;https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals&quot;&gt;Anthropic API Fundamentals course&lt;/a&gt; to get a solid foundation.&lt;/p&gt; 
&lt;h2&gt;Explore Further&lt;/h2&gt; 
&lt;p&gt;Looking for more resources to enhance your experience with Claude and AI assistants? Check out these helpful links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.anthropic.com/claude/docs/guide-to-anthropics-prompt-engineering-resources&quot;&gt;Anthropic developer documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://support.anthropic.com&quot;&gt;Anthropic support docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.anthropic.com/discord&quot;&gt;Anthropic Discord community&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;The Anthropic Cookbook thrives on the contributions of the developer community. We value your input, whether it&#39;s submitting an idea, fixing a typo, adding a new guide, or improving an existing one. By contributing, you help make this resource even more valuable for everyone.&lt;/p&gt; 
&lt;p&gt;To avoid duplication of efforts, please review the existing issues and pull requests before contributing.&lt;/p&gt; 
&lt;p&gt;If you have ideas for new examples or guides, share them on the &lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/issues&quot;&gt;issues page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Table of recipes&lt;/h2&gt; 
&lt;h3&gt;Skills&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/skills/classification&quot;&gt;Classification&lt;/a&gt;: Explore techniques for text and data classification using Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/skills/retrieval_augmented_generation&quot;&gt;Retrieval Augmented Generation&lt;/a&gt;: Learn how to enhance Claude&#39;s responses with external knowledge.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/skills/summarization&quot;&gt;Summarization&lt;/a&gt;: Discover techniques for effective text summarization with Claude.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Tool Use and Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/tool_use&quot;&gt;Tool use&lt;/a&gt;: Learn how to integrate Claude with external tools and functions to extend its capabilities. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/tool_use/customer_service_agent.ipynb&quot;&gt;Customer service agent&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/tool_use/calculator_tool.ipynb&quot;&gt;Calculator integration&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_make_sql_queries.ipynb&quot;&gt;SQL queries&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Third-Party Integrations&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/third_party&quot;&gt;Retrieval augmented generation&lt;/a&gt;: Supplement Claude&#39;s knowledge with external data sources. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Pinecone/rag_using_pinecone.ipynb&quot;&gt;Vector databases (Pinecone)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Wikipedia/wikipedia-search-cookbook.ipynb/&quot;&gt;Wikipedia&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/read_web_pages_with_haiku.ipynb&quot;&gt;Web pages&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/Brave/web_search_using_brave.ipynb&quot;&gt;Internet search (Brave)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/third_party/VoyageAI/how_to_create_embeddings.md&quot;&gt;Embeddings with Voyage AI&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Multimodal Capabilities&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/tree/main/multimodal&quot;&gt;Vision with Claude&lt;/a&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/getting_started_with_vision.ipynb&quot;&gt;Getting started with images&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/best_practices_for_vision.ipynb&quot;&gt;Best practices for vision&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/reading_charts_graphs_powerpoints.ipynb&quot;&gt;Interpreting charts and graphs&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/how_to_transcribe_text.ipynb&quot;&gt;Extracting content from forms&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/illustrated_responses.ipynb&quot;&gt;Generate images with Claude&lt;/a&gt;: Use Claude with Stable Diffusion for image generation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Advanced Techniques&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/multimodal/using_sub_agents.ipynb&quot;&gt;Sub-agents&lt;/a&gt;: Learn how to use Haiku as a sub-agent in combination with Opus.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/pdf_upload_summarization.ipynb&quot;&gt;Upload PDFs to Claude&lt;/a&gt;: Parse and pass PDFs as text to Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_evals.ipynb&quot;&gt;Automated evaluations&lt;/a&gt;: Use Claude to automate the prompt evaluation process.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/how_to_enable_json_mode.ipynb&quot;&gt;Enable JSON mode&lt;/a&gt;: Ensure consistent JSON output from Claude.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/building_moderation_filter.ipynb&quot;&gt;Create a moderation filter&lt;/a&gt;: Use Claude to create a content moderation filter for your application.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anthropics/anthropic-cookbook/raw/main/misc/prompt_caching.ipynb&quot;&gt;Prompt caching&lt;/a&gt;: Learn techniques for efficient prompt caching with Claude.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/aws-samples/anthropic-on-aws&quot;&gt;Anthropic on AWS&lt;/a&gt;: Explore examples and solutions for using Claude on AWS infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/aws-samples/&quot;&gt;AWS Samples&lt;/a&gt;: A collection of code samples from AWS which can be adapted for use with Claude. Note that some samples may require modification to work optimally with Claude.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>greyhatguy007/Machine-Learning-Specialization-Coursera</title>
      <link>https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera</link>
      <description>&lt;p&gt;Contains Solutions and Notes for the Machine Learning Specialization By Stanford University and Deeplearning.ai - Coursera (2022) by Prof. Andrew NG&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Specialization Coursera&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/greyhatguy007/Machine-Learning-Specialization-Coursera/main/resources/title-head.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;Contains Solutions and Notes for the &lt;a href=&quot;https://www.coursera.org/specializations/machine-learning-introduction/?utm_medium=coursera&amp;amp;utm_source=home-page&amp;amp;utm_campaign=mlslaunch2022IN&quot;&gt;Machine Learning Specialization&lt;/a&gt; by Andrew NG on Coursera&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note : If you would like to have a deeper understanding of the concepts by understanding all the math required, have a look at &lt;a href=&quot;https://github.com/greyhatguy007/Mathematics-for-Machine-Learning-and-Data-Science-Specialization-Coursera&quot;&gt;Mathematics for Machine Learning and Data Science&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Course 1 : &lt;a href=&quot;https://www.coursera.org/learn/machine-learning?specialization=machine-learning-introduction&quot;&gt;Supervised Machine Learning: Regression and Classification &lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1&quot;&gt;Week 1&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Regression&quot;&gt;Practice quiz: Regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Supervised%20vs%20unsupervised%20learning&quot;&gt;Practice quiz: Supervised vs unsupervised learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Practice%20quiz%20-%20Train%20the%20model%20with%20gradient%20descent&quot;&gt;Practice quiz: Train the model with gradient descent&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab03_Model_Representation_Soln.ipynb&quot;&gt;Model Representation&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab04_Cost_function_Soln.ipynb&quot;&gt;Cost Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week1/Optional%20Labs/C1_W1_Lab05_Gradient_Descent_Soln.ipynb&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2&quot;&gt;Week 2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Practice%20quiz%20-%20Gradient%20descent%20in%20practice&quot;&gt;Practice quiz: Gradient descent in practice&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Practice%20quiz%20-%20Multiple%20linear%20regression&quot;&gt;Practice quiz: Multiple linear regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb&quot;&gt;Numpy Vectorization&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab02_Multiple_Variable_Soln.ipynb&quot;&gt;Multi Variate Regression&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb&quot;&gt;Feature Scaling&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb&quot;&gt;Feature Engineering&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab05_Sklearn_GD_Soln.ipynb&quot;&gt;Sklearn Gradient Descent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/Optional%20Labs/C1_W2_Lab05_Sklearn_GD_Soln.ipynb&quot;&gt;Sklearn Normal Method&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/C1W2A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week2/C1W2A1/C1_W2_Linear_Regression.ipynb&quot;&gt;Linear Regression&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3&quot;&gt;Week 3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Practice%20quiz%20-%20Cost%20function%20for%20logistic%20regression&quot;&gt;Practice quiz: Cost function for logistic regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Practice%20quiz%20-%20Gradient%20descent%20for%20logistic%20regression&quot;&gt;Practice quiz: Gradient descent for logistic regression&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab01_Classification_Soln.ipynb&quot;&gt;Classification&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab02_Sigmoid_function_Soln.ipynb&quot;&gt;Sigmoid Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab03_Decision_Boundary_Soln.ipynb&quot;&gt;Decision Boundary&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab04_LogisticLoss_Soln.ipynb&quot;&gt;Logistic Loss&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab05_Cost_Function_Soln.ipynb&quot;&gt;Cost Function&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab06_Gradient_Descent_Soln.ipynb&quot;&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab07_Scikit_Learn_Soln.ipynb&quot;&gt;Scikit Learn - Logistic Regression&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab08_Overfitting_Soln.ipynb&quot;&gt;Overfitting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/Optional%20Labs/C1_W3_Lab09_Regularization_Soln.ipynb&quot;&gt;Regularization&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/C1W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C1%20-%20Supervised%20Machine%20Learning%20-%20Regression%20and%20Classification/week3/C1W3A1/C1_W3_Logistic_Regression.ipynb&quot;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/195768f3c1a83e42298d3f61dae99d01&quot;&gt;Certificate Of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;br /&gt; 
&lt;h2&gt;Course 2 : &lt;a href=&quot;https://www.coursera.org/learn/advanced-learning-algorithms?specialization=machine-learning-introduction&quot;&gt;Advanced Learning Algorithms&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1&quot;&gt;Week 1&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20Neural%20networks%20intuition&quot;&gt;Practice quiz: Neural networks intuition&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20Neural%20network%20model&quot;&gt;Practice quiz: Neural network model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice%20quiz%20-%20TensorFlow%20implementation&quot;&gt;Practice quiz: TensorFlow implementation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/Practice-Quiz-Neural-Networks-Implementation-in-python&quot;&gt;Practice quiz : Neural Networks Implementation in Numpy&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab01_Neurons_and_Layers.ipynb&quot;&gt;Neurons and Layers&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab02_CoffeeRoasting_TF.ipynb&quot;&gt;Coffee Roasting&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/optional-labs/C2_W1_Lab02_CoffeeRoasting_TF.ipynb&quot;&gt;Coffee Roasting Using Numpy&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/C2W1A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week1/C2W1A1/C2_W1_Assignment.ipynb&quot;&gt;Neural Networks for Binary Classification&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;br /&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2&quot;&gt;Week 2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Neural-Network-Training&quot;&gt;Practice quiz : Neural Networks Training&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Activation-Functions&quot;&gt;Practice quiz : Activation Functions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-quiz-Multiclass-Classification&quot;&gt;Practice quiz : Multiclass Classification&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/Practice-Quiz-Additional-Neural-Network-Concepts&quot;&gt;Practice quiz : Additional Neural Networks Concepts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs&quot;&gt;Optional Labs&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_Relu.ipynb&quot;&gt;RElu&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_SoftMax.ipynb&quot;&gt;Softmax&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/optional-labs/C2_W2_Multiclass_TF.ipynb&quot;&gt;Multiclass Classification&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/C2W2A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week2/C2W2A1/C2_W2_Assignment.ipynb&quot;&gt;Neural Networks For Handwritten Digit Recognition - Multiclass&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3&quot;&gt;Week 3&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/Practice-Quiz-Advice-for-applying-machine-learning&quot;&gt;Practice quiz : Advice for Applying Machine Learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/practice-quiz-bias-and-variance&quot;&gt;Practice quiz : Bias and Variance&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/practice-quiz-machine-learning-development-process&quot;&gt;Practice quiz : Machine Learning Development Process&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/C2W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week3/C2W3A1/C2_W3_Assignment.ipynb&quot;&gt;Advice for Applied Machine Learning&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4&quot;&gt;Week 4&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-decision-trees&quot;&gt;Practice quiz : Decision Trees&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-decision-tree-learning&quot;&gt;Practice quiz : Decision Trees Learning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/practice-quiz-tree-ensembles&quot;&gt;Practice quiz : Decision Trees Ensembles&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/C2W4A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C2%20-%20Advanced%20Learning%20Algorithms/week4/C2W4A1/C2_W4_Decision_Tree_with_Markdown.ipynb&quot;&gt;Decision Trees&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/c9a7766b0c6eab27db2e955376d29bf7&quot;&gt;Certificate of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;br /&gt; 
&lt;h2&gt;Course 3 : &lt;a href=&quot;https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning?specialization=machine-learning-introduction&quot;&gt;Unsupervised Learning, Recommenders, Reinforcement Learning&lt;/a&gt;&lt;/h2&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1&quot;&gt;Week 1&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/Practice%20Quiz%20-%20Clustering&quot;&gt;Practice quiz : Clustering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/Practice%20Quiz%20-%20Anomaly%20Detection&quot;&gt;Practice quiz : Anomaly Detection&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A&quot;&gt;Programming Assignments&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A/C3W1A1/C3_W1_KMeans_Assignment.ipynb&quot;&gt;K means&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week1/C3W1A/C3W1A2/C3_W1_Anomaly_Detection.ipynb&quot;&gt;Anomaly Detection&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2&quot;&gt;Week 2&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Collaborative%20Filtering&quot;&gt;Practice quiz : Collaborative Filtering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Recommender%20systems%20implementation&quot;&gt;Practice quiz : Recommender systems implementation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/Practice%20Quiz%20-%20Content-based%20filtering&quot;&gt;Practice quiz : Content-based filtering&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2&quot;&gt;Programming Assignments&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2/C3W2A1/C3_W2_Collaborative_RecSys_Assignment.ipynb&quot;&gt;Collaborative Filtering RecSys&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week2/C3W2/C3W2A2/C3_W2_RecSysNN_Assignment.ipynb&quot;&gt;RecSys using Neural Networks&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3&quot;&gt;Week 3&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20Reinforcement%20learning%20introduction&quot;&gt;Practice quiz : Reinforcement learning introduction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20State-action%20value%20function&quot;&gt;Practice Quiz : State-action value function&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/Practice%20Quiz%20-%20Continuous%20state%20spaces&quot;&gt;Practice Quiz : Continuous state spaces&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/tree/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/C3W3A1&quot;&gt;Programming Assignment&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera/raw/main/C3%20-%20Unsupervised%20Learning%2C%20Recommenders%2C%20Reinforcement%20Learning/week3/C3W3A1/C3_W3_A1_Assignment.ipynb&quot;&gt;Deep Q-Learning - Lunar Lander&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;a href=&quot;https://coursera.org/share/5bf5ee456b0c806df9b8622067b47ca6&quot;&gt;Certificate of Completion&lt;/a&gt;&lt;/h4&gt; 
&lt;h3&gt;&lt;a href=&quot;https://coursera.org/share/a15ac6426f90924491a542850700a759&quot;&gt;Specialization Certificate&lt;/a&gt;&lt;/h3&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;hr /&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;h3&gt;Stargazers over time&lt;/h3&gt; 
 &lt;p&gt;&lt;a href=&quot;https://starchart.cc/greyhatguy007/Machine-Learning-Specialization-Coursera&quot;&gt;&lt;img src=&quot;https://starchart.cc/greyhatguy007/Machine-Learning-Specialization-Coursera.svg?variant=adaptive&quot; alt=&quot;Stargazers over time&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://hits.seeyoufarm.com&quot;&gt;&lt;img src=&quot;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fgreyhatguy007%2FMachine-Learning-Specialization-Coursera&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=hits&amp;amp;edge_flat=false&quot; alt=&quot;Hits&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h3&gt;Course Review :&lt;/h3&gt; 
&lt;p&gt;This Course is a best place towards becoming a Machine Learning Engineer. Even if you&#39;re an expert, many algorithms are covered in depth such as decision trees which may help in further improvement of skills.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Special thanks to &lt;a href=&quot;https://www.andrewng.org/&quot;&gt;Professor Andrew Ng&lt;/a&gt; for structuring and tailoring this Course.&lt;/strong&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;hr /&gt; 
&lt;h4&gt;An insight of what you might be able to accomplish at the end of this specialization :&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;i&gt;Write an unsupervised learning algorithm to &lt;strong&gt;Land the Lunar Lander&lt;/strong&gt; Using Deep Q-Learning&lt;/i&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;The Rover was trained to land correctly on the surface, correctly between the flags as indicators after many unsuccessful attempts in learning how to do it.&lt;/li&gt; 
   &lt;li&gt;The final landing after training the agent using appropriate parameters :&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/77543865/182395635-703ae199-ba79-4940-86eb-23dd90093ab3.mp4&quot;&gt;https://user-images.githubusercontent.com/77543865/182395635-703ae199-ba79-4940-86eb-23dd90093ab3.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;i&gt;Write an algorithm for a &lt;strong&gt;Movie Recommender System&lt;/strong&gt;&lt;/i&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;A movie database is collected based on its genre.&lt;/li&gt; 
   &lt;li&gt;A content based filtering and collaborative filtering algorithm is trained and the movie recommender system is implemented.&lt;/li&gt; 
   &lt;li&gt;It gives movie recommendentations based on the movie genre.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/77543865/182398093-c7387754-34a9-4044-b842-0085060c3525.png&quot; alt=&quot;movie_recommendation&quot; /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;i&gt; And Much More !! &lt;/i&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Concluding, this is a course which I would recommend everyone to take. Not just because you learn many new stuffs, but also the assignments are real life examples which are &lt;em&gt;exciting to complete&lt;/em&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;strong&gt;Happy Learning :))&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AI4Finance-Foundation/FinRobot</title>
      <link>https://github.com/AI4Finance-Foundation/FinRobot</link>
      <description>&lt;p&gt;FinRobot: An Open-Source AI Agent Platform for Financial Analysis using LLMs 🚀 🚀 🚀&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; width=&quot;30%&quot; alt=&quot;image&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139&quot; /&gt; 
&lt;/div&gt; 
&lt;h1&gt;FinRobot: An Open-Source AI Agent Platform for Financial Analysis using Large Language Models&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/%5Bhttps://pepy.tech/project/finrobot%5D(https://pepy.tech/project/finrobot)&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/finrobot&quot; alt=&quot;Downloads&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/finrobot&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/finrobot/week&quot; alt=&quot;Downloads&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.python.org/downloads/release/python-360/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.6-blue.svg?sanitize=true&quot; alt=&quot;Python 3.8&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/finrobot/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/finrobot.svg?sanitize=true&quot; alt=&quot;PyPI&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/github/license/AI4Finance-Foundation/finrobot.svg?color=brightgreen&quot; alt=&quot;License&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/issues-raw/AI4Finance-Foundation/finrobot?label=Issues&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/issues-closed-raw/AI4Finance-Foundation/finrobot?label=Closed+Issues&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/issues-pr-raw/AI4Finance-Foundation/finrobot?label=Open+PRs&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/issues-pr-closed-raw/AI4Finance-Foundation/finrobot?label=Closed+PRs&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://raw.githubusercontent.com/AI4Finance-Foundation/FinRobot/master/figs/logo_white_background.jpg&quot; width=&quot;40%&quot; /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;FinRobot&lt;/strong&gt; is an AI Agent Platform that transcends the scope of FinGPT, representing a comprehensive solution meticulously designed for financial applications. It integrates &lt;strong&gt;a diverse array of AI technologies&lt;/strong&gt;, extending beyond mere language models. This expansive vision highlights the platform&#39;s versatility and adaptability, addressing the multifaceted needs of the financial industry.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Concept of AI Agent&lt;/strong&gt;: an AI Agent is an intelligent entity that uses large language models as its brain to perceive its environment, make decisions, and execute actions. Unlike traditional artificial intelligence, AI Agents possess the ability to independently think and utilize tools to progressively achieve given objectives.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14767&quot;&gt;Whitepaper of FinRobot&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/trsr8SXpW5&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/trsr8SXpW5&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://api.visitorbadge.io/api/VisitorHit?user=AI4Finance-Foundation&amp;amp;repo=FinRobot&amp;amp;countColor=%23B17A&quot; alt=&quot;Visitors&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;FinRobot Ecosystem&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/6b30d9c1-35e5-4d36-a138-7e2769718f62&quot; width=&quot;90%&quot; /&gt; 
&lt;/div&gt; 
&lt;h3&gt;The overall framework of FinRobot is organized into four distinct layers, each designed to address specific aspects of financial AI processing and application:&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Financial AI Agents Layer&lt;/strong&gt;: The Financial AI Agents Layer now includes Financial Chain-of-Thought (CoT) prompting, enhancing complex analysis and decision-making capacity. Market Forecasting Agents, Document Analysis Agents, and Trading Strategies Agents utilize CoT to dissect financial challenges into logical steps, aligning their advanced algorithms and domain expertise with the evolving dynamics of financial markets for precise, actionable insights.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial LLMs Algorithms Layer&lt;/strong&gt;: The Financial LLMs Algorithms Layer configures and utilizes specially tuned models tailored to specific domains and global market analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LLMOps and DataOps Layers&lt;/strong&gt;: The LLMOps layer implements a multi-source integration strategy that selects the most suitable LLMs for specific financial tasks, utilizing a range of state-of-the-art models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-source LLM Foundation Models Layer&lt;/strong&gt;: This foundational layer supports the plug-and-play functionality of various general and specialized LLMs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;FinRobot: Agent Workflow&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/ff8033be-2326-424a-ac11-17e2c9c4983d&quot; width=&quot;60%&quot; /&gt; 
&lt;/div&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Perception&lt;/strong&gt;: This module captures and interprets multimodal financial data from market feeds, news, and economic indicators, using sophisticated techniques to structure the data for thorough analysis.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Brain&lt;/strong&gt;: Acting as the core processing unit, this module perceives data from the Perception module with LLMs and utilizes Financial Chain-of-Thought (CoT) processes to generate structured instructions.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;: This module executes instructions from the Brain module, applying tools to translate analytical insights into actionable outcomes. Actions include trading, portfolio adjustments, generating reports, or sending alerts, thereby actively influencing the financial environment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;FinRobot: Smart Scheduler&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/06fa0b78-ac53-48d3-8a6e-98d15386327e&quot; width=&quot;60%&quot; /&gt; 
&lt;/div&gt; 
&lt;p&gt;The Smart Scheduler is central to ensuring model diversity and optimizing the integration and selection of the most appropriate LLM for each task.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Director Agent&lt;/strong&gt;: This component orchestrates the task assignment process, ensuring that tasks are allocated to agents based on their performance metrics and suitability for specific tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Registration&lt;/strong&gt;: Manages the registration and tracks the availability of agents within the system, facilitating an efficient task allocation process.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agent Adaptor&lt;/strong&gt;: Tailor agent functionalities to specific tasks, enhancing their performance and integration within the overall system.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Task Manager&lt;/strong&gt;: Manages and stores different general and fine-tuned LLMs-based agents tailored for various financial tasks, updated periodically to ensure relevance and efficacy.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;File Structure&lt;/h2&gt; 
&lt;p&gt;The main folder &lt;strong&gt;finrobot&lt;/strong&gt; has three subfolders &lt;strong&gt;agents, data_source, functional&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FinRobot
├── finrobot (main folder)
│   ├── agents
│   	├── agent_library.py
│   	└── workflow.py
│   ├── data_source
│   	├── finnhub_utils.py
│   	├── finnlp_utils.py
│   	├── fmp_utils.py
│   	├── sec_utils.py
│   	└── yfinance_utils.py
│   ├── functional
│   	├── analyzer.py
│   	├── charting.py
│   	├── coding.py
│   	├── quantitative.py
│   	├── reportlab.py
│   	└── text.py
│   ├── toolkits.py
│   └── utils.py
│
├── configs
├── experiments
├── tutorials_beginner (hands-on tutorial)
│   ├── agent_fingpt_forecaster.ipynb
│   └── agent_annual_report.ipynb 
├── tutorials_advanced (advanced tutorials for potential finrobot developers)
│   ├── agent_trade_strategist.ipynb
│   ├── agent_fingpt_forecaster.ipynb
│   ├── agent_annual_report.ipynb 
│   ├── lmm_agent_mplfinance.ipynb
│   └── lmm_agent_opt_smacross.ipynb
├── setup.py
├── OAI_CONFIG_LIST_sample
├── config_api_keys_sample
├── requirements.txt
└── README.md
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation:&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;1. (Recommended) Create a new virtual environment&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;conda create --name finrobot python=3.10
conda activate finrobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;2. download the FinRobot repo use terminal or download it manually&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git clone https://github.com/AI4Finance-Foundation/FinRobot.git
cd FinRobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;3. install finrobot &amp;amp; dependencies from source or pypi&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;get our latest release from pypi&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U finrobot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or install from this repo directly&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;4. modify OAI_CONFIG_LIST_sample file&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;1) rename OAI_CONFIG_LIST_sample to OAI_CONFIG_LIST
2) remove the four lines of comment within the OAI_CONFIG_LIST file
3) add your own openai api-key &amp;lt;your OpenAI API key here&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;5. modify config_api_keys_sample file&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;1) rename config_api_keys_sample to config_api_keys
2) remove the comment within the config_api_keys file
3) add your own finnhub-api &quot;YOUR_FINNHUB_API_KEY&quot;
4) add your own financialmodelingprep and sec-api keys &quot;YOUR_FMP_API_KEY&quot; and &quot;YOUR_SEC_API_KEY&quot; (for financial report generation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;6. start navigating the tutorials or the demos below:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# find these notebooks in tutorials
1) agent_annual_report.ipynb
2) agent_fingpt_forecaster.ipynb
3) agent_trade_strategist.ipynb
4) lmm_agent_mplfinance.ipynb
5) lmm_agent_opt_smacross.ipynb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Demos&lt;/h2&gt; 
&lt;h3&gt;1. Market Forecaster Agent (Predict Stock Movements Direction)&lt;/h3&gt; 
&lt;p&gt;Takes a company&#39;s ticker symbol, recent basic financials, and market news as input and predicts its stock movements.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Import&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import autogen
from finrobot.utils import get_current_date, register_keys_from_json
from finrobot.agents.workflow import SingleAssistant
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Config&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Read OpenAI API keys from a JSON file
llm_config = {
    &quot;config_list&quot;: autogen.config_list_from_json(
        &quot;../OAI_CONFIG_LIST&quot;,
        filter_dict={&quot;model&quot;: [&quot;gpt-4-0125-preview&quot;]},
    ),
    &quot;timeout&quot;: 120,
    &quot;temperature&quot;: 0,
}

# Register FINNHUB API keys
register_keys_from_json(&quot;../config_api_keys&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;company = &quot;NVDA&quot;

assitant = SingleAssistant(
    &quot;Market_Analyst&quot;,
    llm_config,
    # set to &quot;ALWAYS&quot; if you want to chat instead of simply receiving the prediciton
    human_input_mode=&quot;NEVER&quot;,
)
assitant.chat(
    f&quot;Use all the tools provided to retrieve information available for {company} upon {get_current_date()}. Analyze the positive developments and potential concerns of {company} &quot;
    &quot;with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. &quot;
    f&quot;Then make a rough prediction (e.g. up/down by 2-3%) of the {company} stock price movement for next week. Provide a summary analysis to support your prediction.&quot;
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Result&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/812ec23a-9cb3-4fad-b716-78533ddcd9dc&quot; width=&quot;40%&quot; /&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/9a2f9f48-b0e1-489c-8679-9a4c530f313c&quot; width=&quot;41%&quot; /&gt; 
&lt;/div&gt; 
&lt;h3&gt;2. Financial Analyst Agent for Report Writing (Equity Research Report)&lt;/h3&gt; 
&lt;p&gt;Take a company&#39;s 10-k form, financial data, and market data as input and output an equity research report&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Import&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
import autogen
from textwrap import dedent
from finrobot.utils import register_keys_from_json
from finrobot.agents.workflow import SingleAssistantShadow
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Config&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;llm_config = {
    &quot;config_list&quot;: autogen.config_list_from_json(
        &quot;../OAI_CONFIG_LIST&quot;,
        filter_dict={
            &quot;model&quot;: [&quot;gpt-4-0125-preview&quot;],
        },
    ),
    &quot;timeout&quot;: 120,
    &quot;temperature&quot;: 0.5,
}
register_keys_from_json(&quot;../config_api_keys&quot;)

# Intermediate strategy modules will be saved in this directory
work_dir = &quot;../report&quot;
os.makedirs(work_dir, exist_ok=True)

assistant = SingleAssistantShadow(
    &quot;Expert_Investor&quot;,
    llm_config,
    max_consecutive_auto_reply=None,
    human_input_mode=&quot;TERMINATE&quot;,
)

&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Run&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;company = &quot;Microsoft&quot;
fyear = &quot;2023&quot;

message = dedent(
    f&quot;&quot;&quot;
    With the tools you&#39;ve been provided, write an annual report based on {company}&#39;s {fyear} 10-k report, format it into a pdf.
    Pay attention to the followings:
    - Explicitly explain your working plan before you kick off.
    - Use tools one by one for clarity, especially when asking for instructions. 
    - All your file operations should be done in &quot;{work_dir}&quot;. 
    - Display any image in the chat once generated.
    - All the paragraphs should combine between 400 and 450 words, don&#39;t generate the pdf until this is explicitly fulfilled.
&quot;&quot;&quot;
)

assistant.chat(message, use_cache=True, max_turns=50,
               summary_method=&quot;last_msg&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Result&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/d2d999e0-dc0e-4196-aca1-218f5fadcc5b&quot; width=&quot;60%&quot; /&gt; 
 &lt;img align=&quot;center&quot; src=&quot;https://github.com/AI4Finance-Foundation/FinRobot/assets/31713746/3a21873f-9498-4d73-896b-3740bf6d116d&quot; width=&quot;60%&quot; /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Financial CoT&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Gather Preliminary Data&lt;/strong&gt;: 10-K report, market data, financial ratios&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Analyze Financial Statements&lt;/strong&gt;: balance sheet, income statement, cash flow&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Company Overview and Performance&lt;/strong&gt;: company description, business highlights, segment analysis&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Risk Assessment&lt;/strong&gt;: assess risks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Financial Performance Visualization&lt;/strong&gt;: plot PE ratio and EPS&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Synthesize Findings into Paragraphs&lt;/strong&gt;: combine all parts into a coherent summary&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generate PDF Report&lt;/strong&gt;: use tools to generate PDF automatically&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Quality Assurance&lt;/strong&gt;: check word counts&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;3. Trade Strategist Agent with multimodal capabilities&lt;/h3&gt; 
&lt;h2&gt;AI Agent Papers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Stanford University + Microsoft Research] &lt;a href=&quot;https://arxiv.org/abs/2401.03568&quot;&gt;Agent AI: Surveying the Horizons of Multimodal Interaction&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Stanford University] &lt;a href=&quot;https://arxiv.org/abs/2304.03442&quot;&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Fudan NLP Group] &lt;a href=&quot;https://arxiv.org/abs/2309.07864&quot;&gt;The Rise and Potential of Large Language Model Based Agents: A Survey&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Fudan NLP Group] &lt;a href=&quot;https://github.com/WooooDyy/LLM-Agent-Paper-List&quot;&gt;LLM-Agent-Paper-List&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Tsinghua University] &lt;a href=&quot;https://arxiv.org/abs/2312.11970&quot;&gt;Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Renmin University] &lt;a href=&quot;https://arxiv.org/pdf/2308.11432.pdf&quot;&gt;A Survey on Large Language Model-based Autonomous Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Nanyang Technological University] &lt;a href=&quot;https://arxiv.org/abs/2402.18485&quot;&gt;FinAgent: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI Agent Blogs and Videos&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Medium] &lt;a href=&quot;https://medium.com/humansdotai/an-introduction-to-ai-agents-e8c4afd2ee8f&quot;&gt;An Introduction to AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[Medium] &lt;a href=&quot;https://medium.com/@aitrendorbit/unmasking-the-best-character-ai-chatbots-2024-351de43792f4#the-best-character-ai-chatbots&quot;&gt;Unmasking the Best Character AI Chatbots | 2024&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[big-picture] &lt;a href=&quot;https://blog.big-picture.com/en/chatgpt-next-level-meet-10-autonomous-ai-agents-auto-gpt-babyagi-agentgpt-microsoft-jarvis-chaosgpt-friends/&quot;&gt;ChatGPT, Next Level: Meet 10 Autonomous AI Agents&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[TowardsDataScience] &lt;a href=&quot;https://towardsdatascience.com/navigating-the-world-of-llm-agents-a-beginners-guide-3b8d499db7a9&quot;&gt;Navigating the World of LLM Agents: A Beginner’s Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;[YouTube] &lt;a href=&quot;https://www.youtube.com/watch?v=iVbN95ica_k&quot;&gt;Introducing Devin - The &quot;First&quot; AI Agent Software Engineer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;AI Agent Open-Source Framework &amp;amp; Tool&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Significant-Gravitas/AutoGPT&quot;&gt;AutoGPT (163k stars)&lt;/a&gt; is a tool for everyone to use, aiming to democratize AI, making it accessible for everyone to use and build upon.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain&quot;&gt;LangChain (87.4k stars)&lt;/a&gt; is a framework for developing context-aware applications powered by language models, enabling them to connect to sources of context and rely on the model&#39;s reasoning capabilities for responses and actions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/geekan/MetaGPT&quot;&gt;MetaGPT (41k stars)&lt;/a&gt; is a multi-agent open-source framework that assigns different roles to GPTs, forming a collaborative software entity to execute complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langgenius/dify&quot;&gt;dify (34.1.7k stars)&lt;/a&gt; is an LLM application development platform. It integrates the concepts of Backend as a Service and LLMOps, covering the core tech stack required for building generative AI-native applications, including a built-in RAG engine&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/autogen&quot;&gt;AutoGen (27.4k stars)&lt;/a&gt; is a framework for developing LLM applications with conversational agents that collaborate to solve tasks. These agents are customizable, support human interaction, and operate in modes combining LLMs, human inputs, and tools.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/ChatDev&quot;&gt;ChatDev (24.1k stars)&lt;/a&gt; is a framework that focuses on developing conversational AI Agents capable of dialogue and question-answering. It provides a range of pre-trained models and interactive interfaces, facilitating the development of customized chat Agents for users.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yoheinakajima/babyagi&quot;&gt;BabyAGI (19.5k stars)&lt;/a&gt; is an AI-powered task management system, dedicated to building AI Agents with preliminary general intelligence.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/joaomdmoura/crewAI&quot;&gt;CrewAI (16k stars)&lt;/a&gt; is a framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/TransformerOptimus/SuperAGI&quot;&gt;SuperAGI (14.8k stars)&lt;/a&gt; is a dev-first open-source autonomous AI agent framework enabling developers to build, manage &amp;amp; run useful autonomous agents.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/labring/FastGPT&quot;&gt;FastGPT (14.6k stars)&lt;/a&gt; is a knowledge-based platform built on the LLM, offers out-of-the-box data processing and model invocation capabilities, allows for workflow orchestration through Flow visualization.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/XAgent&quot;&gt;XAgent (7.8k stars)&lt;/a&gt; is an open-source experimental Large Language Model (LLM) driven autonomous agent that can automatically solve various tasks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dataelement/bisheng&quot;&gt;Bisheng (7.8k stars)&lt;/a&gt; is a leading open-source platform for developing LLM applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBMB/XAgent&quot;&gt;Voyager (5.3k stars)&lt;/a&gt; An Open-Ended Embodied Agent with Large Language Models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/camel-ai/camel&quot;&gt;CAMEL (4.7k stars)&lt;/a&gt; is a framework that offers a comprehensive set of tools and algorithms for building multimodal AI Agents, enabling them to handle various data forms such as text, images, and speech.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langfuse/langfuse&quot;&gt;Langfuse (4.3k stars)&lt;/a&gt; is a language fusion framework that can integrate the language abilities of multiple AI Agents, enabling them to simultaneously possess multilingual understanding and generation capabilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citing FinRobot&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;@inproceedings{
zhou2024finrobot,
title={FinRobot: {AI} Agent for Equity Research and Valuation with Large Language Models},
author={Tianyu Zhou and Pinqiao Wang and Yilin Wu and Hongyang Yang},
booktitle={ICAIF 2024: The 1st Workshop on Large Language Models and Generative AI for Finance},
year={2024}
}

@article{yang2024finrobot,
  title={FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models},
  author={Yang, Hongyang and Zhang, Boyu and Wang, Neng and Guo, Cheng and Zhang, Xiaoli and Lin, Likun and Wang, Junlin and Zhou, Tianyu and Guan, Mao and Zhang, Runjia and others},
  journal={arXiv preprint arXiv:2405.14767},
  year={2024}
}

@inproceedings{han2024enhancing,
  title={Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research},
  author={Han, Xuewen and Wang, Neng and Che, Shangkun and Yang, Hongyang and Zhang, Kunpeng and Xu, Sean Xin},
  booktitle={ICAIF 2024: Proceedings of the 5th ACM International Conference on AI in Finance},
  pages={538--546},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: The codes and documents provided herein are released under the Apache-2.0 license. They should not be construed as financial counsel or recommendations for live trading. It is imperative to exercise caution and consult with qualified financial professionals prior to any trading or investment actions.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>spmallick/learnopencv</title>
      <link>https://github.com/spmallick/learnopencv</link>
      <description>&lt;p&gt;Learn OpenCV : C++ and Python Examples&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LearnOpenCV&lt;/h1&gt; 
&lt;p&gt;This repository contains code for Computer Vision, Deep learning, and AI research articles shared on our blog &lt;a href=&quot;https://www.LearnOpenCV.com&quot;&gt;LearnOpenCV.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Want to become an expert in AI? &lt;a href=&quot;https://opencv.org/courses/&quot;&gt;AI Courses by OpenCV&lt;/a&gt; is a great place to start.&lt;/p&gt; 
&lt;a href=&quot;https://opencv.org/courses/&quot;&gt; &lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://learnopencv.com/wp-content/uploads/2023/01/AI-Courses-By-OpenCV-Github.png&quot; /&gt; &lt;/p&gt; &lt;/a&gt; 
&lt;h2&gt;List of Blog Posts&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Blog Post&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;Code&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/video-rag-for-long-videos/&quot;&gt;Video-RAG: Training-Free Retrieval for Long-Video LVLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Video-RAG_Training_Free_Retrieval_for_Long_Video_LVLMs&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-detection-with-vlms-ft-qwen2-5-vl/&quot;&gt;Object Detection and Spatial Understanding with VLMs ft. Qwen2.5-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/object-detection-with-vlms&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/langgraph-self-correcting-agent-code-generation/&quot;&gt;LangGraph: Building Self-Correcting RAG Agent for Code Generation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/LangGraph_Building_Self_Correcting_RAG_Agent_for_Code_Generation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/sinusoidal-position-embeddings/&quot;&gt;Inside Sinusoidal Position Embeddings: A Sense of Order&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Sinusoidal_Position_Embeddings&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/rope-position-embeddings/&quot;&gt;Inside RoPE: Rotary Magic into Position Embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Inside_RoPE_Position_Embeddings&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/simlingo-vision-language-action-model-for-autonomous-driving/&quot;&gt;SimLingo-Vision-Language-Action-Model-for-Autonomous-Driving&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SimLingo-Vision-Language-Action-Model-for-Autonomous-Driving&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/finetuning-gemma-3n-medical-vqa/&quot;&gt;FineTuning Gemma 3n for Medical VQA on ROCOv2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/finetuning-gemma3n&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/smollm3-explained/&quot;&gt;SmolLM3 Blueprint: SOTA 3B-Parameter LLM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/langgraph-building-a-visual-web-browser-agent/&quot;&gt;LangGraph-A-Visual-Automation-and-Summarization-Pipeline&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/LangGraph-A-Visual-Automation-and-Summarization-Pipeline&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-anomalyclip-medical-anomaly-clip/&quot;&gt;Fine-Tuning AnomalyCLIP: Class-Agnostic Zero-Shot Anomaly Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-AnomalyCLIP&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/siglip-2-deepminds-multilingual-vision-language-model/&quot;&gt;SigLIP 2: DeepMind’s Multilingual Vision-Language Model&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/medgemma-explained/&quot;&gt;MedGemma: Google’s Medico VLM for Clinical QA, Imaging, and More&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/medgemma&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nanonets-ocr-s/&quot;&gt;Nanonets-OCR-s: Enabling Rich, Structured Markdown for Document Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/optimizing-vjepa-2-in-real-time-video-classification/&quot;&gt;Optimizing VJEPA-2: Tackling Latency &amp;amp; Context in Real-Time Video Classification Scripts&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/VJEPA-2-Video-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/?p=73731&amp;amp;preview_id=73731&amp;amp;preview_nonce=beb70ccf8e&amp;amp;preview=true#heading-7&quot;&gt;V-JEPA 2: Meta’s Breakthrough in AI for the Physical World&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/V-JEPA-2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/cosmos-reason-vlm-video-vqa/&quot;&gt;NVIDIA Cosmos Reason1: Video Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Cosmos-Reason1-Video-Understanding&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/gr00t-n1_5-explained/&quot;&gt;GR00T N1.5 Explained&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/llava-training-a-visual-assistant/&quot;&gt;LLaVA&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/LLaVA&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/smolvla-lerobot-vision-language-action-model/&quot;&gt;SmolVLA: Affordable &amp;amp; Efficient VLA Robotics on Consumer GPUs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/smolvla&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-grounding-dino/&quot;&gt;Fine-Tuning Grounding DINO: Open-Vocabulary Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Grounding-DINO-Open-Vocabulary-Object-Detection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/qwen3/&quot;&gt;Getting Started with Qwen3 – The Thinking Expert&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/qwen3&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/modern-gpu-architecture-explained/&quot;&gt;Inside the GPU: A Comprehensive Guide to Modern Graphics Architecture&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/distributed-parallel-training-pytorch-multi-gpu-setup/&quot;&gt;Distributed Parallel Training: PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Distributed-Training-PyTorch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/monai-medical-imaging-pytorch/&quot;&gt;MONAI: The Definitive Framework for Medical Imaging Powered by PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/sana-sprint-the-one-step-revolution-in-high-quality-ai-image-synthesis/&quot;&gt;SANA-Sprint: The One-Step Revolution in High-Quality AI Image Synthesis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/framepack-video-diffusion/&quot;&gt;FramePack-Video-Diffusion-but-feels-like-Image-Diffusion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FramePack-Video-Diffusion-but-feels-like-Image-Diffusion&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/model-weights-file-formats-in-machine-learning/&quot;&gt;Model Weights File Formats in Machine Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/unsloth-guide-efficient-llm-fine-tuning/&quot;&gt;Unsloth: A Guide from Basics to Fine-Tuning Vision Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Unsloth_A_Guide_From_Basics_to_Fine_Tuning_Vision_Models&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/iterative-closest-point-icp-explained/&quot;&gt;Iterative Closest Point (ICP) Algorithm Explained&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/Iterative-Closest-Point-ICP&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/medsam2-explained/&quot;&gt;MedSAM2 Explained: One Prompt to Segment Anything in Medical Imaging&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/medsam2-explained&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/batch-normalization-and-dropout-as-regularizers/&quot;&gt;Batch Normalization and Dropout as Regularizers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/dinov2-self-supervised-vision-transformer/&quot;&gt;DINOv2_by_Meta_A_Self-Supervised_foundational_vision_model&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/DINOv2_by_Meta_A_Self-Supervised_foundational_vision_model&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/embedding-models-explained/&quot;&gt;Beginner&#39;s Guide to Embedding Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mast3r-slam-realtime-dense-slam-explained/&quot;&gt;MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/MASt3R-SLAM&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/googles-a2a-protocol-heres-what-you-need-to-know/&quot;&gt;Google&#39;s A2A Protocol&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-sana-image-generation-model/&quot;&gt;Nvidia SANA : Faster Image Generation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/rf-detr-object-detection/&quot;&gt;Fine-tuning RF-DETR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/Fine-tuning-RF-DETR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/qwen2.5-omni/&quot;&gt;Qwen2.5-Omni: A Real-Time Multimodal AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/vision-language-action-models-lerobot-policy/&quot;&gt;Vision Language Action Models: Robotic Control&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Vision-Language-Action-Models&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-gemma-3/&quot;&gt;Fine-Tuning Gemma 3 VLM using QLoRA for LaTeX-OCR Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Gemma-3-VLM-using-QLoRA-for-LaTeX-OCR-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-comfyui-for-stable-diffusion/&quot;&gt;ComfyUI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ComfyUI&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/gemma-3/&quot;&gt;Gemma-3: A Comprehensive Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo11-on-raspberry-pi/&quot;&gt;YOLO11 on Raspberry Pi: Optimizing Object Detection for Edge Devices&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/yolo11-on-raspberry-pi&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/vggt-visual-geometry-grounded-transformer-3d-reconstruction/&quot;&gt;VGGT: Visual Geometry Grounded Transformer – For Dense 3D Reconstruction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/VGGT-3D-Reconstruction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/understanding-ddim/&quot;&gt;DDIM: The Faster, Improved Version of DDPM for Efficient AI Image Generation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DDIM-The-Faster-Improved-Version-of-DDPM-for-Efficient-AI-Image-Generation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-model-context-protocol/&quot;&gt;Introduction to Model Context Protocol (MCP)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mast3r-sfm-grounding-image-matching-3d/&quot;&gt;MASt3R and MASt3R-SfM Explanation: Image Matching and 3D Reconstruction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/MASt3R-SfM-3D-Reconstruction-Image-Matching&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/matanyone-for-better-video-matting/&quot;&gt;MatAnyone Explained: Consistent Memory for Better Video Matting&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/MatAnyone-Explained-Consistent-Memory-for-Better-Video-Matting&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/graphrag-explained-knowledge-graphs-medical/&quot;&gt;GraphRAG: For Medical Document Analysis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Graphrag-Medical-Document-Analysis&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/omniparser-vision-based-gui-agent/&quot;&gt;OmniParser: Vision Based GUI Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-yolov12/&quot;&gt;Fine-Tuning-YOLOv12-Comparison-With-YOLOv11-And-YOLOv7-Based-Darknet&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv12-Comparison-With-YOLOv11-And-YOLOv7-Based-Darknet&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/finetuning-retinanet&quot;&gt;FineTuning RetinaNet for Wildlife Detection with PyTorch: A Step-by-Step Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/finetuning-retinanet&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/dust3r-geometric-3d-vision/&quot;&gt;DUSt3R: Geometric 3D Vision Made Easy : Explanation and Results&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DUSt3R-Dense-3D-Reconstruction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov12&quot;&gt;YOLOv12: Attention Meets Speed&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv12&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/video-generation-models/&quot;&gt;Video Generation: A Diffusion based approach&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Video-Generation-A-Diffusion-based-approach&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/agentic-ai/&quot;&gt;Agentic AI: A Comprehensive Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Agentic-AI-A-Comprehensive-Introduction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/finetuning-sam2/&quot;&gt;Finetuning SAM2 for Leaf Disease Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/finetuning-sam2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-insertion-in-gaussian-splatting/&quot;&gt;Object Insertion in Gaussian Splatting: Paper Explained and Training Code for MCMC and Bilateral Grid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Insertion-in-Gaussian-Splatting&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/depth-pro-monocular-metric-depth&quot;&gt;Depth Pro: Sharp Monocular Metric Depth&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DepthPro-Monocular-Metric-Depth&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-stable-diffusion-3-5m/&quot;&gt;Fine-tuning-Stable-Diffusion-3_5-UI-images&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-Stable-Diffusion-3_5-UI-images&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/simsiam/&quot;&gt;SimSiam: Streamlining SSL with Stop-Gradient Mechanism&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SimSiam-Streamlining-SSL-with-Stop-Gradient-Mechanism&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/image-captioning/&quot;&gt;Image Captioning using ResNet and LSTM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Image-Captioning-using-ResNet-and-LSTM&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/molmo-vlm&quot;&gt;Molmo VLM: Paper Explanation and Demo&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Molmo-VLM-SAM2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/3d-gaussian-splatting/&quot;&gt;3D Gaussian Splatting Paper Explanation: Training Custom Datasets with NeRF-Studio Gsplats&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/3D-Gaussian-Splatting-Code&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/flux-ai-image-generator/&quot;&gt;FLUX Image Generation: Experimenting with the Parameters&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Flux-Image-Generation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/contrastive-learning-simclr-and-byol-with-code-example/&quot;&gt;Contrastive-Learning-SimCLR-and-BYOL(With Code Example)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Contrastive-Learning-SimCLR-and-BYOL&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/annotated-nerf-pytorch/&quot;&gt;The Annotated NeRF : Training on Custom Dataset from Scratch in Pytorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Annotated-NeRF&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/stable-diffusion-3/&quot;&gt;Stable Diffusion 3 and 3.5: Paper Explanation and Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Stable-Diffusion-3&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/lightrag/&quot;&gt;LightRAG - Legal Document Analysis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/LightRAG-Legal&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-ai-summit-2024-india-overview/&quot;&gt;NVIDIA AI Summit 2024 – India Overview&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/speech-to-speech/&quot;&gt;Introduction to Speech to Speech: Most Efficient Form of NLP&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/speech-to-speech&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/3d-u-net-brats/&quot;&gt;Training 3D U-Net for Brain Tumor Segmentation (BraTS-GLI)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Training_3D_U-Net_Brain_Tumor_Seg&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/detr-overview-and-inference/&quot;&gt;DETR: Overview and Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DETR-Overview_and_Inference&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo11/&quot;&gt;YOLO11: Faster Than You Can Imagine!&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLO11&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tune-dino-self-supervised-learning-segmentation/&quot;&gt;Exploring DINO: Self-Supervised Transformers for Road Segmentation with ResNet50 and U-Net&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Exploring-DINO-dino-road-segmentation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/sapiens-human-vision-models&quot;&gt;Sapiens: Foundation for Human Vision Models by Meta&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Sapiens-Human-Vision-Model-Meta&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/multimodal-rag-with-colpali&quot;&gt;Multimodal RAG with ColPali and Gemini&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Multimodal-RAG-with-ColPali-Gemini&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/pid-controller-ros-2-carla/&quot;&gt;Building Autonomous Vehicle in Carla: Path Following with PID Control &amp;amp; ROS 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Building_Autonomous_Vehicle_in_Carla_Path_Following_with_PID_Control_ROS2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/handwritten-text-recognition-using-ocr/&quot;&gt;Handwritten Text Recognition using OCR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Handwritten_Text_Recognition_using_OCR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/clip-model&quot;&gt;Training CLIP from Sratch for Image Retrieval&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Training-CLIP-from-Scratch-for-Image-Retrieval&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/lidar-slam-with-ros2&quot;&gt;Introduction to LiDAR SLAM: LOAM and LeGO-LOAM Paper and Code Explanation with ROS 2 Implementation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/LeGO-LOAM-ROS2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/recommendation-system-using-vector-search&quot;&gt;Recommendation System using Vector Search&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Recommendation-System-using-Vector-Search&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-whisper-on-custom-dataset/&quot;&gt;Fine Tuning Whisper on Custom Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Whisper-on-Custom-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/sam-2/&quot;&gt;SAM 2 – Promptable Segmentation for Images and Videos&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SAM_2_Segment_Anything_Model_2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/feature-matching/&quot;&gt;Introduction to Feature Matching Using Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Feature-Matching-Using-Neural-Networks&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/robot-operating-system-introduction&quot;&gt;Introduction to ROS2 (Robot Operating System 2): Tutorial on ROS2 Working, DDS, ROS1 RMW, Topics, Nodes, Publisher, Subscriber in Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-ROS2-in-python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/cvpr-2024-research-papers&quot;&gt;CVPR 2024 Research Papers - Part- 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/cvpr-2024-research-papers-part2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/cvpr2024/&quot;&gt;CVPR 2024: An Overview and Key Papers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/CVPR-2024&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-detection-on-edge-device&quot;&gt;Object Detection on Edge Device - OAK-D-Lite&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Detection-on-Edge-Devices&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-yolov10/&quot;&gt;Fine-Tuning YOLOv10 Models on Custom Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv10-Models-Custom-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ros2-and-carla-setup-guide/&quot;&gt;ROS2 and Carla Setup Guide for Ubuntu 22.04&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/monocular-slam-in-python/&quot;&gt;Understanding Visual SLAM for Robotics Perception: Building Monocular SLAM from Scratch in Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Monocular%20SLAM%20for%20Robotics%20implementation%20in%20python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/u2-net-image-segmentation/&quot;&gt;Enhancing Image Segmentation using U2-Net: An Approach to Efficient Background Removal&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Efficient-Background-Removal-using-U2-Net&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov10/&quot;&gt;YOLOv10: The Dual-Head OG of YOLO Series&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv10&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-faster-r-cnn/&quot;&gt;Fine-tuning Faster R-CNN on Sea Rescue Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-Faster-R-CNN-on-SeaRescue-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/recommendation-system/&quot;&gt;Mastering Recommendation System: A Complete Guide&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/automatic-speech-recognition/&quot;&gt;Automatic Speech Recognition with Diarization : Speech-to-Text&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Automatic-Speech-Recognition-with-Diarization-Speech-to-Text&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mobilevit-keras-3/&quot;&gt;Building MobileViT Image Classification Model from Scratch In Keras 3&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Building%20MobileViT%20from%20Scratch%20in%20Keras%203&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/sdxl-inpainting/&quot;&gt;SDXL Inpainting: Fusing Image Inpainting with Stable Diffusion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SDXL-inpainting&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov9-instance-segmentation-on-medical-dataset/&quot;&gt;YOLOv9 Instance Segmentation on Medical Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv9-Instance-Segmentation-on-Medical-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/a-comprehensive-guide-to-robotics/&quot;&gt;A Comprehensive Guide to Robotics&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/integrating-gradio-with-opencv-dnn/&quot;&gt;Integrating Gradio with OpenCV DNN&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Integrating-Gradio-with-OpenCV-DNN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-yolov9/&quot;&gt;Fine-Tuning YOLOv9 on Custom Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv9-Models-Custom-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/dreambooth-using-diffusers/&quot;&gt;Dreambooth using Diffusers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Dreambooth_using_Diffusers&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/hugging-face-diffusers/&quot;&gt;Introduction to Hugging Face Diffusers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction_to_Diffusers&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ultralytics-explorer-api/&quot;&gt;Introduction to Ultralytics Explorer API&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-Ultralytics-Explorer-API&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov9-advancing-the-yolo-legacy/&quot;&gt;YOLOv9: Advancing the YOLO Legacy&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv9-Advancing-the-YOLO-Legacy&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-llms-using-peft/&quot;&gt;Fine-Tuning LLMs using PEFT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-LLMs-using-PEFT&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deciphering-llms/&quot;&gt;Depth Anything: Accelerating Monocular Depth Perception&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Depth-Anything&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deciphering-llms/&quot;&gt;Deciphering LLMs: From Transformers to Quantization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Deciphering-LLMs&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo-loss-function-gfl-vfl-loss/&quot;&gt;YOLO Loss Function Part 2: GFL and VFL Loss&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLO-Loss-Functions-Part2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov8-object-tracking-and-counting-with-opencv/&quot;&gt;YOLOv8-Object-Tracking-and-Counting-with-OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv8-Object-Tracking-and-Counting-with-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/adas-stereo-vision/&quot;&gt;Stereo Vision in ADAS: Pioneering Depth Perception Beyond LiDAR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ADAS-Stereo-Vision&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo-loss-function-siou-focal-loss/&quot;&gt;YOLO Loss Function Part 1: SIoU and Focal Loss&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLO-Loss-Functions-Part1&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/moving-object-detection-with-opencv/&quot;&gt;Moving Object Detection with OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Moving-Object-Detection-with-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/3d-lidar-object-detection/&quot;&gt;Integrating ADAS with Keypoint Feature Pyramid Network for 3D LiDAR Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.dropbox.com/scl/fi/3n1s68jtfkjmw2f5e5ctv/3D-LiDAR-Object-Detection.zip?rlkey=d8q6xvlxis4oxso4qki87omvc&amp;amp;dl=1&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mastering-all-yolo-models&quot;&gt;Mastering All YOLO Models from YOLOv1 to YOLO-NAS: Papers Explained (2024)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/intro-to-gradcam/&quot;&gt;GradCAM: Enhancing Neural Network Interpretability in the Realm of Explainable AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.dropbox.com/scl/fo/3p3sg5fnvhrvi9vp00i0w/h?rlkey=1x01uz5o7esex7p6c8r534iyn&amp;amp;dl=1&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/text-summarization-using-t5/&quot;&gt;Text Summarization using T5: Fine-Tuning and Building Gradio App&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Text-Summarization-using-T5-Fine-Tuning-and-Building-Gradio-App&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/3d-lidar-visualization/&quot;&gt;3D LiDAR Visualization using Open3D: A Case Study on 2D KITTI Depth Frames for Autonomous Driving&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/3D-LiDAR-Perception&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-t5/&quot;&gt;Fine Tuning T5: Text2Text Transfer Transformer for Building a Stack Overflow Tag Generator&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-T5-Text2Text-Transformer-for-Strack-Overflow-Tag-Generation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/segformer-fine-tuning-for-lane-detection&quot;&gt;SegFormer 🤗 : Fine-Tuning for Improved Lane Detection in Autonomous Vehicles&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-SegFormer-For-Lane-Detection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-bert&quot;&gt;Fine-Tuning BERT using Hugging Face Transformers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-BERT-using-Hugging-Face-Transformers&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo-nas-pose&quot;&gt;YOLO-NAS Pose&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLO-NAS-Pose&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/bert-bidirectional-encoder-representations-from-transformers/&quot;&gt;BERT: Bidirectional Encoder Representations from Transformers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/BERT-Bidirectional-Encoder-Representations-from-Transformers&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/comparing-kerascv-yolov8-models/&quot;&gt;Comparing KerasCV YOLOv8 Models on the Global Wheat Data 2020&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Comparing-KerasCV-YOLOv8-Models-on-the-Global-Wheat-Data-2020&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/top-5-ai-papers-of-september-2023/&quot;&gt;Top 5 AI papers of September 2023&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/advanced-driver-assistance-systems/&quot;&gt;Empowering Drivers: The Rise and Role of Advanced Driver Assistance Systems&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/kerascv-deeplabv3-plus-semantic-segmentation/&quot;&gt;Semantic Segmentation using KerasCV DeepLabv3+&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Semantic-Segmentation-using-KerasCV-with-DeepLabv3-Plus&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-detection-using-kerascv-yolov8/&quot;&gt;Object Detection using KerasCV YOLOv8&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Detection-using-KerasCV-YOLOv8&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/animal-pose-estimation/&quot;&gt;Fine-tuning YOLOv8 Pose Models for Animal Pose Estimation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-YOLOv8-Pose-Models-for-Animal-Pose-Estimation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/top-5-ai-papers-of-august-2023/&quot;&gt;Top 5 AI papers of August 2023&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-trocr-training-trocr-to-recognize-curved-text/&quot;&gt;Fine Tuning TrOCR - Training TrOCR to Recognize Curved Text&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-TrOCR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/trocr-getting-started-with-transformer-based-ocr/&quot;&gt;TrOCR - Getting Started with Transformer Based OCR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TrOCR-Getting-Started-with-Transformer-Based-OCR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/facial-emotion-recognition/&quot;&gt;Facial Emotion Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Facial-Emotion-Recognition&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-keypoint-similarity/&quot;&gt;Object Keypoint Similarity in Keypoint Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Keypoint-Similarity-in-Keypoint-Detection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/real-time-deep-sort-with-torchvision-detectors/&quot;&gt;Real Time Deep SORT with Torchvision Detectors&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Real_Time_Deep_SORT_using_Torchvision_Detectors&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/top-5-ai-papers-of-july-2023/&quot;&gt;Top 5 AI papers of July 2023&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/medical-image-segmentation/&quot;&gt;Medical Image Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Medical-Image-Segmentation-Using-HuggingFace-&amp;amp;-PyTorch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/weighted-boxes-fusion/&quot;&gt;Weighted Boxes Fusion in Object Detection: A Comparison with Non-Maximum Suppression&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Weighted-Boxes-Fusion-in-Object-Detection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/medical-multi-label/&quot;&gt;Medical Multi-label Classification with PyTorch &amp;amp; Lightning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Medical_Multi-label_Classification_with_PyTorch_&amp;amp;_Lightning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/paddlepaddle/&quot;&gt;Getting Started with PaddlePaddle: Exploring Object Detection, Segmentation, and Keypoints&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-PaddlePaddle&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/drone-programming-with-computer-vision/&quot;&gt;Drone Programming With Computer Vision A Beginners Guide&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Drone-Programming-With-Computer-Vision-A-Beginners-Guide&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/building-pip-installable-package-pypi/&quot;&gt;How to Build a Pip Installable Package &amp;amp; Upload to PyPi&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/iou-loss-functions-object-detection/&quot;&gt;IoU Loss Functions for Faster &amp;amp; More Accurate Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/slicing-aided-hyper-inference/&quot;&gt;Exploring Slicing Aided Hyper Inference for Small Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Exploring-Slicing-Aided-Hyper-Inference&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/face-recognition-models/&quot;&gt;Advancements in Face Recognition Models, Toolkit and Datasets&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/train-yolo-nas-on-custom-dataset/&quot;&gt;Train YOLO NAS on Custom Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Train-YOLO-NAS-on-Custom-Dataset&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/train-yolov8-instance-segmentation/&quot;&gt;Train YOLOv8 Instance Segmentation on Custom Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Train-YOLOv8-Instance-Segmentation-on-Custom-Data&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolo-nas/&quot;&gt;YOLO-NAS: New Object Detection Model Beats YOLOv6 &amp;amp; YOLOv8&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLO-NAS_Introduction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/segment-anything/&quot;&gt;Segment Anything – A Foundation Model for Image Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Segment-Anything-A-Foundation-Model-for-Image-Segmentation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/video-to-slides-converter-using-background-subtraction/&quot;&gt;Build a Video to Slides Converter Application using the Power of Background Estimation and Frame Differencing in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Build-a-Video-to-Slides-Converter-Application-using-the-Power-of-Background-Estimation-and-Frame-Differencing-in-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/a-closer-look-at-cvat-perfecting-your-annotations/&quot;&gt;A Closer Look at CVAT: Perfecting Your Annotations&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=yxX_0-zr-2U&amp;amp;list=PLfYPZalDvZDLvFhjuflhrxk_lLplXUqqB&quot;&gt;YouTube&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/controlnet/&quot;&gt;ControlNet - Achieving Superior Image Generation Results&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ControlNet-Achieving-Superior-Image-Generation-Results&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/instructpix2pix/&quot;&gt;InstructPix2Pix - Edit Images With Prompts&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/InstructPix2Pix-Edit-Images-With-Prompts&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-spring-gtc-2023-day-4/&quot;&gt;NVIDIA Spring GTC 2023 Day 4: Ending on a High Note with Top Moments from the Finale!&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-spring-gtc-2023-day-3-digging-deeper-into-deep-learning-semiconductors-more/&quot;&gt;NVIDIA Spring GTC 2023 Day 3: Digging deeper into Deep Learning, Semiconductors &amp;amp; more!&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-spring-gtc-2023-day-2-jensens-keynote-the-iphone-moment-of-ai-is-here/&quot;&gt;NVIDIA Spring GTC 2023 Day 2: Jensen’s keynote &amp;amp; the iPhone moment of AI is here!&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-spring-gtc-2023-day-1-highlights-welcome-to-the-future/&quot;&gt;NVIDIA Spring GTC 2023 Day 1: Welcome to the future!&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-gtc-spring-2023-curtain-raiser/&quot;&gt;NVIDIA GTC Spring 2023 Curtain Raiser&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/stable-diffusion-generative-ai/&quot;&gt;Stable Diffusion - A New Paradigm in Generative AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Stable-Diffusion-A-New-Paradigm-in-Generative-AI&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/opencv-face-recognition-api/&quot;&gt;OpenCV Face Recognition – Does Face Recognition Work on AI-Generated Images?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/denoising-diffusion-probabilistic-models/&quot;&gt;An In-Depth Guide to Denoising Diffusion Probabilistic Models – From Theory to Implementation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Guide-to-training-DDPMs-from-Scratch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/rise-of-midjourney-ai-art/&quot;&gt;From Pixels to Paintings: The Rise of Midjourney AI Art&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mastering-dall-e-2/&quot;&gt;Mastering DALL·E 2: A Breakthrough in AI Art Generation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ai-art-generation-tools/&quot;&gt;Top 10 AI Art Generation Tools using Diffusion Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/the-future-of-image-recognition-is-here-pytorch-vision-transformer/&quot;&gt;The Future of Image Recognition is Here: PyTorch Vision Transformer&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Vision_Transformer_PyTorch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/attention-mechanism-in-transformer-neural-networks/&quot;&gt;Understanding Attention Mechanism in Transformer Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Attention_Mechanism_Introduction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deploy-deep-learning-model-huggingface-spaces/&quot;&gt;Deploying a Deep Learning Model using Hugging Face Spaces and Gradio&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Deploying-a-Deep-Learning-Model-using-Hugging-Face-Spaces-and-Gradio&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/train-yolov8-on-custom-dataset/&quot;&gt;Train YOLOv8 on Custom Dataset – A Complete Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Train-YOLOv8-on-Custom-Dataset-A-Complete-Tutorial&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/image-generation-using-diffusion-models/&quot;&gt;Introduction to Diffusion Models for Image Generation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-Diffusion-Models-for-Image-Generation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/building-automated-image-annotation-tool-pyopenannotate/&quot;&gt;Building An Automated Image Annotation Tool: PyOpenAnnotate&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Building-An-Automated-Image-Annotation-Tool-PyOpenAnnotate/&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ultralytics-yolov8/&quot;&gt;Ultralytics YOLOv8: State-of-the-Art YOLO Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Ultralytics-YOLOv8-State-of-the-Art-YOLO-Models&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/getting-started-with-yolov5-instance-segmentation/&quot;&gt;Getting Started with YOLOv5 Instance Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Getting-Started-with-YOLOv5-Instance-Segmentation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deeplabv3-ultimate-guide/&quot;&gt;The Ultimate Guide To DeepLabv3 - With PyTorch Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/The-ultimate-guide-to-deeplabv3&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ai-fitness-trainer-using-mediapipe/&quot;&gt;AI Fitness Trainer using MediaPipe: Squats Analysis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/AI-Fitness-Trainer-Using-MediaPipe-Analyzing-Squats&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolor-paper-explanation-inference-an-in-depth-analysis/&quot;&gt;YoloR - Paper Explanation &amp;amp; Inference -An In-Depth Analysis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YoloR-paper-explanation-analysis&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/automated-image-annotation-tool-using-opencv-python/&quot;&gt;Roadmap To an Automated Image Annotation Tool Using Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Roadmap-To-an-Automated-Image-Annotation-Tool-Using-Python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/performance-comparison-of-yolo-models/&quot;&gt;Performance Comparison of YOLO Object Detection Models – An Intensive Study&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fcos-anchor-free-object-detection-explained/&quot;&gt;FCOS - Anchor Free Object Detection Explained&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FCOS-Inference-using-PyTorch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov6-custom-dataset-training/&quot;&gt;YOLOv6 Custom Dataset Training – Underwater Trash Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv6-Custom-Dataset-Training-Underwater-Trash-Detection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/what-is-exif-data-in-images/&quot;&gt;What is EXIF Data in Images?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/What-is-EXIF-Data-in-Images&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/t-sne-t-distributed-stochastic-neighbor-embedding-explained/&quot;&gt;t-SNE: T-Distributed Stochastic Neighbor Embedding Explained&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/t-SNE-with-Tensorboard&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/centernet-anchor-free-object-detection-explained/&quot;&gt;CenterNet: Objects as Points – Anchor-free Object Detection Explained&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/centernet-with-tf-hub&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov7-pose-vs-mediapipe-in-human-pose-estimation/&quot;&gt;YOLOv7 Pose vs MediaPipe in Human Pose Estimation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv7-Pose-vs-MediaPipe-in-Human-Pose-Estimation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov6-object-detection/&quot;&gt;YOLOv6 Object Detection – Paper Explanation and Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv6-Object-Detection-Paper-Explanation-and-Inference&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolox-object-detector-paper-explanation-and-custom-training/&quot;&gt;YOLOX Object Detector Paper Explanation and Custom Training&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/driver-drowsiness-detection-using-mediapipe-in-python/&quot;&gt;Driver Drowsiness Detection Using Mediapipe In Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Driver-Drowsiness-detection-using-Mediapipe-in-Python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/gtc-2022-big-bang-ai-announcements-everything-you-need-to-know/&quot;&gt;GTC 2022 Big Bang AI announcements: Everything you need to know&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-gtc-2022-the-most-important-ai-event-this-fall/&quot;&gt;NVIDIA GTC 2022 : The most important AI event this Fall&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-tracking-and-reidentification-with-fairmot/&quot;&gt;Object Tracking and Reidentification with FairMOT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Tracking-and-Reidentification-with-FairMOT&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/what-is-face-detection-the-ultimate-guide/&quot;&gt;What is Face Detection? – The Ultimate Guide for 2022&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Face-Detection-Ultimate-Guide&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/custom-document-segmentation-using-deep-learning/&quot;&gt;Document Scanner: Custom Semantic Segmentation using PyTorch-DeepLabV3&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Document-Scanner-Custom-Semantic-Segmentation-using-PyTorch-DeepLabV3&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/fine-tuning-yolov7-on-custom-dataset/&quot;&gt;Fine Tuning YOLOv7 on Custom Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv7&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/Center-Stage-for-zoom-call-using-mediapipe/&quot;&gt;Center Stage for Zoom Calls using MediaPipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/CenterStage&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/&quot;&gt;Mean Average Precision (mAP) in Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/yolov7-object-detection-paper-explanation-and-inference/&quot;&gt;YOLOv7 Object Detection Paper Explanation and Inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv7-Object-Detection-Paper-Explanation-and-Inference&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/pothole-detection-using-yolov4-and-darknet/&quot;&gt;Pothole Detection using YOLOv4 and Darknet&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Pothole-Detection-using-YOLOv4-and-Darknet&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/automatic-document-scanner-using-opencv/&quot;&gt;Automatic Document Scanner using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Automatic-Document-Scanner&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning-part-2/&quot;&gt;Demystifying GPU architectures for deep learning: Part 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/gpu_arch_and_CUDA&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning/&quot;&gt;Demystifying GPU Architectures For Deep Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/gpu_arch_and_CUDA&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/intersection-over-unioniou-in-object-detection-and-segmentation/&quot;&gt;Intersection-over-Union(IoU)-in-Object-Detection-and-Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Intersection-over-Union-IoU-in-Object-Detection-and-Segmentation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/understanding-multiple-object-tracking-using-deepsort/&quot;&gt;Understanding Multiple Object Tracking using DeepSORT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Understanding-Multiple-Object-Tracking-using-DeepSORT&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/optical-character-recognition-using-paddleocr/&quot;&gt;Optical Character Recognition using PaddleOCR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Optical-Character-Recognition-using-PaddleOCR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/gesture-control-in-zoom-call-using-mediapipe/&quot;&gt;Gesture Control in Zoom Call using Mediapipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/zoom-gestures&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deep-dive-into-tensorflow-model-optimization-toolkit/&quot;&gt;A Deep Dive into Tensorflow Model Optimization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/A-Deep-Dive-into-Tensorflow-Model-Optimization&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/depthai-pipeline-overview-creating-a-complex-pipeline/&quot;&gt;DepthAI Pipeline Overview: Creating a Complex Pipeline&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OAK-DepthAi-Pipeline-Overview&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/tensorflow-lite-model-maker-create-models-for-on-device-machine-learning/&quot;&gt;TensorFlow Lite Model Maker: Create Models for On-Device Machine Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Tensorflow-Lite-Model-Maker-Create-Models-for-On-Device-ML&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/tensorflow-lite-model-optimization-for-on-device-machine-learning&quot;&gt;TensorFlow Lite: Model Optimization for On Device Machine Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Lite-Model-Optimization-for-On-Device-MachineLearning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-detection-with-depth-measurement-with-oak-d/&quot;&gt;Object detection with depth measurement using pre-trained models with OAK-D&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OAK-Object-Detection-with-Depth&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/custom-object-detection-training-using-yolov5/&quot;&gt;Custom Object Detection Training using YOLOv5&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Custom-Object-Detection-Training-using-YOLOv5&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/object-detection-using-yolov5-and-opencv-dnn-in-c-and-python/&quot;&gt;Object Detection using Yolov5 and OpenCV DNN (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Object-Detection-using-YOLOv5-and-OpenCV-DNN-in-CPP-and-Python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/create-snapchat-instagram-filters-using-mediapipe/&quot;&gt;Create Snapchat/Instagram filters using Mediapipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Create-AR-filters-using-Mediapipe&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/autosar-c-compliant-deep-learning-inference-with-tensorrt/&quot;&gt;AUTOSAR C++ compliant deep learning inference with TensorRT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/industrial_cv_TensorRT_cpp&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-gtc-2022-day-4-highlights-meet-the-new-jetson-orin/&quot;&gt;NVIDIA GTC 2022 Day 4 Highlights: Meet the new Jetson Orin&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-gtc-2022-day-3-highlights-deep-dive-into-hopper-architecture/&quot;&gt;NVIDIA GTC 2022 Day 3 Highlights: Deep Dive into Hopper architecture&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/nvidia-gtc-2022-day-2-highlights/&quot;&gt;NVIDIA GTC 2022 Day 2 Highlights: Jensen’s Keynote&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/gtc-day-1-highlights/&quot;&gt;NVIDIA GTC 2022 Day 1 Highlights: Brilliant Start&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/automatic-license-plate-recognition-using-deep-learning/&quot;&gt;Automatic License Plate Recognition using Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ALPR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/building-a-body-posture-analysis-system-using-mediapipe/&quot;&gt;Building a Poor Body Posture Detection and Alert System using MediaPipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Posture-analysis-system-using-MediaPipe-Pose&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-mediapipe/&quot;&gt;Introduction to MediaPipe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-MediaPipe&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/disparity-estimation-using-deep-learning/&quot;&gt;Disparity Estimation using Deep Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Disparity-Estimation-Using-Deep-Learning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/how-to-build-chrome-dino-game-bot-using-opencv-feature-matching/&quot;&gt;How to build Chrome Dino game bot using OpenCV Feature Matching&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Chrome-Dino-Bot-using-OpenCV-feature-matching&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/top-10-sources-to-find-computer-vision-and-ai-models/&quot;&gt;Top 10 Sources to Find Computer Vision and AI Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/multi-attribute-and-graph-based-object-detection/&quot;&gt;Multi-Attribute and Graph-based Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/plastic-waste-detection-with-deep-learning/&quot;&gt;Plastic Waste Detection with Deep Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Plastic-Waste-Detection-with-Deep-Learning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ensemble-deep-learning-based-defect-classification-and-detection-in-sem-images/&quot;&gt;Ensemble Deep Learning-based Defect Classification and Detection in SEM Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/building-industrial-embedded-deep-learning-inference-pipelines-with-tensorrt/&quot;&gt;Building Industrial embedded deep learning inference pipelines with TensorRT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/industrial_cv_TensorRT_python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/transfer-learning-for-medical-images/&quot;&gt;Transfer Learning for Medical Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/stereo-vision-and-depth-estimation-using-opencv-ai-kit/&quot;&gt;Stereo Vision and Depth Estimation using OpenCV AI Kit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/oak-getting-started&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-opencv-ai-kit-and-depthai/&quot;&gt;Introduction to OpenCV AI Kit and DepthAI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/oak-getting-started&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/wechat-qr-code-scanner-in-opencv&quot;&gt;WeChat QR Code Scanner in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/WeChat-QRCode-Scanner-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/ai-behind-the-diwali-2021-not-just-a-cadbury-ad/&quot;&gt;AI behind the Diwali 2021 ‘Not just a Cadbury ad’&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/model-selection-and-benchmarking-with-modelplace-ai/&quot;&gt;Model Selection and Benchmarking with Modelplace.AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://modelplace.ai/&quot;&gt;Model Zoo&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/real-time-style-transfer-in-a-zoom-meeting/&quot;&gt;Real-time style transfer in a zoom meeting&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/style-transfer-zoom&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-openvino-deep-learning-workbench/&quot;&gt;Introduction to OpenVino Deep Learning Workbench&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Introduction-to-OpenVino-Deep-Learning-Workbench&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/running-openvino-models-on-intel-integrated-gpu/&quot;&gt;Running OpenVino Models on Intel Integrated GPU&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Running-OpenVino-Models-on-Intel-Integrated-GPU&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/post-training-quantization-with-openvino-toolkit/&quot;&gt;Post Training Quantization with OpenVino Toolkit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Post-Training-Quantization-with-OpenVino-Toolkit&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-intel-openvino-toolkit/&quot;&gt;Introduction to Intel OpenVINO Toolkit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/human-action-recognition-using-detectron2-and-lstm/&quot;&gt;Human Action Recognition using Detectron2 and LSTM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Human-Action-Recognition-Using-Detectron2-And-Lstm&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/paired-image-to-image-translation-pix2pix/&quot;&gt;Pix2Pix:Image-to-Image Translation in PyTorch &amp;amp; TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Image-to-Image-Translation-with-GAN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/&quot;&gt;Conditional GAN (cGAN) in PyTorch and TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Conditional-GAN-PyTorch-TensorFlow&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deep-convolutional-gan-in-pytorch-and-tensorflow/&quot;&gt;Deep Convolutional GAN in PyTorch and TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Deep-Convolutional-GAN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-generative-adversarial-networks/&quot;&gt;Introduction to Generative Adversarial Networks (GANs)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Intro-to-Generative-Adversarial-Network&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/human-pose-estimation-using-keypoint-rcnn-in-pytorch/&quot;&gt;Human Pose Estimation using Keypoint RCNN in PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Keypoint-RCNN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch&quot;&gt;Non Maximum Suppression: Theory and Implementation in PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Non-Maximum-Suppression&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/mrnet-multitask-approach/&quot;&gt;MRNet – The Multi-Task Approach&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/MRnet-MultiTask-Approach&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/generative-and-discriminative-models/&quot;&gt;Generative and Discriminative Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/playing-chromes-t-rex-game-with-facial-gestures/&quot;&gt;Playing Chrome&#39;s T-Rex Game with Facial Gestures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Playing-Chrome-TRex-Game-with-Facial-Gestures&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/variational-autoencoder-in-tensorflow/&quot;&gt;Variational Autoencoder in TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Variational-Autoencoder-TensorFlow&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/autoencoder-in-tensorflow-2-beginners-guide/&quot;&gt;Autoencoder in TensorFlow 2: Beginner’s Guide&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Autoencoder-in-TensorFlow&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/deep-learning-with-opencvs-dnn-module-a-definitive-guide/&quot;&gt;Deep Learning with OpenCV DNN Module: A Definitive Guide&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Deep-Learning-with-OpenCV-DNN-Module&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/depth-perception-using-stereo-camera-python-c/&quot;&gt;Depth perception using stereo camera (Python/C++)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Depth-Perception-Using-Stereo-Camera&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/contour-detection-using-opencv-python-c/&quot;&gt;Contour Detection using OpenCV (Python/C++)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Contour-Detection-using-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/super-resolution-in-opencv/&quot;&gt;Super Resolution in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/Super-Resolution-in-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/improving-illumination-in-night-time-images/&quot;&gt;Improving Illumination in Night Time Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Improving-Illumination-in-Night-Time-Images&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/introduction-to-video-classification-and-human-activity-recognition/&quot;&gt;Video Classification and Human Activity Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/video-classification-and-human-activity-recognition&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/how-to-use-opencv-dnn-module-with-nvidia-gpu-on-windows&quot;&gt;How to use OpenCV DNN Module with Nvidia GPU on Windows&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OpenCV-dnn-gpu-support-Windows&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/opencv-dnn-with-gpu-support/&quot;&gt;How to use OpenCV DNN Module with NVIDIA GPUs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OpenCV-dnn-gpu-support-Linux&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/code-opencv-in-visual-studio/&quot;&gt;Code OpenCV in Visual Studio&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/install-opencv-on-windows/&quot;&gt;Install OpenCV on Windows – C++ / Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Install-OpenCV-Windows-exe&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/face-recognition-with-arcface/&quot;&gt;Face Recognition with ArcFace&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Face-Recognition-with-ArcFace&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/background-subtraction-with-opencv-and-bgs-libraries/&quot;&gt;Background Subtraction with OpenCV and BGS Libraries&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Background-Subtraction&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/optical-flow-using-deep-learning-raft/&quot;&gt;RAFT: Optical Flow estimation using Deep Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-Estimation-using-Deep-Learning-RAFT&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/&quot;&gt;Making A Low-Cost Stereo Camera Using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/stereo-camera&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/optical-flow-in-opencv&quot;&gt;Optical Flow in OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-in-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/&quot;&gt;Introduction to Epipolar Geometry and Stereo Vision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/EpipolarGeometryAndStereoVision&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/classification-with-localization/&quot;&gt;Classification With Localization: Convert any keras Classifier to a Detector&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Classification-with-localization-convert-any-keras-classifier-into-a-detector/README.md&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/photoshop-filters-in-opencv/&quot;&gt;Photoshop Filters in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Photoshop-Filters-in-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/tetris-with-opencv-python&quot;&gt;Tetris Game using OpenCV Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Tetris&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-with-opencv-for-android/&quot;&gt;Image Classification with OpenCV for Android&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-Android&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-with-opencv-java&quot;&gt;Image Classification with OpenCV Java&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-with-Java&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/pytorch-to-tensorflow-model-conversion/&quot;&gt;PyTorch to Tensorflow Model Conversion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-TensorFlow-Model-Conversion&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/snake-game-with-opencv-python/&quot;&gt;Snake Game with OpenCV Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SnakeGame&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/stanford-mrnet-challenge-classifying-knee-mris/&quot;&gt;Stanford MRNet Challenge: Classifying Knee MRIs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/MRNet-Single-Model&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/experiment-logging-with-tensorboard-and-wandb&quot;&gt;Experiment Logging with TensorBoard and wandb&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Vision-Experiment-Logging&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/understanding-lens-distortion/&quot;&gt;Understanding Lens Distortion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/UnderstandingLensDistortion&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-matting-with-state-of-the-art-method-f-b-alpha-matting/&quot;&gt;Image Matting with state-of-the-art Method “F, B, Alpha Matting”&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FBAMatting&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/bag-of-tricks-for-image-classification-lets-check-if-it-is-working-or-not/&quot;&gt;Bag Of Tricks For Image Classification - Let&#39;s check if it is working or not&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Bag-Of-Tricks-For-Image-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/getting-started-opencv-cuda-module/&quot;&gt;Getting Started with OpenCV CUDA Module&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Getting-Started-OpenCV-CUDA-Module&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/training-a-custom-object-detector-with-dlib-making-gesture-controlled-applications/&quot;&gt;Training a Custom Object Detector with DLIB &amp;amp; Making Gesture Controlled Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Training_a_custom_hand_detector_with_dlib&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/how-to-run-inference-using-tensorrt-c-api/&quot;&gt;How To Run Inference Using TensorRT C++ API&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT-CPP&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/using-facial-landmarks-for-overlaying-faces-with-masks/&quot;&gt;Using Facial Landmarks for Overlaying Faces with Medical Masks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FaceMaskOverlay&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/tensorboard-with-pytorch-lightning&quot;&gt;Tensorboard with PyTorch Lightning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TensorBoard-With-Pytorch-Lightning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/otsu-thresholding-with-opencv/&quot;&gt;Otsu&#39;s Thresholding with OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/otsu-method&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/pytorch-to-coreml-model-conversion/&quot;&gt;PyTorch-to-CoreML-model-conversion&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-CoreML-model-conversion&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/playing-rock-paper-scissors-with-ai/&quot;&gt;Playing Rock, Paper, Scissors with AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Playing-rock-paper-scissors-with-AI&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/&quot;&gt;CNN Receptive Field Computation Using Backprop with TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/cnn-fully-convolutional-image-classification-with-tensorflow&quot;&gt;CNN Fully Convolutional Image Classification with TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Fully-Convolutional-Image-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/&quot;&gt;How to convert a model from PyTorch to TensorRT and speed up inference&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/efficient-image-loading/&quot;&gt;Efficient image loading&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Efficient-image-loading&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/graph-convolutional-networks-model-relations-in-data/&quot;&gt;Graph Convolutional Networks: Model Relations In Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Graph-Convolutional-Networks-Model-Relations-In-Data&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/&quot;&gt;Getting Started with Federated Learning with PyTorch and PySyft&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/creating-a-virtual-pen-and-eraser-with-opencv/&quot;&gt;Creating a Virtual Pen &amp;amp; Eraser&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Creating-a-Virtual-Pen-and-Eraser&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/getting-started-with-pytorch-lightning/&quot;&gt;Getting Started with PyTorch Lightning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Pytorch-Lightning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/multi-label-image-classification-with-pytorch-image-tagging/&quot;&gt;Multi-Label Image Classification with PyTorch: Image Tagging&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification-Image-Tagging&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/Funny-Mirrors-Using-OpenCV/&quot;&gt;Funny Mirrors Using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FunnyMirrors&quot;&gt;code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/t-sne-for-feature-visualization/&quot;&gt;t-SNE for ResNet feature visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TSNE&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/multi-label-image-classification-with-pytorch/&quot;&gt;Multi-Label Image Classification with Pytorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop/&quot;&gt;CNN Receptive Field Computation Using Backprop&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Receptive-Field-With-Backprop&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/&quot;&gt;CNN Receptive Field Computation Using Backprop with TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/augmented-reality-using-aruco-markers-in-opencv-(c++-python)/&quot;&gt;Augmented Reality using AruCo Markers in OpenCV(C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/AugmentedRealityWithArucoMarkers&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/fully-convolutional-image-classification-on-arbitrary-sized-image/&quot;&gt;Fully Convolutional Image Classification on Arbitrary Sized Image&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Fully-Convolutional-Image-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/camera-calibration-using-opencv/&quot;&gt;Camera Calibration using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/CameraCalibration&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/geometry-of-image-formation/&quot;&gt;Geometry of Image Formation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/ensuring-training-reproducibility-in-pytorch&quot;&gt;Ensuring Training Reproducibility in Pytorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/gaze-tracking/&quot;&gt;Gaze Tracking&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/simple-background-estimation-in-videos-using-opencv-c-python/&quot;&gt;Simple Background Estimation in Videos Using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/VideoBackgroundEstimation&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/applications-of-foreground-background-separation-with-semantic-segmentation/&quot;&gt;Applications of Foreground-Background separation with Semantic Segmentation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/app-seperation-semseg&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/efficientnet-theory-code&quot;&gt;EfficientNet: Theory + Code&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/EfficientNet&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/mask-r-cnn-instance-segmentation-with-pytorch/&quot;&gt;PyTorch for Beginners: Mask R-CNN Instance Segmentation with PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/spmallick/learnopencv/master/PyTorch-Mask-RCNN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch&quot;&gt;PyTorch for Beginners: Faster R-CNN Object Detection with PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-faster-RCNN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/&quot;&gt;PyTorch for Beginners: Semantic Segmentation using torchvision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-Segmentation-torchvision&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-using-pre-trained-models-using-pytorch/&quot;&gt;PyTorch for Beginners: Comparison of pre-trained models for Image Classification&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Image-classification-pre-trained-models/Image_Classification_using_pre_trained_models.ipynb&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/pytorch-for-beginners-basics/&quot;&gt;PyTorch for Beginners: Basics&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/PyTorch-for-Beginners/PyTorch_for_Beginners.ipynb&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/pytorch-model-inference-using-onnx-and-caffe2/&quot;&gt;PyTorch Model Inference using ONNX and Caffe2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Inference-for-PyTorch-Models/ONNX-Caffe2&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/&quot;&gt;Image Classification Using Transfer Learning in PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Image-Classification-in-PyTorch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/hangman-creating-games-in-opencv/&quot;&gt;Hangman: Creating games in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Hangman&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-inpainting-with-opencv-c-python/&quot;&gt;Image Inpainting with OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Image-Inpainting&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/hough-transform-with-opencv-c-python/&quot;&gt;Hough Transform with OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Hough-Transform&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/&quot;&gt;Xeus-Cling: Run C++ code in Jupyter Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/XeusCling&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/&quot;&gt;Gender &amp;amp; Age Classification using OpenCV Deep Learning ( C++/Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/AgeGender&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/invisibility-cloak-using-color-detection-and-segmentation-with-opencv/&quot;&gt;Invisibility Cloak using Color Detection and Segmentation with OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/InvisibilityCloak&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/fast-image-downloader-for-open-images-v4/&quot;&gt;Fast Image Downloader for Open Images V4 (Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/downloadOpenImages&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-based-text-detection-using-opencv-c-python/&quot;&gt;Deep Learning based Text Detection Using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/TextDetectionEAST&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/&quot;&gt;Video Stabilization Using Point Feature Matching in OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/VideoStabilization&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/training-yolov3-deep-learning-based-custom-object-detector/&quot;&gt;Training YOLOv3 : Deep Learning based Custom Object Detector&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/YOLOv3-Training-Snowman-Detector&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/using-openvino-with-opencv/&quot;&gt;Using OpenVINO with OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OpenVINO-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/duplicate-search-on-quora-dataset/&quot;&gt;Duplicate Search on Quora Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Quora-Dataset-Duplicate-Search&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/shape-matching-using-hu-moments-c-python/&quot;&gt;Shape Matching using Hu Moments (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/HuMoments&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-centos-7/&quot;&gt;Install OpenCV 4 on CentOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-centos.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-centos-7/&quot;&gt;Install OpenCV 3.4.4 on CentOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-centos.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-red-hat/&quot;&gt;Install OpenCV 3.4.4 on Red Hat (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-red-hat.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-red-hat/&quot;&gt;Install OpenCV 4 on Red Hat (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-on-red-hat.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-macos/&quot;&gt;Install OpenCV 4 on macOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/InstallScripts/installOpenCV-4-macos.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-raspberry-pi/&quot;&gt;Install OpenCV 3.4.4 on Raspberry Pi&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-raspberry-pi.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-macos/&quot;&gt;Install OpenCV 3.4.4 on macOS (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-macos.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/opencv-qr-code-scanner-c-and-python/&quot;&gt;OpenCV QR Code Scanner (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/QRCode-OpenCV&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-windows/&quot;&gt;Install OpenCV 3.4.4 on Windows (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-3&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-16-04/&quot;&gt;Install OpenCV 3.4.4 on Ubuntu 16.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-Ubuntu-16-04.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-18-04/&quot;&gt;Install OpenCV 3.4.4 on Ubuntu 18.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-3-on-Ubuntu-18-04.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/universal-sentence-encoder&quot;&gt;Universal Sentence Encoder&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/Universal-Sentence-Encoder&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-raspberry-pi/&quot;&gt;Install OpenCV 4 on Raspberry Pi&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-raspberry-pi.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-windows/&quot;&gt;Install OpenCV 4 on Windows (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-4&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/&quot;&gt;Face Detection – Dlib, OpenCV, and Deep Learning ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FaceDetectionComparison&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/hand-keypoint-detection-using-deep-learning-and-opencv/&quot;&gt;Hand Keypoint Detection using Deep Learning and OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/HandPose&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/&quot;&gt;Deep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Mask-RCNN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/&quot;&gt;Install OpenCV 4 on Ubuntu 18.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-on-Ubuntu-18-04.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-4-on-ubuntu-16-04/&quot;&gt;Install OpenCV 4 on Ubuntu 16.04 (C++ and Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/raw/master/InstallScripts/installOpenCV-4-on-Ubuntu-16-04.sh&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/&quot;&gt;Multi-Person Pose Estimation in OpenCV using OpenPose&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OpenPose-Multi-Person&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/heatmap-for-logo-detection-using-opencv-python/&quot;&gt;Heatmap for Logo Detection using OpenCV (Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/heatmap&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/&quot;&gt;Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ObjectDetection-YOLO&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/&quot;&gt;Convex Hull using OpenCV in Python and C++&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ConvexHull&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/multitracker-multiple-object-tracking-using-opencv-c-python/&quot;&gt;MultiTracker : Multiple Object Tracking using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/MultiObjectTracker&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/convolutional-neural-network-based-image-colorization-using-opencv/&quot;&gt;Convolutional Neural Network based Image Colorization using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Colorization&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/svm-using-scikit-learn-in-python/&quot;&gt;SVM using scikit-learn&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/&quot;&gt;GOTURN: Deep Learning based Object Tracking&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/GOTURN&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/&quot;&gt;Find the Center of a Blob (Centroid) using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/CenterofBlob&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/support-vector-machines-svm/&quot;&gt;Support Vector Machines (SVM)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/batch-normalization-in-deep-networks/&quot;&gt;Batch Normalization in Deep Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/BatchNormalization&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-character-classification-using-synthetic-dataset/&quot;&gt;Deep Learning based Character Classification using Synthetic Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/CharClassification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-quality-assessment-brisque/&quot;&gt;Image Quality Assessment : BRISQUE&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ImageMetrics&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/understanding-alexnet/&quot;&gt;Understanding AlexNet&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-based-text-recognition-ocr-using-tesseract-and-opencv/&quot;&gt;Deep Learning based Text Recognition (OCR) using Tesseract and OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OCR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/&quot;&gt;Deep Learning based Human Pose Estimation using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/OpenPose&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/&quot;&gt;Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/how-to-convert-your-opencv-c-code-into-a-python-module/&quot;&gt;How to convert your OpenCV C++ code into a Python module&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/pymodule&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/cv4faces-best-project-award-2018/&quot;&gt;CV4Faces : Best Project Award 2018&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/facemark-facial-landmark-detection-using-opencv/&quot;&gt;Facemark : Facial Landmark Detection using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FacialLandmarkDetection&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/&quot;&gt;Image Alignment (Feature Based) using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ImageAlignment-FeatureBased&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/barcode-and-qr-code-scanner-using-zbar-and-opencv/&quot;&gt;Barcode and QR code Scanner using ZBar and OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/barcode-QRcodeScanner&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/&quot;&gt;Keras Tutorial : Fine-tuning using pre-trained models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Keras-Fine-Tuning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/opencv-transparent-api/&quot;&gt;OpenCV Transparent API&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/face-reconstruction-using-eigenfaces-cpp-python/&quot;&gt;Face Reconstruction using EigenFaces (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ReconstructFaceUsingEigenFaces&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/eigenface-using-opencv-c-python/&quot;&gt;Eigenface using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/EigenFace&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/principal-component-analysis/&quot;&gt;Principal Component Analysis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/&quot;&gt;Keras Tutorial : Transfer Learning using pre-trained models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Keras-Transfer-Learning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/&quot;&gt;Keras Tutorial : Using pre-trained Imagenet models&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Keras-ImageNet-Models&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/technical-aspects-of-a-digital-slr/&quot;&gt;Technical Aspects of a Digital SLR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/using-harry-potter-interactive-wand-with-opencv-to-create-magic/&quot;&gt;Using Harry Potter interactive wand with OpenCV to create magic&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/install-opencv-3-and-dlib-on-windows-python-only/&quot;&gt;Install OpenCV 3 and Dlib on Windows ( Python only )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras&quot;&gt;Image Classification using Convolutional Neural Networks in Keras&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/KerasCNN-CIFAR&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/&quot;&gt;Understanding Autoencoders using Tensorflow (Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/DenoisingAutoencoder&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/best-project-award-computer-vision-for-faces/&quot;&gt;Best Project Award : Computer Vision for Faces&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/&quot;&gt;Understanding Activation Functions in Deep Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/&quot;&gt;Image Classification using Feedforward Neural Network in Keras&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/kromydas/learnopencv/tree/master/Keras-MLP-MNIST-Classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/exposure-fusion-using-opencv-cpp-python/&quot;&gt;Exposure Fusion using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ExposureFusion&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.learnopencv.com/understanding-feedforward-neural-networks/&quot;&gt;Understanding Feedforward Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/high-dynamic-range-hdr-imaging-using-opencv-cpp-python&quot;&gt;High Dynamic Range (HDR) Imaging using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/hdr&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/deep-learning-using-keras-the-basics&quot;&gt;Deep learning using Keras – The Basics&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/kromydas/learnopencv/tree/master/Keras-Linear-Regression&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/selective-search-for-object-detection-cpp-python/&quot;&gt;Selective Search for Object Detection (C++ / Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SelectiveSearch&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/&quot;&gt;Installing Deep Learning Frameworks on Ubuntu with CUDA support&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/parallel-pixel-access-in-opencv-using-foreach/&quot;&gt;Parallel Pixel Access in OpenCV using forEach&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/forEach&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/cvui-gui-lib-built-on-top-of-opencv-drawing-primitives/&quot;&gt;cvui: A GUI lib built on top of OpenCV drawing primitives&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/UI-cvui&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-dlib-on-windows/&quot;&gt;Install Dlib on Windows&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-dlib-on-ubuntu/&quot;&gt;Install Dlib on Ubuntu&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-opencv3-on-ubuntu/&quot;&gt;Install OpenCV3 on Ubuntu&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/&quot;&gt;Read, Write and Display a video using OpenCV ( C++/ Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/VideoReadWriteDisplay&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-dlib-on-macos/&quot;&gt;Install Dlib on MacOS&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-opencv3-on-macos/&quot;&gt;Install OpenCV 3 on MacOS&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-opencv3-on-windows/&quot;&gt;Install OpenCV 3 on Windows&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/get-opencv-build-information-getbuildinformation/&quot;&gt;Get OpenCV Build Information ( getBuildInformation )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/color-spaces-in-opencv-cpp-python/&quot;&gt;Color spaces in OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ColorSpaces&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/&quot;&gt;Neural Networks : A 30,000 Feet View for Beginners&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/alpha-blending-using-opencv-cpp-python/&quot;&gt;Alpha Blending using OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/AlphaBlending&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/user-stories-how-readers-of-this-blog-are-applying-their-knowledge-to-build-applications/&quot;&gt;User stories : How readers of this blog are applying their knowledge to build applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/how-to-select-a-bounding-box-roi-in-opencv-cpp-python/&quot;&gt;How to select a bounding box ( ROI ) in OpenCV (C++/Python) ?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/automatic-red-eye-remover-using-opencv-cpp-python/&quot;&gt;Automatic Red Eye Remover using OpenCV (C++ / Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/RedEyeRemover&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/&quot;&gt;Bias-Variance Tradeoff in Machine Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/embedded-computer-vision-which-device-should-you-choose/&quot;&gt;Embedded Computer Vision: Which device should you choose?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/object-tracking-using-opencv-cpp-python/&quot;&gt;Object Tracking using OpenCV (C++/Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/tracking&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/&quot;&gt;Handwritten Digits Classification : An OpenCV ( C++ / Python ) Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/digits-classification&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv/&quot;&gt;Training a better Haar and LBP cascade based Eye Detector using OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/deep-learning-book-gift-recipients/&quot;&gt;Deep Learning Book Gift Recipients&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/minified-opencv-haar-and-lbp-cascades/&quot;&gt;Minified OpenCV Haar and LBP Cascades&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ninjaEyeDetector&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/deep-learning-book-gift/&quot;&gt;Deep Learning Book Gift&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/histogram-of-oriented-gradients/&quot;&gt;Histogram of Oriented Gradients&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/image-recognition-and-object-detection-part1/&quot;&gt;Image Recognition and Object Detection : Part 1&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/&quot;&gt;Head Pose Estimation using OpenCV and Dlib&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/HeadPose&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/live-cv/&quot;&gt;Live CV : A Computer Vision Coding Application&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/&quot;&gt;Approximate Focal Length for Webcams and Cell Phone Cameras&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/configuring-qt-for-opencv-on-osx/&quot;&gt;Configuring Qt for OpenCV on OSX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/qt-test&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/rotation-matrix-to-euler-angles/&quot;&gt;Rotation Matrix To Euler Angles&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/RotationMatrixToEulerAngles&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/speeding-up-dlib-facial-landmark-detector/&quot;&gt;Speeding up Dlib’s Facial Landmark Detector&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/warp-one-triangle-to-another-using-opencv-c-python/&quot;&gt;Warp one triangle to another using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/WarpTriangle&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/average-face-opencv-c-python-tutorial/&quot;&gt;Average Face : OpenCV ( C++ / Python ) Tutorial&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FaceAverage&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/face-swap-using-opencv-c-python/&quot;&gt;Face Swap using OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FaceSwap&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/face-morph-using-opencv-cpp-python/&quot;&gt;Face Morph Using OpenCV — C++ / Python&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FaceMorph&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/deep-learning-example-using-nvidia-digits-3-on-ec2/&quot;&gt;Deep Learning Example using NVIDIA DIGITS 3 on EC2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/nvidia-digits-3-on-ec2/&quot;&gt;NVIDIA DIGITS 3 on EC2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/homography-examples-using-opencv-python-c/&quot;&gt;Homography Examples using OpenCV ( Python / C ++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Homography&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/filling-holes-in-an-image-using-opencv-python-c/&quot;&gt;Filling holes in an image using OpenCV ( Python / C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Holes&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/how-to-find-frame-rate-or-frames-per-second-fps-in-opencv-python-cpp/&quot;&gt;How to find frame rate or frames per second (fps) in OpenCV ( Python / C++ ) ?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FPS&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/delaunay-triangulation-and-voronoi-diagram-using-opencv-c-python/&quot;&gt;Delaunay Triangulation and Voronoi Diagram using OpenCV ( C++ / Python)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Delaunay&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/opencv-c-vs-python-vs-matlab-for-computer-vision/&quot;&gt;OpenCV (C++ vs Python) vs MATLAB for Computer Vision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/facial-landmark-detection/&quot;&gt;Facial Landmark Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/why-does-opencv-use-bgr-color-format/&quot;&gt;Why does OpenCV use BGR color format ?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/computer-vision-for-predicting-facial-attractiveness/&quot;&gt;Computer Vision for Predicting Facial Attractiveness&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/FacialAttractiveness&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/&quot;&gt;applyColorMap for pseudocoloring in OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Colormap&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/image-alignment-ecc-in-opencv-c-python/&quot;&gt;Image Alignment (ECC) in OpenCV ( C++ / Python )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/ImageAlignment&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/how-to-find-opencv-version-python-cpp/&quot;&gt;How to find OpenCV version in Python and C++ ?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/baidu-banned-from-ilsvrc-2015/&quot;&gt;Baidu banned from ILSVRC 2015&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/opencv-transparent-api/&quot;&gt;OpenCV Transparent API&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/how-computer-vision-solved-the-greatest-soccer-mystery-of-all-times/&quot;&gt;How Computer Vision Solved the Greatest Soccer Mystery of All Time&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/embedded-vision-summit-2015/&quot;&gt;Embedded Vision Summit 2015&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/read-an-image-in-opencv-python-cpp/&quot;&gt;Read an Image in OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/imread&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/&quot;&gt;Non-Photorealistic Rendering using OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/NonPhotorealisticRendering&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp/&quot;&gt;Seamless Cloning using OpenCV ( Python , C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/SeamlessCloning&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/opencv-threshold-python-cpp/&quot;&gt;OpenCV Threshold ( Python , C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/Threshold&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/blob-detection-using-opencv-python-c/&quot;&gt;Blob Detection Using OpenCV ( Python, C++ )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/spmallick/learnopencv/tree/master/BlobDetector&quot;&gt;Code&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/turn-your-opencv-Code-into-a-web-api-in-under-10-minutes-part-1/&quot;&gt;Turn your OpenCV Code into a Web API in under 10 minutes — Part 1&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/how-to-compile-opencv-sample-Code/&quot;&gt;How to compile OpenCV sample Code ?&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.learnopencv.com/install-opencv-3-on-yosemite-osx-10-10-x/&quot;&gt;Install OpenCV 3 on Yosemite ( OSX 10.10.x )&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
  </channel>
</rss>
