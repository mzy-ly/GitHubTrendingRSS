<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub All Languages Weekly Trending</title>
    <description>Weekly Trending of All Languages in GitHub</description>
    <pubDate>Wed, 13 Aug 2025 01:40:48 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>umami-software/umami</title>
      <link>https://github.com/umami-software/umami</link>
      <description>&lt;p&gt;Umami is a modern, privacy-focused alternative to Google Analytics.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://content.umami.is/website/images/umami-logo.png&quot; alt=&quot;Umami Logo&quot; width=&quot;100&quot; /&gt; &lt;/p&gt; 
&lt;h1 align=&quot;center&quot;&gt;Umami&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;i&gt;Umami is a simple, fast, privacy-focused alternative to Google Analytics.&lt;/i&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/umami-software/umami/releases&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/release/umami-software/umami.svg?sanitize=true&quot; alt=&quot;GitHub Release&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/umami-software/umami/raw/master/LICENSE&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/license/umami-software/umami.svg?sanitize=true&quot; alt=&quot;MIT License&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/umami-software/umami/actions&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/umami-software/umami/ci.yml&quot; alt=&quot;Build Status&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://analytics.umami.is/share/LGazGOecbDtaIwDr/umami.is&quot; style=&quot;text-decoration: none;&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Try%20Demo%20Now-Click%20Here-brightgreen&quot; alt=&quot;Umami Demo&quot; /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; 
&lt;p&gt;A detailed getting started guide can be found at &lt;a href=&quot;https://umami.is/docs/&quot;&gt;umami.is/docs&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üõ† Installing from Source&lt;/h2&gt; 
&lt;h3&gt;Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;A server with Node.js version 18.18 or newer&lt;/li&gt; 
 &lt;li&gt;A database. Umami supports &lt;a href=&quot;https://www.mariadb.org/&quot;&gt;MariaDB&lt;/a&gt; (minimum v10.5), &lt;a href=&quot;https://www.mysql.com/&quot;&gt;MySQL&lt;/a&gt; (minimum v8.0) and &lt;a href=&quot;https://www.postgresql.org/&quot;&gt;PostgreSQL&lt;/a&gt; (minimum v12.14) databases.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Get the Source Code and Install Packages&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/umami-software/umami.git
cd umami
npm install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Configure Umami&lt;/h3&gt; 
&lt;p&gt;Create an &lt;code&gt;.env&lt;/code&gt; file with the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;DATABASE_URL=connection-url
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The connection URL format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;postgresql://username:mypassword@localhost:5432/mydb
mysql://username:mypassword@localhost:3306/mydb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Build the Application&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;npm run build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;The build step will create tables in your database if you are installing for the first time. It will also create a login user with username &lt;strong&gt;admin&lt;/strong&gt; and password &lt;strong&gt;umami&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Start the Application&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;npm run start
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;By default, this will launch the application on &lt;code&gt;http://localhost:3000&lt;/code&gt;. You will need to either &lt;a href=&quot;https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/&quot;&gt;proxy&lt;/a&gt; requests from your web server or change the &lt;a href=&quot;https://nextjs.org/docs/api-reference/cli#production&quot;&gt;port&lt;/a&gt; to serve the application directly.&lt;/em&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üê≥ Installing with Docker&lt;/h2&gt; 
&lt;p&gt;To build the Umami container and start up a Postgres database, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, to pull just the Umami Docker image with PostgreSQL support:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker pull docker.umami.is/umami-software/umami:postgresql-latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or with MySQL support:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker pull docker.umami.is/umami-software/umami:mysql-latest
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üîÑ Getting Updates&lt;/h2&gt; 
&lt;p&gt;To get the latest features, simply do a pull, install any new dependencies, and rebuild:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git pull
npm install
npm run build
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To update the Docker image, simply pull the new images and rebuild:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker compose pull
docker compose up --force-recreate -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üõü Support&lt;/h2&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/umami-software/umami&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/GitHub--blue?style=social&amp;amp;logo=github&quot; alt=&quot;GitHub&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://twitter.com/umami_software&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Twitter--blue?style=social&amp;amp;logo=twitter&quot; alt=&quot;Twitter&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://linkedin.com/company/umami-software&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/LinkedIn--blue?style=social&amp;amp;logo=linkedin&quot; alt=&quot;LinkedIn&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://umami.is/discord&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Discord--blue?style=social&amp;amp;logo=discord&quot; alt=&quot;Discord&quot; /&gt; &lt;/a&gt; &lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lvgl/lvgl</title>
      <link>https://github.com/lvgl/lvgl</link>
      <description>&lt;p&gt;Embedded graphics library to create beautiful UIs for any MCU, MPU and display type.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/sponsors/lvgl&quot; target=&quot;_blank&quot;&gt;&lt;img align=&quot;left&quot; src=&quot;https://lvgl.io/github-assets/sponsor.png&quot; height=&quot;32px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;right&quot;&gt; &lt;b&gt;English&lt;/b&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/lvgl/lvgl/master/docs/README_zh.md&quot;&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/lvgl/lvgl/master/docs/README_pt_BR.md&quot;&gt;Portugu√™s do Brasil&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/lvgl/lvgl/master/docs/README_jp.md&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/lvgl/lvgl/master/docs/README_he.md&quot;&gt;◊¢◊ë◊®◊ô◊™&lt;/a&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p align=&quot;center&quot;&gt; &amp;nbsp; &lt;img src=&quot;https://lvgl.io/github-assets/logo-colored.png&quot; width=&quot;300px&quot; /&gt; &lt;/p&gt; 
&lt;p&gt;&amp;nbsp; &lt;/p&gt;
&lt;h1 align=&quot;center&quot;&gt;Light and Versatile Graphics Library&lt;/h1&gt; &amp;nbsp; 
&lt;br /&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://lvgl.io/github-assets/smartwatch-demo.gif&quot; /&gt; &amp;nbsp; &amp;nbsp; 
 &lt;img border=&quot;1px&quot; src=&quot;https://lvgl.io/github-assets/widgets-demo.gif&quot; /&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://lvgl.io&quot; title=&quot;Homepage of LVGL&quot;&gt;Website &lt;/a&gt; | &lt;a href=&quot;https://docs.lvgl.io/&quot; title=&quot;Detailed documentation with 100+ examples&quot;&gt;Docs&lt;/a&gt; | &lt;a href=&quot;https://forum.lvgl.io&quot; title=&quot;Get help and help others&quot;&gt;Forum&lt;/a&gt; | &lt;a href=&quot;https://lvgl.io/demos&quot; title=&quot;Demos running in your browser&quot;&gt;Demos&lt;/a&gt; | &lt;a href=&quot;https://lvgl.io/services&quot; title=&quot;Graphics design, UI implementation and consulting&quot;&gt;Services&lt;/a&gt; &lt;/p&gt; 
&lt;br /&gt; 
&lt;h2&gt;&lt;span&gt;üìí&lt;/span&gt; Overview&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Mature and Well-known&lt;/strong&gt;&lt;br /&gt; LVGL is the most popular free and open source embedded graphics library to create beautiful UIs for any MCU, MPU and display type. It&#39;s supported by industry leading vendors and projects like &amp;nbsp;Arm, STM32, NXP, Espressif, Nuvoton, Arduino, RT-Thread, Zephyr, NuttX, Adafruit and many more.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Feature Rich&lt;/strong&gt;&lt;br /&gt; It has all the features to create modern and beautiful GUIs: 30+ built-in widgets, a powerful style system, web inspired layout managers, and a typography system supporting many languages. To integrate LVGL into your platform, all you need is at least 32kB RAM and 128 kB Flash, a C compiler, a frame buffer, and at least an 1/10 screen sized buffer for rendering.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Services&lt;/strong&gt;&lt;br /&gt; Our team is ready to help you with graphics design, UI implementation and consulting services. Contact us if you need some support during the development of your next GUI project.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üöÄ&lt;/span&gt; Features&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Free and Portable&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A fully portable C (C++ compatible) library with no external dependencies.&lt;/li&gt; 
 &lt;li&gt;Can be compiled to any MCU or MPU, with any (RT)OS.&lt;/li&gt; 
 &lt;li&gt;Supports monochrome, ePaper, OLED or TFT displays, or even monitors. &lt;a href=&quot;https://docs.lvgl.io/master/details/main-modules/display/index.html&quot;&gt;Displays&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Distributed under the MIT license, so you can easily use it in commercial projects too.&lt;/li&gt; 
 &lt;li&gt;Needs only 32kB RAM and 128 kB Flash, a frame buffer, and at least an 1/10 screen sized buffer for rendering.&lt;/li&gt; 
 &lt;li&gt;OS, External memory and GPU are supported but not required.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Widgets, Styles, Layouts and more&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;30+ built-in &lt;a href=&quot;https://docs.lvgl.io/master/details/widgets/index.html&quot;&gt;Widgets&lt;/a&gt;: &amp;nbsp;Button, Label, Slider, Chart, Keyboard, Meter, Arc, Table and many more.&lt;/li&gt; 
 &lt;li&gt;Flexible &lt;a href=&quot;https://docs.lvgl.io/master/details/common-widget-features/styles/index.html&quot;&gt;Style system&lt;/a&gt; with &amp;nbsp;~100 style properties to customize any part of the widgets in any state.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.lvgl.io/master/details/common-widget-features/layouts/flex.html&quot;&gt;Flexbox&lt;/a&gt; and &lt;a href=&quot;https://docs.lvgl.io/master/details/common-widget-features/layouts/grid.html&quot;&gt;Grid&lt;/a&gt;-like layouts engines to automatically size and position the widgets in a responsive way.&lt;/li&gt; 
 &lt;li&gt;Texts are rendered with UTF-8 encoding supporting CJK, Thai, Hindi, Arabic, Persian writing systems.&lt;/li&gt; 
 &lt;li&gt;Word wrapping, kerning, text scrolling, sub-pixel rendering, Pinyin-IME Chinese input, Emojis in texts.&lt;/li&gt; 
 &lt;li&gt;Rendering engine supporting animations, anti-aliasing, opacity, smooth scrolling, shadows, image transformation, etc &amp;nbsp;&lt;/li&gt; 
 &lt;li&gt;Supports Mouse, Touchpad, Keypad, Keyboard, External buttons, Encoder &lt;a href=&quot;https://docs.lvgl.io/master/details/main-modules/indev.html&quot;&gt;Input devices&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.lvgl.io/master/details/main-modules/display/overview.html#how-many-displays-can-lvgl-use&quot;&gt;Multiple display&lt;/a&gt; support.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Binding and Build Support&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://blog.lvgl.io/2019-02-20/micropython-bindings&quot;&gt;MicroPython Binding&lt;/a&gt; exposes LVGL API&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://blog.lvgl.io/2022-08-24/pikascript-and-lvgl&quot;&gt;PikaScript Binding&lt;/a&gt; python on MCU lighter and easier.&lt;/li&gt; 
 &lt;li&gt;No custom build system is used. You can build LVGL as you build the other files of your project.&lt;/li&gt; 
 &lt;li&gt;Support for Make and &lt;a href=&quot;https://docs.lvgl.io/master/details/integration/building/cmake.html&quot;&gt;CMake&lt;/a&gt; is included out of the box.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.lvgl.io/master/details/integration/ide/pc-simulator.html&quot;&gt;Develop on PC&lt;/a&gt; and use the same UI code on embedded hardware.&lt;/li&gt; 
 &lt;li&gt;Convert the C UI code to HTML file with our &lt;a href=&quot;https://github.com/lvgl/lv_web_emscripten&quot;&gt;Emscripten port&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Docs, Tools, and Services&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Detailed &lt;a href=&quot;https://docs.lvgl.io/&quot;&gt;Documentation&lt;/a&gt; with &lt;a href=&quot;https://docs.lvgl.io/master/examples.html&quot;&gt;100+ simple examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://lvgl.io/services&quot;&gt;Services&lt;/a&gt; such as User interface design, Implementation and Consulting to make UI development simpler and faster.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;‚ù§Ô∏è&lt;/span&gt; Sponsor&lt;/h2&gt; 
&lt;p&gt;If LVGL saved you a lot of time and money or you just had fun using it, consider &lt;a href=&quot;https://github.com/sponsors/lvgl&quot;&gt;Supporting its Development&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How do we spend the donations?&lt;/strong&gt;&lt;br /&gt; Our goal is to provide financial compensation for people who do the most for LVGL. It means not only the maintainers but anyone who implements a great feature should get a payment from the accumulated money. We use the donations to cover our operational costs like servers and related services.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How to donate?&lt;/strong&gt;&lt;br /&gt; We use &lt;a href=&quot;https://github.com/sponsors/lvgl&quot;&gt;GitHub Sponsors&lt;/a&gt; where you can easily send one time or recurring donations. You can also see all of our expenses in a transparent way.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How to get paid for your contribution?&lt;/strong&gt;&lt;br /&gt; If someone implements or fixes an issue labeled as &lt;a href=&quot;https://github.com/lvgl/lvgl/labels/Sponsored&quot;&gt;Sponsored&lt;/a&gt; he or she will get a payment for that work. We estimate the required time, complexity and importance of the issue and set a price accordingly. To jump in just comment on a &lt;a href=&quot;https://github.com/lvgl/lvgl/labels/Sponsored&quot;&gt;Sponsored&lt;/a&gt; issue saying &quot;Hi, I&#39;d like to deal with it. This is how I&#39;m planning to fix/implement it...&quot;. A work is considered ready when it&#39;s approved and merged by a maintainer. After that you can submit and expense at &lt;a href=&quot;https://opencollective.com/lvgl&quot;&gt;opencollective.com&lt;/a&gt; and you will receive the payment in a few days.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Organizations supporting LVGL&lt;/strong&gt;&lt;br /&gt; &lt;a href=&quot;https://opencollective.com/lvgl&quot;&gt;&lt;img src=&quot;https://opencollective.com/lvgl/organizations.svg?width=600&quot; alt=&quot;Sponsors of LVGL&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Individuals supporting LVGL&lt;/strong&gt;&lt;br /&gt; &lt;a href=&quot;https://opencollective.com/lvgl&quot;&gt;&lt;img src=&quot;https://contrib.rocks/image?repo=lvgl/lvgl&amp;amp;max=48&quot; alt=&quot;Backers of LVGL&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üì¶&lt;/span&gt; Packages&lt;/h2&gt; 
&lt;p&gt;LVGL is available as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.lvgl.io/master/details/integration/framework/arduino.html&quot;&gt;Arduino library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://registry.platformio.org/libraries/lvgl/lvgl&quot;&gt;PlatformIO package&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.lvgl.io/master/details/integration/os/zephyr.html&quot;&gt;Zephyr library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://components.espressif.com/components/lvgl/lvgl&quot;&gt;ESP-IDF(ESP32) component&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.nxp.com/design/software/embedded-software/lvgl-open-source-graphics-library:LITTLEVGL-OPEN-SOURCE-GRAPHICS-LIBRARY&quot;&gt;NXP MCUXpresso component&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.lvgl.io/master/details/integration/os/nuttx.html&quot;&gt;NuttX library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.lvgl.io/master/details/integration/os/rt-thread.html&quot;&gt;RT-Thread RTOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CMSIS-Pack&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://doc.riot-os.org/group__pkg__lvgl.html#details&quot;&gt;RIOT OS package&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;span&gt;ü§ñ&lt;/span&gt; Examples&lt;/h2&gt; 
&lt;p&gt;See some examples of creating widgets, using layouts and applying styles. You will find C and MicroPython code, and links to try out or edit the examples in an online MicroPython editor.&lt;/p&gt; 
&lt;p&gt;For more examples check out the &lt;a href=&quot;https://github.com/lvgl/lvgl/tree/master/examples&quot;&gt;Examples&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h3&gt;Hello world label&lt;/h3&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/kisvegabor/test/raw/master/readme_example_1.png&quot; alt=&quot;Simple Hello world label example in LVGL&quot; /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;C code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;/*Change the active screen&#39;s background color*/
lv_obj_set_style_bg_color(lv_screen_active(), lv_color_hex(0x003a57), LV_PART_MAIN);

/*Create a white label, set its text and align it to the center*/
lv_obj_t * label = lv_label_create(lv_screen_active());
lv_label_set_text(label, &quot;Hello world&quot;);
lv_obj_set_style_text_color(label, lv_color_hex(0xffffff), LV_PART_MAIN);
lv_obj_align(label, LV_ALIGN_CENTER, 0, 0);
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;MicroPython code | &lt;a href=&quot;https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_direct=4ab7c40c35b0dc349aa2f0c3b00938d7d8e8ac9f&quot; target=&quot;_blank&quot;&gt;Online Simulator&lt;/a&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Change the active screen&#39;s background color
scr = lv.screen_active()
scr.set_style_bg_color(lv.color_hex(0x003a57), lv.PART.MAIN)

# Create a white label, set its text and align it to the center
label = lv.label(lv.screen_active())
label.set_text(&quot;Hello world&quot;)
label.set_style_text_color(lv.color_hex(0xffffff), lv.PART.MAIN)
label.align(lv.ALIGN.CENTER, 0, 0)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;Button with Click Event&lt;/h3&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/kisvegabor/test/raw/master/readme_example_2.gif&quot; alt=&quot;LVGL button with label example&quot; /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;C code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;lv_obj_t * button = lv_button_create(lv_screen_active()); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; /*Add a button to the current screen*/
lv_obj_center(button); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;         /*Set its position*/
lv_obj_set_size(button, 100, 50); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;/*Set its size*/
lv_obj_add_event_cb(button, button_event_cb, LV_EVENT_CLICKED, NULL); /*Assign a callback to the button*/

lv_obj_t * label = lv_label_create(button); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;/*Add a label to the button*/
lv_label_set_text(label, &quot;Button&quot;); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; /*Set the labels text*/
lv_obj_center(label); &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; /*Align the label to the center*/
...

void button_event_cb(lv_event_t * e)
{
&amp;nbsp; printf(&quot;Clicked\n&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;MicroPython code | &lt;a href=&quot;https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&amp;amp;script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&amp;amp;script_direct=926bde43ec7af0146c486de470c53f11f167491e&quot; target=&quot;_blank&quot;&gt;Online Simulator&lt;/a&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def button_event_cb(e):
&amp;nbsp; print(&quot;Clicked&quot;)

# Create a Button and a Label
button = lv.button(lv.screen_active())
button.center()
button.set_size(100, 50)
button.add_event_cb(button_event_cb, lv.EVENT.CLICKED, None)

label = lv.label(button)
label.set_text(&quot;Button&quot;)
label.center()
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;Checkboxes with Layout&lt;/h3&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/kisvegabor/test/raw/master/readme_example_3.gif&quot; alt=&quot;Checkboxes with layout in LVGL&quot; /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;C code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;
lv_obj_set_flex_flow(lv_screen_active(), LV_FLEX_FLOW_COLUMN);
lv_obj_set_flex_align(lv_screen_active(), LV_FLEX_ALIGN_CENTER, LV_FLEX_ALIGN_START, LV_FLEX_ALIGN_CENTER);

lv_obj_t * cb;
cb = lv_checkbox_create(lv_screen_active());
lv_checkbox_set_text(cb, &quot;Apple&quot;);
lv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);

cb = lv_checkbox_create(lv_screen_active());
lv_checkbox_set_text(cb, &quot;Banana&quot;);
lv_obj_add_state(cb, LV_STATE_CHECKED);
lv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);

cb = lv_checkbox_create(lv_screen_active());
lv_checkbox_set_text(cb, &quot;Lemon&quot;);
lv_obj_add_state(cb, LV_STATE_DISABLED);
lv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);

cb = lv_checkbox_create(lv_screen_active());
lv_obj_add_state(cb, LV_STATE_CHECKED);
lv_obj_add_state(cb, LV_STATE_DISABLED);
lv_checkbox_set_text(cb, &quot;Melon\nand a new line&quot;);
lv_obj_add_event_cb(cb, event_handler, LV_EVENT_ALL, NULL);
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;MicroPython code | &lt;a href=&quot;https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&amp;amp;script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&amp;amp;script_direct=311d37e5f70daf1cb0d2cad24c7f72751b5f1792&quot; target=&quot;_blank&quot;&gt;Online Simulator&lt;/a&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;def event_handler(e):
    code = e.get_code()
    obj = e.get_target_obj()
    if code == lv.EVENT.VALUE_CHANGED:
        txt = obj.get_text()
        if obj.get_state() &amp;amp; lv.STATE.CHECKED:
            state = &quot;Checked&quot;
        else:
            state = &quot;Unchecked&quot;
        print(txt + &quot;:&quot; + state)


lv.screen_active().set_flex_flow(lv.FLEX_FLOW.COLUMN)
lv.screen_active().set_flex_align(lv.FLEX_ALIGN.CENTER, lv.FLEX_ALIGN.START, lv.FLEX_ALIGN.CENTER)

cb = lv.checkbox(lv.screen_active())
cb.set_text(&quot;Apple&quot;)
cb.add_event_cb(event_handler, lv.EVENT.ALL, None)

cb = lv.checkbox(lv.screen_active())
cb.set_text(&quot;Banana&quot;)
cb.add_state(lv.STATE.CHECKED)
cb.add_event_cb(event_handler, lv.EVENT.ALL, None)

cb = lv.checkbox(lv.screen_active())
cb.set_text(&quot;Lemon&quot;)
cb.add_state(lv.STATE.DISABLED)
cb.add_event_cb(event_handler, lv.EVENT.ALL, None)

cb = lv.checkbox(lv.screen_active())
cb.add_state(lv.STATE.CHECKED | lv.STATE.DISABLED)
cb.set_text(&quot;Melon&quot;)
cb.add_event_cb(event_handler, lv.EVENT.ALL, None)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;Styling a Slider&lt;/h3&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/kisvegabor/test/raw/master/readme_example_4.gif&quot; alt=&quot;Styling a slider with LVGL&quot; /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;C code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;lv_obj_t * slider = lv_slider_create(lv_screen_active());
lv_slider_set_value(slider, 70, LV_ANIM_OFF);
lv_obj_set_size(slider, 300, 20);
lv_obj_center(slider);

/*Add local styles to MAIN part (background rectangle)*/
lv_obj_set_style_bg_color(slider, lv_color_hex(0x0F1215), LV_PART_MAIN);
lv_obj_set_style_bg_opa(slider, 255, LV_PART_MAIN);
lv_obj_set_style_border_color(slider, lv_color_hex(0x333943), LV_PART_MAIN);
lv_obj_set_style_border_width(slider, 5, LV_PART_MAIN);
lv_obj_set_style_pad_all(slider, 5, LV_PART_MAIN);

/*Create a reusable style sheet for the INDICATOR part*/
static lv_style_t style_indicator;
lv_style_init(&amp;amp;style_indicator);
lv_style_set_bg_color(&amp;amp;style_indicator, lv_color_hex(0x37B9F5));
lv_style_set_bg_grad_color(&amp;amp;style_indicator, lv_color_hex(0x1464F0));
lv_style_set_bg_grad_dir(&amp;amp;style_indicator, LV_GRAD_DIR_HOR);
lv_style_set_shadow_color(&amp;amp;style_indicator, lv_color_hex(0x37B9F5));
lv_style_set_shadow_width(&amp;amp;style_indicator, 15);
lv_style_set_shadow_spread(&amp;amp;style_indicator, 5);
4
/*Add the style sheet to the slider&#39;s INDICATOR part*/
lv_obj_add_style(slider, &amp;amp;style_indicator, LV_PART_INDICATOR);

/*Add the same style to the KNOB part too and locally overwrite some properties*/
lv_obj_add_style(slider, &amp;amp;style_indicator, LV_PART_KNOB);

lv_obj_set_style_outline_color(slider, lv_color_hex(0x0096FF), LV_PART_KNOB);
lv_obj_set_style_outline_width(slider, 3, LV_PART_KNOB);
lv_obj_set_style_outline_pad(slider, -5, LV_PART_KNOB);
lv_obj_set_style_shadow_spread(slider, 2, LV_PART_KNOB);
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;MicroPython code | &lt;a href=&quot;https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&amp;amp;script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&amp;amp;script_direct=c431c7b4dfd2cc0dd9c392b74365d5af6ea986f0&quot; target=&quot;_blank&quot;&gt;Online Simulator&lt;/a&gt; &lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Create a slider and add the style
slider = lv.slider(lv.screen_active())
slider.set_value(70, lv.ANIM.OFF)
slider.set_size(300, 20)
slider.center()

# Add local styles to MAIN part (background rectangle)
slider.set_style_bg_color(lv.color_hex(0x0F1215), lv.PART.MAIN)
slider.set_style_bg_opa(255, lv.PART.MAIN)
slider.set_style_border_color(lv.color_hex(0x333943), lv.PART.MAIN)
slider.set_style_border_width(5, lv.PART.MAIN)
slider.set_style_pad_all(5, lv.PART.MAIN)

# Create a reusable style sheet for the INDICATOR part
style_indicator = lv.style_t()
style_indicator.init()
style_indicator.set_bg_color(lv.color_hex(0x37B9F5))
style_indicator.set_bg_grad_color(lv.color_hex(0x1464F0))
style_indicator.set_bg_grad_dir(lv.GRAD_DIR.HOR)
style_indicator.set_shadow_color(lv.color_hex(0x37B9F5))
style_indicator.set_shadow_width(15)
style_indicator.set_shadow_spread(5)

# Add the style sheet to the slider&#39;s INDICATOR part
slider.add_style(style_indicator, lv.PART.INDICATOR)
slider.add_style(style_indicator, lv.PART.KNOB)

# Add the same style to the KNOB part too and locally overwrite some properties
slider.set_style_outline_color(lv.color_hex(0x0096FF), lv.PART.KNOB)
slider.set_style_outline_width(3, lv.PART.KNOB)
slider.set_style_outline_pad(-5, lv.PART.KNOB)
slider.set_style_shadow_spread(2, lv.PART.KNOB)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;br /&gt; 
&lt;h3&gt;English, Hebrew (mixed LTR-RTL) and Chinese texts&lt;/h3&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/kisvegabor/test/raw/master/readme_example_5.png&quot; alt=&quot;English, Hebrew and Chinese texts with LVGL&quot; /&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;C code&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-c&quot;&gt;lv_obj_t * ltr_label = lv_label_create(lv_screen_active());
lv_label_set_text(ltr_label, &quot;In modern terminology, a microcontroller is similar to a system on a chip (SoC).&quot;);
lv_obj_set_style_text_font(ltr_label, &amp;amp;lv_font_montserrat_16, 0);
lv_obj_set_width(ltr_label, 310);
lv_obj_align(ltr_label, LV_ALIGN_TOP_LEFT, 5, 5);

lv_obj_t * rtl_label = lv_label_create(lv_screen_active());
lv_label_set_text(rtl_label,&quot;◊û◊¢◊ë◊ì, ◊ê◊ï ◊ë◊©◊û◊ï ◊î◊û◊ú◊ê ◊ô◊ó◊ô◊ì◊™ ◊¢◊ô◊ë◊ï◊ì ◊û◊®◊õ◊ñ◊ô◊™ (◊ë◊ê◊†◊í◊ú◊ô◊™: CPU - Central Processing Unit).&quot;);
lv_obj_set_style_base_dir(rtl_label, LV_BASE_DIR_RTL, 0);
lv_obj_set_style_text_font(rtl_label, &amp;amp;lv_font_dejavu_16_persian_hebrew, 0);
lv_obj_set_width(rtl_label, 310);
lv_obj_align(rtl_label, LV_ALIGN_LEFT_MID, 5, 0);

lv_obj_t * cz_label = lv_label_create(lv_screen_active());
lv_label_set_text(cz_label,
                  &quot;ÂµåÂÖ•ÂºèÁ≥ªÁªüÔºàEmbedded SystemÔºâÔºå\nÊòØ‰∏ÄÁßçÂµåÂÖ•Êú∫Ê¢∞ÊàñÁîµÊ∞îÁ≥ªÁªüÂÜÖÈÉ®„ÄÅÂÖ∑Êúâ‰∏ì‰∏ÄÂäüËÉΩÂíåÂÆûÊó∂ËÆ°ÁÆóÊÄßËÉΩÁöÑËÆ°ÁÆóÊú∫Á≥ªÁªü„ÄÇ&quot;);
lv_obj_set_style_text_font(cz_label, &amp;amp;lv_font_source_han_sans_sc_16_cjk, 0);
lv_obj_set_width(cz_label, 310);
lv_obj_align(cz_label, LV_ALIGN_BOTTOM_LEFT, 5, -5);
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;MicroPython code | &lt;a href=&quot;https://sim.lvgl.io/v8.3/micropython/ports/javascript/index.html?script_startup=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/header.py&amp;amp;script=https://raw.githubusercontent.com/lvgl/lvgl/0d9ab4ee0e591aad1970e3c9164fd7c544ecce70/examples/widgets/slider/lv_example_slider_2.py&amp;amp;script_direct=18bb38200a64e10ead1aa17a65c977fc18131842&quot; target=&quot;_blank&quot;&gt;Online Simulator&lt;/a&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;ltr_label = lv.label(lv.screen_active())
ltr_label.set_text(&quot;In modern terminology, a microcontroller is similar to a system on a chip (SoC).&quot;)
ltr_label.set_style_text_font(lv.font_montserrat_16, 0);

ltr_label.set_width(310)
ltr_label.align(lv.ALIGN.TOP_LEFT, 5, 5)

rtl_label = lv.label(lv.screen_active())
rtl_label.set_text(&quot;◊û◊¢◊ë◊ì, ◊ê◊ï ◊ë◊©◊û◊ï ◊î◊û◊ú◊ê ◊ô◊ó◊ô◊ì◊™ ◊¢◊ô◊ë◊ï◊ì ◊û◊®◊õ◊ñ◊ô◊™ (◊ë◊ê◊†◊í◊ú◊ô◊™: CPU - Central Processing Unit).&quot;)
rtl_label.set_style_base_dir(lv.BASE_DIR.RTL, 0)
rtl_label.set_style_text_font(lv.font_dejavu_16_persian_hebrew, 0)
rtl_label.set_width(310)
rtl_label.align(lv.ALIGN.LEFT_MID, 5, 0)

font_hans_sans_16_cjk = lv.font_load(&quot;S:../../assets/font/lv_font_source_han_sans_sc_16_cjk.fnt&quot;)

cz_label = lv.label(lv.screen_active())
cz_label.set_style_text_font(font_hans_sans_16_cjk, 0)
cz_label.set_text(&quot;ÂµåÂÖ•ÂºèÁ≥ªÁªüÔºàEmbedded SystemÔºâÔºå\nÊòØ‰∏ÄÁßçÂµåÂÖ•Êú∫Ê¢∞ÊàñÁîµÊ∞îÁ≥ªÁªüÂÜÖÈÉ®„ÄÅÂÖ∑Êúâ‰∏ì‰∏ÄÂäüËÉΩÂíåÂÆûÊó∂ËÆ°ÁÆóÊÄßËÉΩÁöÑËÆ°ÁÆóÊú∫Á≥ªÁªü„ÄÇ&quot;)
cz_label.set_width(310)
cz_label.align(lv.ALIGN.BOTTOM_LEFT, 5, -5)

&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;&lt;span&gt;‚ñ∂&lt;/span&gt; Get started&lt;/h2&gt; 
&lt;p&gt;This list will guide you to get started with LVGL step-by-step.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Get Familiar with LVGL&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check the &lt;a href=&quot;https://lvgl.io/demos&quot;&gt;Online demos&lt;/a&gt; to see LVGL in action (3 minutes).&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href=&quot;https://docs.lvgl.io/master/intro/introduction/index.html&quot;&gt;Introduction&lt;/a&gt; page of the documentation (5 minutes).&lt;/li&gt; 
 &lt;li&gt;Get familiar with the basics on the &lt;a href=&quot;https://docs.lvgl.io/master/intro/getting_started/learn_the_basics.html&quot;&gt;Quick overview&lt;/a&gt; page (15 minutes).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Start to Use LVGL&lt;/strong&gt;&lt;/p&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Set up a &lt;a href=&quot;https://docs.lvgl.io/master/details/integration/ide/pc-simulator.html#simulator&quot;&gt;Simulator&lt;/a&gt; (10 minutes).&lt;/li&gt; 
 &lt;li&gt;Try out some &lt;a href=&quot;https://github.com/lvgl/lvgl/tree/master/examples&quot;&gt;Examples&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Port LVGL to a board. See the &lt;a href=&quot;https://docs.lvgl.io/master/details/integration/adding-lvgl-to-your-project/index.html&quot;&gt;Porting&lt;/a&gt; guide or check out the ready-to-use &lt;a href=&quot;https://github.com/lvgl?q=lv_port_&quot;&gt;Projects&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Become a Pro&lt;/strong&gt;&lt;/p&gt; 
&lt;ol start=&quot;7&quot;&gt; 
 &lt;li&gt;Read the &lt;a href=&quot;https://docs.lvgl.io/master/details/main-modules/index.html&quot;&gt;Main-Modules&lt;/a&gt; page to get a better understanding of the library (2-3 hours)&lt;/li&gt; 
 &lt;li&gt;Check the documentation of the &lt;a href=&quot;https://docs.lvgl.io/master/details/widgets/index.html&quot;&gt;Widgets&lt;/a&gt; to see their features and usage&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Get Help and Help Others&lt;/strong&gt;&lt;/p&gt; 
&lt;ol start=&quot;9&quot;&gt; 
 &lt;li&gt;If you have questions go to the &lt;a href=&quot;http://forum.lvgl.io/&quot;&gt;Forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href=&quot;https://docs.lvgl.io/master/contributing/index.html&quot;&gt;Contributing&lt;/a&gt; guide to see how you can help to improve LVGL (15 minutes)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;span&gt;ü§ù&lt;/span&gt; Services&lt;/h2&gt; 
&lt;p&gt;LVGL LLC was established to provide a solid background for LVGL library and to offer several type of services to help you in UI development. With 15+ years of experience in the user interface and graphics industry we can help you the bring your UI to the next level.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Graphics design&lt;/strong&gt; Our in-house graphics designers are experts in creating beautiful modern designs which fit to your product and the resources of your hardware.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI implementation&lt;/strong&gt; We can also implement your UI based on the design you or we have created. You can be sure that we will make the most out of your hardware and LVGL. If a feature or widget is missing from LVGL, don&#39;t worry, we will implement it for you.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consulting and Support&lt;/strong&gt; We can support you with consulting as well to avoid pricey and time consuming mistakes during the UI development.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Board certification&lt;/strong&gt; For companies who are offering development boards, or production ready kits we do board certification which shows how board can run LVGL.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Check out our &lt;a href=&quot;https://lvgl.io/demos&quot;&gt;Demos&lt;/a&gt; as reference. For more information take look at the &lt;a href=&quot;https://lvgl.io/services&quot;&gt;Services page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://lvgl.io/#contact&quot;&gt;Contact us&lt;/a&gt; and tell how we can help.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;üåü&lt;/span&gt; Contributing&lt;/h2&gt; 
&lt;p&gt;LVGL is an open project and contribution is very welcome. There are many ways to contribute from simply speaking about your project, through writing examples, improving the documentation, fixing bugs or even hosting your own project under the LVGL organization.&lt;/p&gt; 
&lt;p&gt;For a detailed description of contribution opportunities visit the &lt;a href=&quot;https://docs.lvgl.io/master/contributing/index.html&quot;&gt;Contributing&lt;/a&gt; section of the documentation.&lt;/p&gt; 
&lt;p&gt;More than 300 people already left their fingerprint in LVGL. Be one them! See you here! &lt;span&gt;üôÇ&lt;/span&gt;&lt;/p&gt; 
&lt;a href=&quot;https://github.com/lvgl/lvgl/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=lvgl/lvgl&amp;amp;max=48&quot; /&gt; &lt;/a&gt; 
&lt;p&gt;... and many other.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-cookbook</title>
      <link>https://github.com/openai/openai-cookbook</link>
      <description>&lt;p&gt;Examples and guides for using the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;a href=&quot;https://cookbook.openai.com&quot; target=&quot;_blank&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;/images/openai-cookbook-white.png&quot; style=&quot;max-width: 100%; width: 400px; margin-bottom: 20px&quot; /&gt; 
  &lt;img alt=&quot;OpenAI Cookbook Logo&quot; src=&quot;https://raw.githubusercontent.com/openai/openai-cookbook/main/images/openai-cookbook.png&quot; width=&quot;400px&quot; /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h3&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ú® Navigate at &lt;a href=&quot;https://cookbook.openai.com&quot;&gt;cookbook.openai.com&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Example code and guides for accomplishing common tasks with the &lt;a href=&quot;https://platform.openai.com/docs/introduction&quot;&gt;OpenAI API&lt;/a&gt;. To run these examples, you&#39;ll need an OpenAI account and associated API key (&lt;a href=&quot;https://platform.openai.com/signup&quot;&gt;create a free account here&lt;/a&gt;). Set an environment variable called &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; with your API key. Alternatively, in most IDEs such as Visual Studio Code, you can create an &lt;code&gt;.env&lt;/code&gt; file at the root of your repo containing &lt;code&gt;OPENAI_API_KEY=&amp;lt;your API key&amp;gt;&lt;/code&gt;, which will be picked up by the notebooks.&lt;/p&gt; 
&lt;p&gt;Most code examples are written in Python, though the concepts can be applied in any language.&lt;/p&gt; 
&lt;p&gt;For other useful tools, guides and courses, check out these &lt;a href=&quot;https://cookbook.openai.com/related_resources&quot;&gt;related resources from around the web&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ggml-org/llama.cpp</title>
      <link>https://github.com/ggml-org/llama.cpp</link>
      <description>&lt;p&gt;LLM inference in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png&quot; alt=&quot;llama&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&quot; alt=&quot;License: MIT&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/ggml-org/llama.cpp&quot; alt=&quot;Release&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml&quot;&gt;&lt;img src=&quot;https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg?sanitize=true&quot; alt=&quot;Server&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/205&quot;&gt;Manifesto&lt;/a&gt; / &lt;a href=&quot;https://github.com/ggml-org/ggml&quot;&gt;ggml&lt;/a&gt; / &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/raw/master/docs/ops.md&quot;&gt;ops&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;LLM inference in C/C++&lt;/p&gt; 
&lt;h2&gt;Recent API changes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/issues/9289&quot;&gt;Changelog for &lt;code&gt;libllama&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/issues/9291&quot;&gt;Changelog for &lt;code&gt;llama-server&lt;/code&gt; REST API&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hot topics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for the &lt;code&gt;gpt-oss&lt;/code&gt; model with native MXFP4 format has been added | &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/15091&quot;&gt;PR&lt;/a&gt; | &lt;a href=&quot;https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss&quot;&gt;Collaboration with NVIDIA&lt;/a&gt; | &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/15095&quot;&gt;Comment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hot PRs: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pulls?q=is%3Apr+label%3Ahot+&quot;&gt;All&lt;/a&gt; | &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pulls?q=is%3Apr+label%3Ahot+is%3Aopen&quot;&gt;Open&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Multimodal support arrived in &lt;code&gt;llama-server&lt;/code&gt;: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/12898&quot;&gt;#12898&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/multimodal.md&quot;&gt;documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VS Code extension for FIM completions: &lt;a href=&quot;https://github.com/ggml-org/llama.vscode&quot;&gt;https://github.com/ggml-org/llama.vscode&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Vim/Neovim plugin for FIM completions: &lt;a href=&quot;https://github.com/ggml-org/llama.vim&quot;&gt;https://github.com/ggml-org/llama.vim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Introducing GGUF-my-LoRA &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/10123&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hugging Face Inference Endpoints now support GGUF out of the box! &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9669&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hugging Face GGUF editor: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9268&quot;&gt;discussion&lt;/a&gt; | &lt;a href=&quot;https://huggingface.co/spaces/CISCai/gguf-editor&quot;&gt;tool&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;Getting started with llama.cpp is straightforward. Here are several ways to install it on your machine:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install &lt;code&gt;llama.cpp&lt;/code&gt; using &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/install.md&quot;&gt;brew, nix or winget&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Run with Docker - see our &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md&quot;&gt;Docker documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Download pre-built binaries from the &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/releases&quot;&gt;releases page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Build from source by cloning this repository - check out &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md&quot;&gt;our build guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once installed, you&#39;ll need a model to work with. Head to the &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/#obtaining-and-quantizing-models&quot;&gt;Obtaining and quantizing models&lt;/a&gt; section to learn more.&lt;/p&gt; 
&lt;p&gt;Example command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;# Use a local model file
llama-cli -m my_model.gguf

# Or download and run a model directly from Hugging Face
llama-cli -hf ggml-org/gemma-3-1b-it-GGUF

# Launch OpenAI-compatible API server
llama-server -hf ggml-org/gemma-3-1b-it-GGUF
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;The main goal of &lt;code&gt;llama.cpp&lt;/code&gt; is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Plain C/C++ implementation without any dependencies&lt;/li&gt; 
 &lt;li&gt;Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks&lt;/li&gt; 
 &lt;li&gt;AVX, AVX2, AVX512 and AMX support for x86 architectures&lt;/li&gt; 
 &lt;li&gt;1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use&lt;/li&gt; 
 &lt;li&gt;Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads GPUs via MUSA)&lt;/li&gt; 
 &lt;li&gt;Vulkan and SYCL backend support&lt;/li&gt; 
 &lt;li&gt;CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;code&gt;llama.cpp&lt;/code&gt; project is the main playground for developing new features for the &lt;a href=&quot;https://github.com/ggml-org/ggml&quot;&gt;ggml&lt;/a&gt; library.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Models&lt;/summary&gt; 
 &lt;p&gt;Typically finetunes of the base models below are supported as well.&lt;/p&gt; 
 &lt;p&gt;Instructions for adding support for new models: &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/HOWTO-add-model.md&quot;&gt;HOWTO-add-model.md&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Text-only&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; LLaMA ü¶ô&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; LLaMA 2 ü¶ôü¶ô&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; LLaMA 3 ü¶ôü¶ôü¶ô&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/mistralai/Mistral-7B-v0.1&quot;&gt;Mistral 7B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=mistral-ai/Mixtral&quot;&gt;Mixtral MoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/databricks/dbrx-instruct&quot;&gt;DBRX&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=tiiuae/falcon&quot;&gt;Falcon&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ymcui/Chinese-LLaMA-Alpaca&quot;&gt;Chinese LLaMA / Alpaca&lt;/a&gt; and &lt;a href=&quot;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&quot;&gt;Chinese LLaMA-2 / Alpaca-2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/bofenghuang/vigogne&quot;&gt;Vigogne (French)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/5423&quot;&gt;BERT&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://bair.berkeley.edu/blog/2023/04/03/koala/&quot;&gt;Koala&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=baichuan-inc/Baichuan&quot;&gt;Baichuan 1 &amp;amp; 2&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/hiyouga/baichuan-7b-sft&quot;&gt;derivations&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=BAAI/Aquila&quot;&gt;Aquila 1 &amp;amp; 2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3187&quot;&gt;Starcoder models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/smallcloudai/Refact-1_6B-fim&quot;&gt;Refact&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3417&quot;&gt;MPT&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3553&quot;&gt;Bloom&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=01-ai/Yi&quot;&gt;Yi models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/stabilityai&quot;&gt;StableLM models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=deepseek-ai/deepseek&quot;&gt;Deepseek models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=Qwen/Qwen&quot;&gt;Qwen models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3557&quot;&gt;PLaMo-13B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=microsoft/phi&quot;&gt;Phi models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/11003&quot;&gt;PhiMoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/gpt2&quot;&gt;GPT-2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/5118&quot;&gt;Orion 14B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=internlm2&quot;&gt;InternLM2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/WisdomShell/codeshell&quot;&gt;CodeShell&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://ai.google.dev/gemma&quot;&gt;Gemma&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/state-spaces/mamba&quot;&gt;Mamba&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/keyfan/grok-1-hf&quot;&gt;Grok-1&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=xverse&quot;&gt;Xverse&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=CohereForAI/c4ai-command-r&quot;&gt;Command-R models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=sea-lion&quot;&gt;SEA-LION&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/GritLM/GritLM-7B&quot;&gt;GritLM-7B&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/GritLM/GritLM-8x7B&quot;&gt;GritLM-8x7B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://allenai.org/olmo&quot;&gt;OLMo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://allenai.org/olmo&quot;&gt;OLMo 2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/allenai/OLMoE-1B-7B-0924&quot;&gt;OLMoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330&quot;&gt;Granite models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/EleutherAI/gpt-neox&quot;&gt;GPT-NeoX&lt;/a&gt; + &lt;a href=&quot;https://github.com/EleutherAI/pythia&quot;&gt;Pythia&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520&quot;&gt;Snowflake-Arctic MoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=Smaug&quot;&gt;Smaug&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/LumiOpen/Poro-34B&quot;&gt;Poro 34B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/1bitLLM&quot;&gt;Bitnet b1.58 models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=flan-t5&quot;&gt;Flan T5&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca&quot;&gt;Open Elm models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/THUDM/chatglm3-6b&quot;&gt;ChatGLM3-6b&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/THUDM/glm-4-9b&quot;&gt;ChatGLM4-9b&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/THUDM/glm-edge-1.5b-chat&quot;&gt;GLMEdge-1.5b&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/THUDM/glm-edge-4b-chat&quot;&gt;GLMEdge-4b&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&quot;&gt;GLM-4-0414&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966&quot;&gt;SmolLM&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct&quot;&gt;EXAONE-3.0-7.8B-Instruct&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a&quot;&gt;FalconMamba Models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/inceptionai/jais-13b-chat&quot;&gt;Jais&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a&quot;&gt;Bielik-11B-v2.3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/BlinkDL/RWKV-LM&quot;&gt;RWKV-6&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1&quot;&gt;QRWKV-6&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct&quot;&gt;GigaChat-20B-A3B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/trillionlabs/Trillion-7B-preview&quot;&gt;Trillion-7B-preview&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32&quot;&gt;Ling models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38&quot;&gt;LFM2 models&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;Multimodal&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e&quot;&gt;LLaVA 1.5 models&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2&quot;&gt;LLaVA 1.6 models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=SkunkworksAI/Bakllava&quot;&gt;BakLLaVA&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/NousResearch/Obsidian-3B-V0.5&quot;&gt;Obsidian&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=Lin-Chen/ShareGPT4V&quot;&gt;ShareGPT4V&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=mobileVLM&quot;&gt;MobileVLM 1.7B/3B models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=Yi-VL&quot;&gt;Yi-VL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=MiniCPM&quot;&gt;Mini CPM&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/vikhyatk/moondream2&quot;&gt;Moondream&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/BAAI-DCAI/Bunny&quot;&gt;Bunny&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=glm-edge&quot;&gt;GLM-EDGE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d&quot;&gt;Qwen2-VL&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Bindings&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Python: &lt;a href=&quot;https://github.com/ddh0/easy-llama&quot;&gt;ddh0/easy-llama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Python: &lt;a href=&quot;https://github.com/abetlen/llama-cpp-python&quot;&gt;abetlen/llama-cpp-python&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Go: &lt;a href=&quot;https://github.com/go-skynet/go-llama.cpp&quot;&gt;go-skynet/go-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Node.js: &lt;a href=&quot;https://github.com/withcatai/node-llama-cpp&quot;&gt;withcatai/node-llama-cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JS/TS (llama.cpp server client): &lt;a href=&quot;https://modelfusion.dev/integration/model-provider/llamacpp&quot;&gt;lgrammel/modelfusion&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JS/TS (Programmable Prompt Engine CLI): &lt;a href=&quot;https://github.com/offline-ai/cli&quot;&gt;offline-ai/cli&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JavaScript/Wasm (works in browser): &lt;a href=&quot;https://github.com/tangledgroup/llama-cpp-wasm&quot;&gt;tangledgroup/llama-cpp-wasm&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Typescript/Wasm (nicer API, available on npm): &lt;a href=&quot;https://github.com/ngxson/wllama&quot;&gt;ngxson/wllama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ruby: &lt;a href=&quot;https://github.com/yoshoku/llama_cpp.rb&quot;&gt;yoshoku/llama_cpp.rb&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (more features): &lt;a href=&quot;https://github.com/edgenai/llama_cpp-rs&quot;&gt;edgenai/llama_cpp-rs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (nicer API): &lt;a href=&quot;https://github.com/mdrokz/rust-llama.cpp&quot;&gt;mdrokz/rust-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (more direct bindings): &lt;a href=&quot;https://github.com/utilityai/llama-cpp-rs&quot;&gt;utilityai/llama-cpp-rs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (automated build from crates.io): &lt;a href=&quot;https://github.com/ShelbyJenkins/llm_client&quot;&gt;ShelbyJenkins/llm_client&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;C#/.NET: &lt;a href=&quot;https://github.com/SciSharp/LLamaSharp&quot;&gt;SciSharp/LLamaSharp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;C#/VB.NET (more features - community license): &lt;a href=&quot;https://docs.lm-kit.com/lm-kit-net/index.html&quot;&gt;LM-Kit.NET&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Scala 3: &lt;a href=&quot;https://github.com/donderom/llm4s&quot;&gt;donderom/llm4s&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Clojure: &lt;a href=&quot;https://github.com/phronmophobic/llama.clj&quot;&gt;phronmophobic/llama.clj&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;React Native: &lt;a href=&quot;https://github.com/mybigday/llama.rn&quot;&gt;mybigday/llama.rn&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Java: &lt;a href=&quot;https://github.com/kherud/java-llama.cpp&quot;&gt;kherud/java-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zig: &lt;a href=&quot;https://github.com/Deins/llama.cpp.zig&quot;&gt;deins/llama.cpp.zig&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Flutter/Dart: &lt;a href=&quot;https://github.com/netdur/llama_cpp_dart&quot;&gt;netdur/llama_cpp_dart&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Flutter: &lt;a href=&quot;https://github.com/xuegao-tzx/Fllama&quot;&gt;xuegao-tzx/Fllama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;PHP (API bindings and features built on top of llama.cpp): &lt;a href=&quot;https://github.com/distantmagic/resonance&quot;&gt;distantmagic/resonance&lt;/a&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/6326&quot;&gt;(more info)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Guile Scheme: &lt;a href=&quot;https://savannah.nongnu.org/projects/guile-llama-cpp&quot;&gt;guile_llama_cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Swift &lt;a href=&quot;https://github.com/srgtuszy/llama-cpp-swift&quot;&gt;srgtuszy/llama-cpp-swift&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Swift &lt;a href=&quot;https://github.com/ShenghaiWang/SwiftLlama&quot;&gt;ShenghaiWang/SwiftLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Delphi &lt;a href=&quot;https://github.com/Embarcadero/llama-cpp-delphi&quot;&gt;Embarcadero/llama-cpp-delphi&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;UIs&lt;/summary&gt; 
 &lt;p&gt;&lt;em&gt;(to have a project listed here, it should clearly state that it depends on &lt;code&gt;llama.cpp&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/yaroslavyaroslav/OpenAI-sublime-text&quot;&gt;AI Sublime Text plugin&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/cztomsik/ava&quot;&gt;cztomsik/ava&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/alexpinel/Dot&quot;&gt;Dot&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ylsdamxssjxxdd/eva&quot;&gt;eva&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/iohub/coLLaMA&quot;&gt;iohub/collama&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/janhq/jan&quot;&gt;janhq/jan&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/johnbean393/Sidekick&quot;&gt;johnbean393/Sidekick&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/zhouwg/kantv?tab=readme-ov-file&quot;&gt;KanTV&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/firatkiral/kodibot&quot;&gt;KodiBot&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.vim&quot;&gt;llama.vim&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/abgulati/LARS&quot;&gt;LARS&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/vietanhdev/llama-assistant&quot;&gt;Llama Assistant&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/guinmoon/LLMFarm?tab=readme-ov-file&quot;&gt;LLMFarm&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/undreamai/LLMUnity&quot;&gt;LLMUnity&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://lmstudio.ai/&quot;&gt;LMStudio&lt;/a&gt; (proprietary)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/mudler/LocalAI&quot;&gt;LocalAI&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/LostRuins/koboldcpp&quot;&gt;LostRuins/koboldcpp&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://mindmac.app&quot;&gt;MindMac&lt;/a&gt; (proprietary)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/MindWorkAI/AI-Studio&quot;&gt;MindWorkAI/AI-Studio&lt;/a&gt; (FSL-1.1-MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/Mobile-Artificial-Intelligence/maid&quot;&gt;Mobile-Artificial-Intelligence/maid&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/Mozilla-Ocho/llamafile&quot;&gt;Mozilla-Ocho/llamafile&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/nat/openplayground&quot;&gt;nat/openplayground&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/nomic-ai/gpt4all&quot;&gt;nomic-ai/gpt4all&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ollama/ollama&quot;&gt;ollama/ollama&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/oobabooga/text-generation-webui&quot;&gt;oobabooga/text-generation-webui&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/a-ghorbani/pocketpal-ai&quot;&gt;PocketPal AI&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/psugihara/FreeChat&quot;&gt;psugihara/FreeChat&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ptsochantaris/emeltal&quot;&gt;ptsochantaris/emeltal&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/pythops/tenere&quot;&gt;pythops/tenere&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/containers/ramalama&quot;&gt;ramalama&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/semperai/amica&quot;&gt;semperai/amica&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/withcatai/catai&quot;&gt;withcatai/catai&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/blackhole89/autopen&quot;&gt;Autopen&lt;/a&gt; (GPL)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Tools&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/akx/ggify&quot;&gt;akx/ggify&lt;/a&gt; ‚Äì download PyTorch models from HuggingFace Hub and convert them to GGML&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/akx/ollama-dl&quot;&gt;akx/ollama-dl&lt;/a&gt; ‚Äì download models from the Ollama library to be used directly with llama.cpp&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/crashr/gppm&quot;&gt;crashr/gppm&lt;/a&gt; ‚Äì launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser&quot;&gt;gpustack/gguf-parser&lt;/a&gt; - review/check the GGUF file and estimate the memory usage&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902&quot;&gt;Styled Lines&lt;/a&gt; (proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Infrastructure&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/intentee/paddler&quot;&gt;Paddler&lt;/a&gt; - Open-source LLMOps platform for hosting and scaling AI in your own infrastructure&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/gpustack/gpustack&quot;&gt;GPUStack&lt;/a&gt; - Manage GPU clusters for running LLMs&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/onicai/llama_cpp_canister&quot;&gt;llama_cpp_canister&lt;/a&gt; - llama.cpp as a smart contract on the Internet Computer, using WebAssembly&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/mostlygeek/llama-swap&quot;&gt;llama-swap&lt;/a&gt; - transparent proxy that adds automatic model switching with llama-server&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/kalavai-net/kalavai-client&quot;&gt;Kalavai&lt;/a&gt; - Crowdsource end to end LLM deployment at any scale&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/InftyAI/llmaz&quot;&gt;llmaz&lt;/a&gt; - ‚ò∏Ô∏è Easy, advanced inference platform for large language models on Kubernetes.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Games&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/MorganRO8/Lucys_Labyrinth&quot;&gt;Lucy&#39;s Labyrinth&lt;/a&gt; - A simple maze game where agents controlled by an AI model will try to trick you.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Supported backends&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
   &lt;th&gt;Target devices&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#metal-build&quot;&gt;Metal&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Apple Silicon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#blas-build&quot;&gt;BLAS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/BLIS.md&quot;&gt;BLIS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/SYCL.md&quot;&gt;SYCL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Intel and Nvidia GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#musa&quot;&gt;MUSA&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Moore Threads GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cuda&quot;&gt;CUDA&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Nvidia GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#hip&quot;&gt;HIP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AMD GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#vulkan&quot;&gt;Vulkan&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cann&quot;&gt;CANN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ascend NPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/OPENCL.md&quot;&gt;OpenCL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Adreno GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#webgpu&quot;&gt;WebGPU [In Progress]&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc&quot;&gt;RPC&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Obtaining and quantizing models&lt;/h2&gt; 
&lt;p&gt;The &lt;a href=&quot;https://huggingface.co&quot;&gt;Hugging Face&lt;/a&gt; platform hosts a &lt;a href=&quot;https://huggingface.co/models?library=gguf&amp;amp;sort=trending&quot;&gt;number of LLMs&lt;/a&gt; compatible with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://huggingface.co/models?library=gguf&amp;amp;sort=trending&quot;&gt;Trending&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://huggingface.co/models?sort=trending&amp;amp;search=llama+gguf&quot;&gt;LLaMA&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can either manually download the GGUF file or directly use any &lt;code&gt;llama.cpp&lt;/code&gt;-compatible models from &lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt; or other model hosting sites, such as &lt;a href=&quot;https://modelscope.cn/&quot;&gt;ModelScope&lt;/a&gt;, by using this CLI argument: &lt;code&gt;-hf &amp;lt;user&amp;gt;/&amp;lt;model&amp;gt;[:quant]&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;llama-cli -hf ggml-org/gemma-3-1b-it-GGUF
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By default, the CLI would download from Hugging Face, you can switch to other options with the environment variable &lt;code&gt;MODEL_ENDPOINT&lt;/code&gt;. For example, you may opt to downloading model checkpoints from ModelScope or other model sharing communities by setting the environment variable, e.g. &lt;code&gt;MODEL_ENDPOINT=https://www.modelscope.cn/&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;After downloading a model, use the CLI tools to run it locally - see below.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;llama.cpp&lt;/code&gt; requires the model to be stored in the &lt;a href=&quot;https://github.com/ggml-org/ggml/raw/master/docs/gguf.md&quot;&gt;GGUF&lt;/a&gt; file format. Models in other data formats can be converted to GGUF using the &lt;code&gt;convert_*.py&lt;/code&gt; Python scripts in this repo.&lt;/p&gt; 
&lt;p&gt;The Hugging Face platform provides a variety of online tools for converting, quantizing and hosting models with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://huggingface.co/spaces/ggml-org/gguf-my-repo&quot;&gt;GGUF-my-repo space&lt;/a&gt; to convert to GGUF format and quantize model weights to smaller sizes&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://huggingface.co/spaces/ggml-org/gguf-my-lora&quot;&gt;GGUF-my-LoRA space&lt;/a&gt; to convert LoRA adapters to GGUF format (more info: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/10123&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://huggingface.co/spaces/CISCai/gguf-editor&quot;&gt;GGUF-editor space&lt;/a&gt; to edit GGUF meta data in the browser (more info: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9268&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/9268&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://ui.endpoints.huggingface.co/&quot;&gt;Inference Endpoints&lt;/a&gt; to directly host &lt;code&gt;llama.cpp&lt;/code&gt; in the cloud (more info: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9669&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To learn more about model quantization, &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/quantize/README.md&quot;&gt;read this documentation&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/main&quot;&gt;&lt;code&gt;llama-cli&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A CLI tool for accessing and experimenting with most of &lt;code&gt;llama.cpp&lt;/code&gt;&#39;s functionality.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Run in conversation mode&lt;/summary&gt; 
   &lt;p&gt;Models with a built-in chat template will automatically activate conversation mode. If this doesn&#39;t occur, you can manually enable it by adding &lt;code&gt;-cnv&lt;/code&gt; and specifying a suitable chat template with &lt;code&gt;--chat-template NAME&lt;/code&gt;&lt;/p&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-cli -m model.gguf

# &amp;gt; hi, who are you?
# Hi there! I&#39;m your helpful assistant! I&#39;m an AI-powered chatbot designed to assist and provide information to users like you. I&#39;m here to help answer your questions, provide guidance, and offer support on a wide range of topics. I&#39;m a friendly and knowledgeable AI, and I&#39;m always happy to help with anything you need. What&#39;s on your mind, and how can I assist you today?
#
# &amp;gt; what is 1+1?
# Easy peasy! The answer to 1+1 is... 2!
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run in conversation mode with custom chat template&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# use the &quot;chatml&quot; template (use -h to see the list of supported templates)
llama-cli -m model.gguf -cnv --chat-template chatml

# use a custom template
llama-cli -m model.gguf -cnv --in-prefix &#39;User: &#39; --reverse-prompt &#39;User:&#39;
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run simple text completion&lt;/summary&gt; 
   &lt;p&gt;To disable conversation mode explicitly, use &lt;code&gt;-no-cnv&lt;/code&gt;&lt;/p&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-cli -m model.gguf -p &quot;I believe the meaning of life is&quot; -n 128 -no-cnv

# I believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don&#39;t align with societal expectations. I think that&#39;s what I love about yoga ‚Äì it&#39;s not just a physical practice, but a spiritual one too. It&#39;s about connecting with yourself, listening to your inner voice, and honoring your own unique journey.
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Constrain the output with a custom grammar&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p &#39;Request: schedule a call at 8pm; Command:&#39;

# {&quot;appointmentTime&quot;: &quot;8pm&quot;, &quot;appointmentDetails&quot;: &quot;schedule a a call&quot;}
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;The &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/&quot;&gt;grammars/&lt;/a&gt; folder contains a handful of sample grammars. To write your own, check out the &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md&quot;&gt;GBNF Guide&lt;/a&gt;.&lt;/p&gt; 
   &lt;p&gt;For authoring more complex JSON grammars, check out &lt;a href=&quot;https://grammar.intrinsiclabs.ai/&quot;&gt;https://grammar.intrinsiclabs.ai/&lt;/a&gt;&lt;/p&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server&quot;&gt;&lt;code&gt;llama-server&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A lightweight, &lt;a href=&quot;https://github.com/openai/openai-openapi&quot;&gt;OpenAI API&lt;/a&gt; compatible, HTTP server for serving LLMs.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Start a local HTTP server with default configuration on port 8080&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-server -m model.gguf --port 8080

# Basic web UI can be accessed via browser: http://localhost:8080
# Chat completion endpoint: http://localhost:8080/v1/chat/completions
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Support multiple-users and parallel decoding&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# up to 4 concurrent requests, each with 4096 max context
llama-server -m model.gguf -c 16384 -np 4
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Enable speculative decoding&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# the draft.gguf model should be a small variant of the target model.gguf
llama-server -m model.gguf -md draft.gguf
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Serve an embedding model&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# use the /embedding endpoint
llama-server -m model.gguf --embedding --pooling cls -ub 8192
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Serve a reranking model&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# use the /reranking endpoint
llama-server -m model.gguf --reranking
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Constrain all outputs with a grammar&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# custom grammar
llama-server -m model.gguf --grammar-file grammar.gbnf

# JSON
llama-server -m model.gguf --grammar-file grammars/json.gbnf
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/perplexity&quot;&gt;&lt;code&gt;llama-perplexity&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A tool for measuring the &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/perplexity/README.md&quot;&gt;perplexity&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/%5Bhttps://huggingface.co/docs/transformers/perplexity%5D(https://huggingface.co/docs/transformers/perplexity)&quot;&gt;^1&lt;/a&gt; (and other quality metrics) of a model over a given text.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Measure the perplexity over a text file&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-perplexity -m model.gguf -f file.txt

# [1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...
# Final estimate: PPL = 5.4007 +/- 0.67339
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Measure KL divergence&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# TODO
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/llama-bench&quot;&gt;&lt;code&gt;llama-bench&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;Benchmark the performance of the inference for various parameters.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Run default benchmark&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-bench -m model.gguf

# Output:
# | model               |       size |     params | backend    | threads |          test |                  t/s |
# | ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         pp512 |      5765.41 ¬± 20.55 |
# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         tg128 |        197.71 ¬± 0.81 |
#
# build: 3e0ba0e60 (4229)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/run&quot;&gt;&lt;code&gt;llama-run&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A comprehensive example for running &lt;code&gt;llama.cpp&lt;/code&gt; models. Useful for inferencing. Used with RamaLama &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/%5BRamaLama%5D(https://github.com/containers/ramalama)&quot;&gt;^3&lt;/a&gt;.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run a model with a specific prompt (by default it&#39;s pulled from Ollama registry)&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-run granite-code
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/simple&quot;&gt;&lt;code&gt;llama-simple&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A minimal example for implementing apps with &lt;code&gt;llama.cpp&lt;/code&gt;. Useful for developers.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Basic text completion&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-simple -m model.gguf

# Hello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called &quot;The Art of
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Contributors can open PRs&lt;/li&gt; 
 &lt;li&gt;Collaborators can push to branches in the &lt;code&gt;llama.cpp&lt;/code&gt; repo and merge PRs into the &lt;code&gt;master&lt;/code&gt; branch&lt;/li&gt; 
 &lt;li&gt;Collaborators will be invited based on contributions&lt;/li&gt; 
 &lt;li&gt;Any help with managing issues, PRs and projects is very appreciated!&lt;/li&gt; 
 &lt;li&gt;See &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&quot;&gt;good first issues&lt;/a&gt; for tasks suitable for first contributions&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for more information&lt;/li&gt; 
 &lt;li&gt;Make sure to read this: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/205&quot;&gt;Inference at the edge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A bit of backstory for those who are interested: &lt;a href=&quot;https://changelog.com/podcast/532&quot;&gt;Changelog podcast&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Other documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/main/README.md&quot;&gt;main (cli)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md&quot;&gt;server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md&quot;&gt;GBNF grammars&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Development documentation&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md&quot;&gt;How to build&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md&quot;&gt;Running on Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/android.md&quot;&gt;Build on Android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/token_generation_performance_tips.md&quot;&gt;Performance troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/wiki/GGML-Tips-&amp;amp;-Tricks&quot;&gt;GGML tips &amp;amp; tricks&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Seminal papers and background on the models&lt;/h4&gt; 
&lt;p&gt;If your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LLaMA: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&quot;&gt;Introducing LLaMA: A foundational, 65-billion-parameter large language model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;GPT-3 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;GPT-3.5 / InstructGPT / ChatGPT: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://openai.com/research/instruction-following&quot;&gt;Aligning language models to follow instructions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;XCFramework&lt;/h2&gt; 
&lt;p&gt;The XCFramework is a precompiled version of the library for iOS, visionOS, tvOS, and macOS. It can be used in Swift projects without the need to compile the library from source. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-swift&quot;&gt;// swift-tools-version: 5.10
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let package = Package(
    name: &quot;MyLlamaPackage&quot;,
    targets: [
        .executableTarget(
            name: &quot;MyLlamaPackage&quot;,
            dependencies: [
                &quot;LlamaFramework&quot;
            ]),
        .binaryTarget(
            name: &quot;LlamaFramework&quot;,
            url: &quot;https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip&quot;,
            checksum: &quot;c19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab&quot;
        )
    ]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The above example is using an intermediate build &lt;code&gt;b5046&lt;/code&gt; of the library. This can be modified to use a different version by changing the URL and checksum.&lt;/p&gt; 
&lt;h2&gt;Completions&lt;/h2&gt; 
&lt;p&gt;Command-line completion is available for some environments.&lt;/p&gt; 
&lt;h4&gt;Bash Completion&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ build/bin/llama-cli --completion-bash &amp;gt; ~/.llama-completion.bash
$ source ~/.llama-completion.bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally this can be added to your &lt;code&gt;.bashrc&lt;/code&gt; or &lt;code&gt;.bash_profile&lt;/code&gt; to load it automatically. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;$ echo &quot;source ~/.llama-completion.bash&quot; &amp;gt;&amp;gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yhirose/cpp-httplib&quot;&gt;yhirose/cpp-httplib&lt;/a&gt; - Single-header HTTP server, used by &lt;code&gt;llama-server&lt;/code&gt; - MIT license&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nothings/stb&quot;&gt;stb-image&lt;/a&gt; - Single-header image format decoder, used by multimodal subsystem - Public domain&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nlohmann/json&quot;&gt;nlohmann/json&lt;/a&gt; - Single-header JSON library, used by various tools/examples - MIT License&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/minja&quot;&gt;minja&lt;/a&gt; - Minimal Jinja parser in C++, used by various tools/examples - MIT License&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/run/linenoise.cpp/linenoise.cpp&quot;&gt;linenoise.cpp&lt;/a&gt; - C++ library that provides readline-like line editing capabilities, used by &lt;code&gt;llama-run&lt;/code&gt; - BSD 2-Clause License&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://curl.se/&quot;&gt;curl&lt;/a&gt; - Client-side URL transfer library, used by various tools/examples - &lt;a href=&quot;https://curl.se/docs/copyright.html&quot;&gt;CURL License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mackron/miniaudio&quot;&gt;miniaudio.h&lt;/a&gt; - Single-header audio format decoder, used by multimodal subsystem - Public domain&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>dyad-sh/dyad</title>
      <link>https://github.com/dyad-sh/dyad</link>
      <description>&lt;p&gt;Free, local, open-source AI app builder ‚ú® v0 / lovable / Bolt alternative üåü Star if you like it!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dyad&lt;/h1&gt; 
&lt;p&gt;Dyad is a local, open-source AI app builder. It&#39;s fast, private, and fully under your control ‚Äî like Lovable, v0, or Bolt, but running right on your machine.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://dyad.sh/&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/f6c83dfc-6ffd-4d32-93dd-4b9c46d17790&quot; alt=&quot;Image&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;More info at: &lt;a href=&quot;http://dyad.sh/&quot;&gt;http://dyad.sh/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚ö°Ô∏è &lt;strong&gt;Local&lt;/strong&gt;: Fast, private and no lock-in.&lt;/li&gt; 
 &lt;li&gt;üõ† &lt;strong&gt;Bring your own keys&lt;/strong&gt;: Use your own AI API keys ‚Äî no vendor lock-in.&lt;/li&gt; 
 &lt;li&gt;üñ•Ô∏è &lt;strong&gt;Cross-platform&lt;/strong&gt;: Easy to run on Mac or Windows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üì¶ Download&lt;/h2&gt; 
&lt;p&gt;No sign-up required. Just download and go.&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://www.dyad.sh/#download&quot;&gt;üëâ Download for your platform&lt;/a&gt;&lt;/h3&gt; 
&lt;h2&gt;ü§ù Community&lt;/h2&gt; 
&lt;p&gt;Join our growing community of AI app builders on &lt;strong&gt;Reddit&lt;/strong&gt;: &lt;a href=&quot;https://www.reddit.com/r/dyadbuilders/&quot;&gt;r/dyadbuilders&lt;/a&gt; - share your projects and get help from the community!&lt;/p&gt; 
&lt;h2&gt;üõ†Ô∏è Contributing&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Dyad&lt;/strong&gt; is open-source (Apache 2.0 licensed).&lt;/p&gt; 
&lt;p&gt;If you&#39;re interested in contributing to dyad, please read our &lt;a href=&quot;https://raw.githubusercontent.com/dyad-sh/dyad/main/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; doc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ollama/ollama</title>
      <link>https://github.com/ollama/ollama</link>
      <description>&lt;p&gt;Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt;
  &amp;nbsp; 
 &lt;a href=&quot;https://ollama.com&quot;&gt; &lt;img alt=&quot;ollama&quot; width=&quot;240&quot; src=&quot;https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h1&gt;Ollama&lt;/h1&gt; 
&lt;p&gt;Get up and running with large language models.&lt;/p&gt; 
&lt;h3&gt;macOS&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://ollama.com/download/Ollama.dmg&quot;&gt;Download&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://ollama.com/download/OllamaSetup.exe&quot;&gt;Download&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Linux&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;curl -fsSL https://ollama.com/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ollama/ollama/raw/main/docs/linux.md&quot;&gt;Manual install instructions&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;The official &lt;a href=&quot;https://hub.docker.com/r/ollama/ollama&quot;&gt;Ollama Docker image&lt;/a&gt; &lt;code&gt;ollama/ollama&lt;/code&gt; is available on Docker Hub.&lt;/p&gt; 
&lt;h3&gt;Libraries&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ollama/ollama-python&quot;&gt;ollama-python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ollama/ollama-js&quot;&gt;ollama-js&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Community&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://discord.gg/ollama&quot;&gt;Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://reddit.com/r/ollama&quot;&gt;Reddit&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;To run and chat with &lt;a href=&quot;https://ollama.com/library/gemma3&quot;&gt;Gemma 3&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama run gemma3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model library&lt;/h2&gt; 
&lt;p&gt;Ollama supports a list of models available on &lt;a href=&quot;https://ollama.com/library&quot; title=&quot;ollama model library&quot;&gt;ollama.com/library&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Here are some example models that can be downloaded:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Parameters&lt;/th&gt; 
   &lt;th&gt;Size&lt;/th&gt; 
   &lt;th&gt;Download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemma 3&lt;/td&gt; 
   &lt;td&gt;1B&lt;/td&gt; 
   &lt;td&gt;815MB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run gemma3:1b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemma 3&lt;/td&gt; 
   &lt;td&gt;4B&lt;/td&gt; 
   &lt;td&gt;3.3GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run gemma3&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemma 3&lt;/td&gt; 
   &lt;td&gt;12B&lt;/td&gt; 
   &lt;td&gt;8.1GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run gemma3:12b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Gemma 3&lt;/td&gt; 
   &lt;td&gt;27B&lt;/td&gt; 
   &lt;td&gt;17GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run gemma3:27b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;QwQ&lt;/td&gt; 
   &lt;td&gt;32B&lt;/td&gt; 
   &lt;td&gt;20GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run qwq&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek-R1&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;4.7GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run deepseek-r1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;DeepSeek-R1&lt;/td&gt; 
   &lt;td&gt;671B&lt;/td&gt; 
   &lt;td&gt;404GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run deepseek-r1:671b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 4&lt;/td&gt; 
   &lt;td&gt;109B&lt;/td&gt; 
   &lt;td&gt;67GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama4:scout&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 4&lt;/td&gt; 
   &lt;td&gt;400B&lt;/td&gt; 
   &lt;td&gt;245GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama4:maverick&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.3&lt;/td&gt; 
   &lt;td&gt;70B&lt;/td&gt; 
   &lt;td&gt;43GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.3&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.2&lt;/td&gt; 
   &lt;td&gt;3B&lt;/td&gt; 
   &lt;td&gt;2.0GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.2&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.2&lt;/td&gt; 
   &lt;td&gt;1B&lt;/td&gt; 
   &lt;td&gt;1.3GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.2:1b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.2 Vision&lt;/td&gt; 
   &lt;td&gt;11B&lt;/td&gt; 
   &lt;td&gt;7.9GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.2-vision&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.2 Vision&lt;/td&gt; 
   &lt;td&gt;90B&lt;/td&gt; 
   &lt;td&gt;55GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.2-vision:90b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.1&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;4.7GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 3.1&lt;/td&gt; 
   &lt;td&gt;405B&lt;/td&gt; 
   &lt;td&gt;231GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama3.1:405b&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phi 4&lt;/td&gt; 
   &lt;td&gt;14B&lt;/td&gt; 
   &lt;td&gt;9.1GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run phi4&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Phi 4 Mini&lt;/td&gt; 
   &lt;td&gt;3.8B&lt;/td&gt; 
   &lt;td&gt;2.5GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run phi4-mini&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mistral&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;4.1GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run mistral&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Moondream 2&lt;/td&gt; 
   &lt;td&gt;1.4B&lt;/td&gt; 
   &lt;td&gt;829MB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run moondream&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Neural Chat&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;4.1GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run neural-chat&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Starling&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;4.1GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run starling-lm&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Code Llama&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;3.8GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run codellama&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Llama 2 Uncensored&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;3.8GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llama2-uncensored&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LLaVA&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;4.5GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run llava&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Granite-3.3&lt;/td&gt; 
   &lt;td&gt;8B&lt;/td&gt; 
   &lt;td&gt;4.9GB&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ollama run granite3.3&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Customize a model&lt;/h2&gt; 
&lt;h3&gt;Import from GGUF&lt;/h3&gt; 
&lt;p&gt;Ollama supports importing GGUF models in the Modelfile:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Create a file named &lt;code&gt;Modelfile&lt;/code&gt;, with a &lt;code&gt;FROM&lt;/code&gt; instruction with the local filepath to the model you want to import.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;FROM ./vicuna-33b.Q4_0.gguf
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Create the model in Ollama&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama create example -f Modelfile
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the model&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama run example
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Import from Safetensors&lt;/h3&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://raw.githubusercontent.com/ollama/ollama/main/docs/import.md&quot;&gt;guide&lt;/a&gt; on importing models for more information.&lt;/p&gt; 
&lt;h3&gt;Customize a prompt&lt;/h3&gt; 
&lt;p&gt;Models from the Ollama library can be customized with a prompt. For example, to customize the &lt;code&gt;llama3.2&lt;/code&gt; model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama pull llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create a &lt;code&gt;Modelfile&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;FROM llama3.2

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM &quot;&quot;&quot;
You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.
&quot;&quot;&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, create and run the model:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ollama create mario -f ./Modelfile
ollama run mario
&amp;gt;&amp;gt;&amp;gt; hi
Hello! It&#39;s your friend Mario.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information on working with a Modelfile, see the &lt;a href=&quot;https://raw.githubusercontent.com/ollama/ollama/main/docs/modelfile.md&quot;&gt;Modelfile&lt;/a&gt; documentation.&lt;/p&gt; 
&lt;h2&gt;CLI Reference&lt;/h2&gt; 
&lt;h3&gt;Create a model&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;ollama create&lt;/code&gt; is used to create a model from a Modelfile.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama create mymodel -f ./Modelfile
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pull a model&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama pull llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;This command can also be used to update a local model. Only the diff will be pulled.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Remove a model&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama rm llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Copy a model&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama cp llama3.2 my-model
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multiline input&lt;/h3&gt; 
&lt;p&gt;For multiline input, you can wrap text with &lt;code&gt;&quot;&quot;&quot;&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; &quot;&quot;&quot;Hello,
... world!
... &quot;&quot;&quot;
I&#39;m a basic program that prints the famous &quot;Hello, world!&quot; message to the console.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multimodal models&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;ollama run llava &quot;What&#39;s in this image? /Users/jmorgan/Desktop/smile.png&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;: The image features a yellow smiley face, which is likely the central focus of the picture.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Pass the prompt as an argument&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama run llama3.2 &quot;Summarize this file: $(cat README.md)&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;: Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Show model information&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama show llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;List models on your computer&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama list
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;List which models are currently loaded&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama ps
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Stop a model which is currently running&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ollama stop llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Start Ollama&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;ollama serve&lt;/code&gt; is used when you want to start ollama without running the desktop application.&lt;/p&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://github.com/ollama/ollama/raw/main/docs/development.md&quot;&gt;developer guide&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Running local builds&lt;/h3&gt; 
&lt;p&gt;Next, start the server:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;./ollama serve
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, in a separate shell, run a model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;./ollama run llama3.2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;REST API&lt;/h2&gt; 
&lt;p&gt;Ollama has a REST API for running and managing models.&lt;/p&gt; 
&lt;h3&gt;Generate a response&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;curl http://localhost:11434/api/generate -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;prompt&quot;:&quot;Why is the sky blue?&quot;
}&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Chat with a model&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;curl http://localhost:11434/api/chat -d &#39;{
  &quot;model&quot;: &quot;llama3.2&quot;,
  &quot;messages&quot;: [
    { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;why is the sky blue?&quot; }
  ]
}&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://raw.githubusercontent.com/ollama/ollama/main/docs/api.md&quot;&gt;API documentation&lt;/a&gt; for all endpoints.&lt;/p&gt; 
&lt;h2&gt;Community Integrations&lt;/h2&gt; 
&lt;h3&gt;Web &amp;amp; Desktop&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/open-webui/open-webui&quot;&gt;Open WebUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/aws-samples/swift-chat&quot;&gt;SwiftChat (macOS with ReactNative)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AugustDev/enchanted&quot;&gt;Enchanted (macOS native)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fmaclen/hollama&quot;&gt;Hollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ParisNeo/lollms-webui&quot;&gt;Lollms-Webui&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/danny-avila/LibreChat&quot;&gt;LibreChat&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bionic-gpt/bionic-gpt&quot;&gt;Bionic GPT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rtcfirefly/ollama-ui&quot;&gt;HTML UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jikkuatwork/saddle&quot;&gt;Saddle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.tagspaces.org&quot;&gt;TagSpaces&lt;/a&gt; (A platform for file-based apps, &lt;a href=&quot;https://docs.tagspaces.org/ai/&quot;&gt;utilizing Ollama&lt;/a&gt; for the generation of tags and descriptions)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ivanfioravanti/chatbot-ollama&quot;&gt;Chatbot UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mckaywrigley/chatbot-ui&quot;&gt;Chatbot UI v2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ollama-interface/Ollama-Gui?tab=readme-ov-file&quot;&gt;Typescript UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/richawo/minimal-llm-ui&quot;&gt;Minimalistic React UI for Ollama Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kevinhermawan/Ollamac&quot;&gt;Ollamac&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/enricoros/big-AGI&quot;&gt;big-AGI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cheshire-cat-ai/core&quot;&gt;Cheshire Cat assistant framework&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/semperai/amica&quot;&gt;Amica&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/BruceMacD/chatd&quot;&gt;chatd&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kghandour/Ollama-SwiftUI&quot;&gt;Ollama-SwiftUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langgenius/dify&quot;&gt;Dify.AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mindmac.app&quot;&gt;MindMac&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jakobhoeg/nextjs-ollama-llm-ui&quot;&gt;NextJS Web Interface for Ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://msty.app&quot;&gt;Msty&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Bin-Huang/Chatbox&quot;&gt;Chatbox&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tgraupmann/WinForm_Ollama_Copilot&quot;&gt;WinForm Ollama Copilot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web&quot;&gt;NextChat&lt;/a&gt; with &lt;a href=&quot;https://docs.nextchat.dev/models/ollama&quot;&gt;Get Started Doc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mmo80/alpaca-webui&quot;&gt;Alpaca WebUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/enoch1118/ollamaGUI&quot;&gt;OllamaGUI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/InternLM/OpenAOE&quot;&gt;OpenAOE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/leonid20000/OdinRunes&quot;&gt;Odin Runes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mrdjohnson/llm-x&quot;&gt;LLM-X&lt;/a&gt; (Progressive Web App)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Mintplex-Labs/anything-llm&quot;&gt;AnythingLLM (Docker + MacOs/Windows/Linux native app)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rapidarchitect/ollama_basic_chat&quot;&gt;Ollama Basic Chat: Uses HyperDiv Reactive UI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/drazdra/ollama-chats&quot;&gt;Ollama-chats RPG&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://intellibar.app/&quot;&gt;IntelliBar&lt;/a&gt; (AI-powered assistant for macOS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AliAhmedNada/jirapt&quot;&gt;Jirapt&lt;/a&gt; (Jira Integration to generate issues, tasks, epics)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AliAhmedNada/ojira&quot;&gt;ojira&lt;/a&gt; (Jira chrome plugin to easily generate descriptions for tasks)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/reid41/QA-Pilot&quot;&gt;QA-Pilot&lt;/a&gt; (Interactive chat tool that can leverage Ollama models for rapid understanding and navigation of GitHub code repositories)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sugarforever/chat-ollama&quot;&gt;ChatOllama&lt;/a&gt; (Open Source Chatbot based on Ollama with Knowledge Bases)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Nagi-ovo/CRAG-Ollama-Chat&quot;&gt;CRAG Ollama Chat&lt;/a&gt; (Simple Web Search with Corrective RAG)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/infiniflow/ragflow&quot;&gt;RAGFlow&lt;/a&gt; (Open-source Retrieval-Augmented Generation engine based on deep document understanding)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/StreamDeploy-DevRel/streamdeploy-llm-app-scaffold&quot;&gt;StreamDeploy&lt;/a&gt; (LLM Application Scaffold)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/swuecho/chat&quot;&gt;chat&lt;/a&gt; (chat web app for teams)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lobehub/lobe-chat&quot;&gt;Lobe Chat&lt;/a&gt; with &lt;a href=&quot;https://lobehub.com/docs/self-hosting/examples/ollama&quot;&gt;Integrating Doc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/datvodinh/rag-chatbot.git&quot;&gt;Ollama RAG Chatbot&lt;/a&gt; (Local Chat with multiple PDFs using Ollama and RAG)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.nurgo-software.com/products/brainsoup&quot;&gt;BrainSoup&lt;/a&gt; (Flexible native client with RAG &amp;amp; multi-agent automation)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Renset/macai&quot;&gt;macai&lt;/a&gt; (macOS client for Ollama, ChatGPT, and other compatible API back-ends)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/josStorer/RWKV-Runner&quot;&gt;RWKV-Runner&lt;/a&gt; (RWKV offline LLM deployment tool, also usable as a client for ChatGPT and Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dezoito/ollama-grid-search&quot;&gt;Ollama Grid Search&lt;/a&gt; (app to evaluate and compare models)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Otacon/olpaka&quot;&gt;Olpaka&lt;/a&gt; (User-friendly Flutter Web App for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://casibase.org&quot;&gt;Casibase&lt;/a&gt; (An open source AI knowledge base and dialogue system combining the latest RAG, SSO, ollama support, and multiple large language models.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/CrazyNeil/OllamaSpring&quot;&gt;OllamaSpring&lt;/a&gt; (Ollama Client for macOS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kartikm7/llocal&quot;&gt;LLocal.in&lt;/a&gt; (Easy to use Electron Desktop Client for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dcSpark/shinkai-apps&quot;&gt;Shinkai Desktop&lt;/a&gt; (Two click install Local AI using Ollama + Files + RAG)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zeyoyt/ailama&quot;&gt;AiLama&lt;/a&gt; (A Discord User App that allows you to interact with Ollama anywhere in Discord)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rapidarchitect/ollama_mesop/&quot;&gt;Ollama with Google Mesop&lt;/a&gt; (Mesop Chat Client implementation with Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SciPhi-AI/R2R&quot;&gt;R2R&lt;/a&gt; (Open-source RAG engine)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/elearningshow/ollama-kis&quot;&gt;Ollama-Kis&lt;/a&gt; (A simple easy-to-use GUI with sample custom LLM for Drivers Education)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://opengpa.org&quot;&gt;OpenGPA&lt;/a&gt; (Open-source offline-first Enterprise Agentic Application)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mateuszmigas/painting-droid&quot;&gt;Painting Droid&lt;/a&gt; (Painting app with AI integrations)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.kerlig.com/&quot;&gt;Kerlig AI&lt;/a&gt; (AI writing assistant for macOS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/MindWorkAI/AI-Studio&quot;&gt;AI Studio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gyopak/sidellama&quot;&gt;Sidellama&lt;/a&gt; (browser-based LLM client)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/trypromptly/LLMStack&quot;&gt;LLMStack&lt;/a&gt; (No-code multi-agent framework to build LLM agents and workflows)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://boltai.com&quot;&gt;BoltAI for Mac&lt;/a&gt; (AI Chat Client for Mac)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/av/harbor&quot;&gt;Harbor&lt;/a&gt; (Containerized LLM Toolkit with Ollama as default backend)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/szczyglis-dev/py-gpt&quot;&gt;PyGPT&lt;/a&gt; (AI desktop assistant for Linux, Windows, and Mac)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Jeffser/Alpaca&quot;&gt;Alpaca&lt;/a&gt; (An Ollama client application for Linux and macOS made with GTK4 and Adwaita)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Significant-Gravitas/AutoGPT/raw/master/docs/content/platform/ollama.md&quot;&gt;AutoGPT&lt;/a&gt; (AutoGPT Ollama integration)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.jonathanhecl.com/go-crew/&quot;&gt;Go-CREW&lt;/a&gt; (Powerful Offline RAG in Golang)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/openvmp/partcad/&quot;&gt;PartCAD&lt;/a&gt; (CAD model generation with OpenSCAD and CadQuery)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ollama4j/ollama4j-web-ui&quot;&gt;Ollama4j Web UI&lt;/a&gt; - Java-based Web UI for Ollama built with Vaadin, Spring Boot, and Ollama4j&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kspviswa/pyOllaMx&quot;&gt;PyOllaMx&lt;/a&gt; - macOS application capable of chatting with both Ollama and Apple MLX models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cline/cline&quot;&gt;Cline&lt;/a&gt; - Formerly known as Claude Dev is a VSCode extension for multi-file/whole-repo coding&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kangfenmao/cherry-studio&quot;&gt;Cherry Studio&lt;/a&gt; (Desktop client with Ollama support)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/1runeberg/confichat&quot;&gt;ConfiChat&lt;/a&gt; (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nickthecook/archyve&quot;&gt;Archyve&lt;/a&gt; (RAG-enabling document library)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rapidarchitect/ollama-crew-mesop&quot;&gt;crewAI with Mesop&lt;/a&gt; (Mesop Web Interface to run crewAI with Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/chyok/ollama-gui&quot;&gt;Tkinter-based client&lt;/a&gt; (Python tkinter-based Client for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/trendy-design/llmchat&quot;&gt;LLMChat&lt;/a&gt; (Privacy focused, 100% local, intuitive all-in-one chat interface)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Leon-Sander/Local-Multimodal-AI-Chat&quot;&gt;Local Multimodal AI Chat&lt;/a&gt; (Ollama-based LLM Chat with support for multiple features, including PDF RAG, voice chat, image-based interactions, and integration with OpenAI.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xark-argo/argo&quot;&gt;ARGO&lt;/a&gt; (Locally download and run Ollama and Huggingface models with RAG and deep research on Mac/Windows/Linux)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/EliasPereirah/OrionChat&quot;&gt;OrionChat&lt;/a&gt; - OrionChat is a web interface for chatting with different AI providers&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bklieger-groq/g1&quot;&gt;G1&lt;/a&gt; (Prototype of using prompting strategies to improve the LLM&#39;s reasoning through o1-like reasoning chains.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lemonit-eric-mao/ollama-web-management&quot;&gt;Web management&lt;/a&gt; (Web management page)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/promptery/promptery&quot;&gt;Promptery&lt;/a&gt; (desktop client for Ollama.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JHubi1/ollama-app&quot;&gt;Ollama App&lt;/a&gt; (Modern and easy-to-use multi-platform client for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/annilq/chat-ollama&quot;&gt;chat-ollama&lt;/a&gt; (a React Native client for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tcsenpai/spacellama&quot;&gt;SpaceLlama&lt;/a&gt; (Firefox and Chrome extension to quickly summarize web pages with ollama in a sidebar)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tcsenpai/youlama&quot;&gt;YouLama&lt;/a&gt; (Webapp to quickly summarize any YouTube video, supporting Invidious as well)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tcsenpai/dualmind&quot;&gt;DualMind&lt;/a&gt; (Experimental app allowing two models to talk to each other in the terminal or in a web interface)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/h1ddenpr0cess20/ollamarama-matrix&quot;&gt;ollamarama-matrix&lt;/a&gt; (Ollama chatbot for the Matrix chat protocol)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anan1213095357/ollama-chat-app&quot;&gt;ollama-chat-app&lt;/a&gt; (Flutter-based chat app)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.perfectmemory.ai/&quot;&gt;Perfect Memory AI&lt;/a&gt; (Productivity AI assists personalized by what you have seen on your screen, heard, and said in the meetings)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hexastack/hexabot&quot;&gt;Hexabot&lt;/a&gt; (A conversational AI builder)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rapidarchitect/reddit_analyzer&quot;&gt;Reddit Rate&lt;/a&gt; (Search and Rate Reddit topics with a weighted summation)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/adarshM84/OpenTalkGpt&quot;&gt;OpenTalkGpt&lt;/a&gt; (Chrome Extension to manage open-source models supported by Ollama, create custom models, and chat with models from a user-friendly UI)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/vinhnx/vt.ai&quot;&gt;VT&lt;/a&gt; (A minimal multimodal AI chat app, with dynamic conversation routing. Supports local models via Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nosia-ai/nosia&quot;&gt;Nosia&lt;/a&gt; (Easy to install and use RAG platform based on Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nbonamy/witsy&quot;&gt;Witsy&lt;/a&gt; (An AI Desktop application available for Mac/Windows/Linux)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/US-Artificial-Intelligence/abbey&quot;&gt;Abbey&lt;/a&gt; (A configurable AI interface server with notebooks, document storage, and YouTube support)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dmayboroda/minima&quot;&gt;Minima&lt;/a&gt; (RAG with on-premises or fully local workflow)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AidfulAI/aidful-ollama-model-delete&quot;&gt;aidful-ollama-model-delete&lt;/a&gt; (User interface for simplified model cleanup)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ItzCrazyKns/Perplexica&quot;&gt;Perplexica&lt;/a&gt; (An AI-powered search engine &amp;amp; an open-source alternative to Perplexity AI)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/oslook/ollama-webui&quot;&gt;Ollama Chat WebUI for Docker &lt;/a&gt; (Support for local docker deployment, lightweight ollama webui)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-tooklit/ollama-docs&quot;&gt;AI Toolkit for Visual Studio Code&lt;/a&gt; (Microsoft-official VSCode extension to chat, test, evaluate models with Ollama support, and use them in your AI applications.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anilkay/MinimalNextOllamaChat&quot;&gt;MinimalNextOllamaChat&lt;/a&gt; (Minimal Web UI for Chat and Model Control)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/TilmanGriesel/chipper&quot;&gt;Chipper&lt;/a&gt; AI interface for tinkerers (Ollama, Haystack RAG, Python)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/CosmicEventHorizon/ChibiChat&quot;&gt;ChibiChat&lt;/a&gt; (Kotlin-based Android app to chat with Ollama and Koboldcpp API endpoints)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/qusaismael/localllm&quot;&gt;LocalLLM&lt;/a&gt; (Minimal Web-App to run ollama models on it with a GUI)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/buiducnhat/ollamazing&quot;&gt;Ollamazing&lt;/a&gt; (Web extension to run Ollama models)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/benhaotang/OpenDeepResearcher-via-searxng&quot;&gt;OpenDeepResearcher-via-searxng&lt;/a&gt; (A Deep Research equivalent endpoint with Ollama support for running locally)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AIDotNet/AntSK&quot;&gt;AntSK&lt;/a&gt; (Out-of-the-box &amp;amp; Adaptable RAG Chatbot)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/1Panel-dev/MaxKB/&quot;&gt;MaxKB&lt;/a&gt; (Ready-to-use &amp;amp; flexible RAG Chatbot)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/danielekp/yla&quot;&gt;yla&lt;/a&gt; (Web interface to freely interact with your customized models)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/RockChinQ/LangBot&quot;&gt;LangBot&lt;/a&gt; (LLM-based instant messaging bots platform, with Agents, RAG features, supports multiple platforms)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/1Panel-dev/1Panel/&quot;&gt;1Panel&lt;/a&gt; (Web-based Linux Server Management Tool)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Soulter/AstrBot/&quot;&gt;AstrBot&lt;/a&gt; (User-friendly LLM-based multi-platform chatbot with a WebUI, supporting RAG, LLM agents, and plugins integration)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ibrahimcetin/reins&quot;&gt;Reins&lt;/a&gt; (Easily tweak parameters, customize system prompts per chat, and enhance your AI experiments with reasoning model support.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Aharon-Bensadoun/Flufy&quot;&gt;Flufy&lt;/a&gt; (A beautiful chat interface for interacting with Ollama&#39;s API. Built with React, TypeScript, and Material-UI.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zeozeozeo/ellama&quot;&gt;Ellama&lt;/a&gt; (Friendly native app to chat with an Ollama instance)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mediar-ai/screenpipe&quot;&gt;screenpipe&lt;/a&gt; Build agents powered by your screen history&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hengkysteen/ollamb&quot;&gt;Ollamb&lt;/a&gt; (Simple yet rich in features, cross-platform built with Flutter and designed for Ollama. Try the &lt;a href=&quot;https://hengkysteen.github.io/demo/ollamb/&quot;&gt;web demo&lt;/a&gt;.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Writeopia/Writeopia&quot;&gt;Writeopia&lt;/a&gt; (Text editor with integration with Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AppFlowy-IO/AppFlowy&quot;&gt;AppFlowy&lt;/a&gt; (AI collaborative workspace with Ollama, cross-platform and self-hostable)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cushydigit/lumina.git&quot;&gt;Lumina&lt;/a&gt; (A lightweight, minimal React.js frontend for interacting with Ollama servers)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pypi.org/project/tiny-notepad&quot;&gt;Tiny Notepad&lt;/a&gt; (A lightweight, notepad-like interface to chat with ollama available on PyPI)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hellotunamayo/macLlama&quot;&gt;macLlama (macOS native)&lt;/a&gt; (A native macOS GUI application for interacting with Ollama models, featuring a chat interface.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/philberndt/GPTranslate&quot;&gt;GPTranslate&lt;/a&gt; (A fast and lightweight, AI powered desktop translation application written with Rust and Tauri. Features real-time translation with OpenAI/Azure/Ollama.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/NGC13009/ollama-launcher&quot;&gt;ollama launcher&lt;/a&gt; (A launcher for Ollama, aiming to provide users with convenient functions such as ollama server launching, management, or configuration.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Aj-Seven/ai-hub&quot;&gt;ai-hub&lt;/a&gt; (AI Hub supports multiple models via API keys and Chat support via Ollama API.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gitlab.com/mayan-edms/mayan-edms&quot;&gt;Mayan EDMS&lt;/a&gt; (Open source document management system to organize, tag, search, and automate your files with powerful Ollama driven workflows.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Cloud&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/run/docs/tutorials/gpu-gemma2-with-ollama&quot;&gt;Google Cloud&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://fly.io/docs/python/do-more/add-ollama/&quot;&gt;Fly.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.koyeb.com/deploy/ollama&quot;&gt;Koyeb&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Terminal&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ggozad/oterm&quot;&gt;oterm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/s-kostyaev/ellama&quot;&gt;Ellama Emacs client&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zweifisch/ollama&quot;&gt;Emacs client&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/paradoxical-dev/neollama&quot;&gt;neollama&lt;/a&gt; UI client for interacting with models from within Neovim&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/David-Kunz/gen.nvim&quot;&gt;gen.nvim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nomnivore/ollama.nvim&quot;&gt;ollama.nvim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/marco-souza/ollero.nvim&quot;&gt;ollero.nvim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gerazov/ollama-chat.nvim&quot;&gt;ollama-chat.nvim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/huynle/ogpt.nvim&quot;&gt;ogpt.nvim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/karthink/gptel&quot;&gt;gptel Emacs client&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dustinblackman/oatmeal&quot;&gt;Oatmeal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pgibler/cmdh&quot;&gt;cmdh&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/npahlfer/ooo&quot;&gt;ooo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/reid41/shell-pilot&quot;&gt;shell-pilot&lt;/a&gt;(Interact with models via pure shell scripts on Linux or macOS)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pythops/tenere&quot;&gt;tenere&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/taketwo/llm-ollama&quot;&gt;llm-ollama&lt;/a&gt; for &lt;a href=&quot;https://llm.datasette.io/en/stable/&quot;&gt;Datasette&#39;s LLM CLI&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anaisbetts/typechat-cli&quot;&gt;typechat-cli&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/djcopley/ShellOracle&quot;&gt;ShellOracle&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yusufcanb/tlm&quot;&gt;tlm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ericcurtin/podman-ollama&quot;&gt;podman-ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sammcj/gollama&quot;&gt;gollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/paulrobello/parllama&quot;&gt;ParLlama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cognitivetech/ollama-ebook-summary/&quot;&gt;Ollama eBook Summary&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rapidarchitect/ollama_moe&quot;&gt;Ollama Mixture of Experts (MOE) in 50 lines of code&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pepo-ec/vim-intelligence-bridge&quot;&gt;vim-intelligence-bridge&lt;/a&gt; Simple interaction of &quot;Ollama&quot; with the Vim editor&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://x-cmd.com/mod/ollama&quot;&gt;x-cmd ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/drunkwcodes/bb7&quot;&gt;bb7&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/marcusziade/Swollama&quot;&gt;SwollamaCLI&lt;/a&gt; bundled with the Swollama Swift package. &lt;a href=&quot;https://github.com/marcusziade/Swollama?tab=readme-ov-file#cli-usage&quot;&gt;Demo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sigoden/aichat&quot;&gt;aichat&lt;/a&gt; All-in-one LLM CLI tool featuring Shell Assistant, Chat-REPL, RAG, AI tools &amp;amp; agents, with access to OpenAI, Claude, Gemini, Ollama, Groq, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rrg92/powershai&quot;&gt;PowershAI&lt;/a&gt; PowerShell module that brings AI to terminal on Windows, including support for Ollama&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Abyss-c0re/deepshell&quot;&gt;DeepShell&lt;/a&gt; Your self-hosted AI assistant. Interactive Shell, Files and Folders analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xyproto/orbiton&quot;&gt;orbiton&lt;/a&gt; Configuration-free text editor and IDE with support for tab completion with Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/molbal/orca-cli&quot;&gt;orca-cli&lt;/a&gt; Ollama Registry CLI Application - Browse, pull, and download models from Ollama Registry in your terminal.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jonathanhecl/gguf-to-ollama&quot;&gt;GGUF-to-Ollama&lt;/a&gt; - Importing GGUF to Ollama made easy (multiplatform)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rapidarchitect/ollama_strands&quot;&gt;AWS-Strands-With-Ollama&lt;/a&gt; - AWS Strands Agents with Ollama Examples&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/attogram/ollama-multirun&quot;&gt;ollama-multirun&lt;/a&gt; - A bash shell script to run a single prompt against any or all of your locally installed ollama models, saving the output and performance statistics as easily navigable web pages. (&lt;a href=&quot;https://attogram.github.io/ai_test_zone/&quot;&gt;Demo&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/attogram/ollama-bash-toolshed&quot;&gt;ollama-bash-toolshed&lt;/a&gt; - Bash scripts to chat with tool using models. Add new tools to your shed with ease. Runs on Ollama.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Apple Vision Pro&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/aws-samples/swift-chat&quot;&gt;SwiftChat&lt;/a&gt; (Cross-platform AI chat app supporting Apple Vision Pro via &quot;Designed for iPad&quot;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AugustDev/enchanted&quot;&gt;Enchanted&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Database&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timescale/pgai&quot;&gt;pgai&lt;/a&gt; - PostgreSQL as a vector database (Create and search embeddings from Ollama models using pgvector) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/timescale/pgai/raw/main/docs/vectorizer-quick-start.md&quot;&gt;Get started guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mindsdb/mindsdb/raw/staging/mindsdb/integrations/handlers/ollama_handler/README.md&quot;&gt;MindsDB&lt;/a&gt; (Connects Ollama models with nearly 200 data platforms and apps)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/philippgille/chromem-go/raw/v0.5.0/embed_ollama.go&quot;&gt;chromem-go&lt;/a&gt; with &lt;a href=&quot;https://github.com/philippgille/chromem-go/tree/v0.5.0/examples/rag-wikipedia-ollama&quot;&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dbkangaroo/kangaroo&quot;&gt;Kangaroo&lt;/a&gt; (AI-powered SQL client and admin tool for popular databases)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Package managers&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://archlinux.org/packages/extra/x86_64/ollama/&quot;&gt;Pacman&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gentoo/guru/tree/master/app-misc/ollama&quot;&gt;Gentoo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://formulae.brew.sh/formula/ollama&quot;&gt;Homebrew&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://artifacthub.io/packages/helm/ollama-helm/ollama&quot;&gt;Helm Chart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://codeberg.org/tusharhero/ollama-guix&quot;&gt;Guix channel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://search.nixos.org/packages?show=ollama&amp;amp;from=0&amp;amp;size=50&amp;amp;sort=relevance&amp;amp;type=packages&amp;amp;query=ollama&quot;&gt;Nix package&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://flox.dev/blog/ollama-part-one&quot;&gt;Flox&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Libraries&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/integrations/chat/ollama/&quot;&gt;LangChain&lt;/a&gt; and &lt;a href=&quot;https://js.langchain.com/docs/integrations/chat/ollama/&quot;&gt;LangChain.js&lt;/a&gt; with &lt;a href=&quot;https://js.langchain.com/docs/tutorials/local_rag/&quot;&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://firebase.google.com/docs/genkit/plugins/ollama&quot;&gt;Firebase Genkit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;crewAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://remembersoftwares.github.io/yacana/&quot;&gt;Yacana&lt;/a&gt; (User-friendly multi-agent framework for brainstorming and executing predetermined flows with built-in tool integration)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/spring-projects/spring-ai&quot;&gt;Spring AI&lt;/a&gt; with &lt;a href=&quot;https://docs.spring.io/spring-ai/reference/api/chat/ollama-chat.html&quot;&gt;reference&lt;/a&gt; and &lt;a href=&quot;https://github.com/tzolov/ollama-tools&quot;&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tmc/langchaingo/&quot;&gt;LangChainGo&lt;/a&gt; with &lt;a href=&quot;https://github.com/tmc/langchaingo/tree/main/examples/ollama-completion-example&quot;&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langchain4j/langchain4j&quot;&gt;LangChain4j&lt;/a&gt; with &lt;a href=&quot;https://github.com/langchain4j/langchain4j-examples/tree/main/ollama-examples/src/main/java&quot;&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Abraxas-365/langchain-rust&quot;&gt;LangChainRust&lt;/a&gt; with &lt;a href=&quot;https://github.com/Abraxas-365/langchain-rust/raw/main/examples/llm_ollama.rs&quot;&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tryAGI/LangChain&quot;&gt;LangChain for .NET&lt;/a&gt; with &lt;a href=&quot;https://github.com/tryAGI/LangChain/raw/main/examples/LangChain.Samples.OpenAI/Program.cs&quot;&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/theodo-group/LLPhant?tab=readme-ov-file#ollama&quot;&gt;LLPhant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.llamaindex.ai/en/stable/examples/llm/ollama/&quot;&gt;LlamaIndex&lt;/a&gt; and &lt;a href=&quot;https://ts.llamaindex.ai/modules/llms/available_llms/ollama&quot;&gt;LlamaIndexTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/BerriAI/litellm&quot;&gt;LiteLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/presbrey/ollamafarm&quot;&gt;OllamaFarm for Go&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awaescher/OllamaSharp&quot;&gt;OllamaSharp for .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gbaptista/ollama-ai&quot;&gt;Ollama for Ruby&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pepperoni21/ollama-rs&quot;&gt;Ollama-rs for Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jmont-dev/ollama-hpp&quot;&gt;Ollama-hpp for C++&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ollama4j/ollama4j&quot;&gt;Ollama4j for Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://modelfusion.dev/integration/model-provider/ollama&quot;&gt;ModelFusion Typescript Library&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kevinhermawan/OllamaKit&quot;&gt;OllamaKit for Swift&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/breitburg/dart-ollama&quot;&gt;Ollama for Dart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cloudstudio/ollama-laravel&quot;&gt;Ollama for Laravel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/davidmigloz/langchain_dart&quot;&gt;LangChainDart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/semantic-kernel/tree/main/python/semantic_kernel/connectors/ai/ollama&quot;&gt;Semantic Kernel - Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/deepset-ai/haystack-integrations/raw/main/integrations/ollama.md&quot;&gt;Haystack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/brainlid/langchain&quot;&gt;Elixir LangChain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JBGruber/rollama&quot;&gt;Ollama for R - rollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hauselin/ollama-r&quot;&gt;Ollama for R - ollama-r&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lebrunel/ollama-ex&quot;&gt;Ollama-ex for Elixir&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/b-tocs/abap_btocs_ollama&quot;&gt;Ollama Connector for SAP ABAP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://testcontainers.com/modules/ollama/&quot;&gt;Testcontainers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://portkey.ai/docs/welcome/integration-guides/ollama&quot;&gt;Portkey&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/svilupp/PromptingTools.jl&quot;&gt;PromptingTools.jl&lt;/a&gt; with an &lt;a href=&quot;https://svilupp.github.io/PromptingTools.jl/dev/examples/working_with_ollama&quot;&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Project-Llama/llamascript&quot;&gt;LlamaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/emirsahin1/llm-axe&quot;&gt;llm-axe&lt;/a&gt; (Python Toolkit for Building LLM Powered Apps)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.gollm.co/examples/ollama-example&quot;&gt;Gollm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jonathanhecl/gollama&quot;&gt;Gollama for Golang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xyproto/ollamaclient&quot;&gt;Ollamaclient for Golang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gitlab.com/tozd/go/fun&quot;&gt;High-level function abstraction in Go&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ArdaGnsrn/ollama-php&quot;&gt;Ollama PHP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/agents-flex/agents-flex&quot;&gt;Agents-Flex for Java&lt;/a&gt; with &lt;a href=&quot;https://github.com/agents-flex/agents-flex/tree/main/agents-flex-llm/agents-flex-llm-ollama/src/test/java/com/agentsflex/llm/ollama&quot;&gt;example&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/parakeet-nest/parakeet&quot;&gt;Parakeet&lt;/a&gt; is a GoLang library, made to simplify the development of small generative AI applications with Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/andygill/haverscript&quot;&gt;Haverscript&lt;/a&gt; with &lt;a href=&quot;https://github.com/andygill/haverscript/tree/main/examples&quot;&gt;examples&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mattt/ollama-swift&quot;&gt;Ollama for Swift&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/marcusziade/Swollama&quot;&gt;Swollama for Swift&lt;/a&gt; with &lt;a href=&quot;https://marcusziade.github.io/Swollama/documentation/swollama/&quot;&gt;DocC&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/prasad89/golamify&quot;&gt;GoLamify&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tusharad/ollama-haskell&quot;&gt;Ollama for Haskell&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nbonamy/multi-llm-ts&quot;&gt;multi-llm-ts&lt;/a&gt; (A Typescript/JavaScript library allowing access to different LLM in a unified API)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lofcz/llmtornado&quot;&gt;LlmTornado&lt;/a&gt; (C# library providing a unified interface for major FOSS &amp;amp; Commercial inference APIs)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dravenk/ollama-zig&quot;&gt;Ollama for Zig&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lunary-ai/abso&quot;&gt;Abso&lt;/a&gt; (OpenAI-compatible TypeScript SDK for any LLM provider)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/goodreasonai/nichey&quot;&gt;Nichey&lt;/a&gt; is a Python package for generating custom wikis for your research topic&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kassane/ollama-d&quot;&gt;Ollama for D&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/HardCodeDev777/OllamaPlusPlus&quot;&gt;OllamaPlusPlus&lt;/a&gt; (Very simple C++ library for Ollama)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Mobile&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/aws-samples/swift-chat&quot;&gt;SwiftChat&lt;/a&gt; (Lightning-fast Cross-platform AI chat app with native UI for Android, iOS, and iPad)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AugustDev/enchanted&quot;&gt;Enchanted&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Mobile-Artificial-Intelligence/maid&quot;&gt;Maid&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JHubi1/ollama-app&quot;&gt;Ollama App&lt;/a&gt; (Modern and easy-to-use multi-platform client for Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/1runeberg/confichat&quot;&gt;ConfiChat&lt;/a&gt; (Lightweight, standalone, multi-platform, and privacy-focused LLM chat interface with optional encryption)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sunshine0523/OllamaServer&quot;&gt;Ollama Android Chat&lt;/a&gt; (No need for Termux, start the Ollama service with one click on an Android device)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ibrahimcetin/reins&quot;&gt;Reins&lt;/a&gt; (Easily tweak parameters, customize system prompts per chat, and enhance your AI experiments with reasoning model support.)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extensions &amp;amp; Plugins&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/MassimilianoPasquini97/raycast_ollama&quot;&gt;Raycast extension&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mxyng/discollama&quot;&gt;Discollama&lt;/a&gt; (Discord bot inside the Ollama discord channel)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/continuedev/continue&quot;&gt;Continue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/thewh1teagle/vibe&quot;&gt;Vibe&lt;/a&gt; (Transcribe and analyze meetings with Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hinterdupfinger/obsidian-ollama&quot;&gt;Obsidian Ollama plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/omagdy7/ollama-logseq&quot;&gt;Logseq Ollama plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/andersrex/notesollama&quot;&gt;NotesOllama&lt;/a&gt; (Apple Notes Ollama plugin)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/samalba/dagger-chatbot&quot;&gt;Dagger Chatbot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mekb-turtle/discord-ai-bot&quot;&gt;Discord AI Bot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ruecat/ollama-telegram&quot;&gt;Ollama Telegram Bot&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ej52/hass-ollama-conversation&quot;&gt;Hass Ollama Conversation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/abrenneke/rivet-plugin-ollama&quot;&gt;Rivet plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/longy2k/obsidian-bmo-chatbot&quot;&gt;Obsidian BMO Chatbot plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/herval/cliobot&quot;&gt;Cliobot&lt;/a&gt; (Telegram bot with Ollama support)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/logancyang/obsidian-copilot&quot;&gt;Copilot for Obsidian plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pfrankov/obsidian-local-gpt&quot;&gt;Obsidian Local GPT plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.openinterpreter.com/language-model-setup/local-models/ollama&quot;&gt;Open Interpreter&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ex3ndr/llama-coder&quot;&gt;Llama Coder&lt;/a&gt; (Copilot alternative using Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bernardo-bruning/ollama-copilot&quot;&gt;Ollama Copilot&lt;/a&gt; (Proxy that allows you to use Ollama as a copilot like GitHub Copilot)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rjmacarthy/twinny&quot;&gt;twinny&lt;/a&gt; (Copilot and Copilot chat alternative using Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/RussellCanfield/wingman-ai&quot;&gt;Wingman-AI&lt;/a&gt; (Copilot code and chat alternative using Ollama and Hugging Face)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/n4ze3m/page-assist&quot;&gt;Page Assist&lt;/a&gt; (Chrome Extension)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/imoize/plasmoid-ollamacontrol&quot;&gt;Plasmoid Ollama Control&lt;/a&gt; (KDE Plasma extension that allows you to quickly manage/control Ollama model)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tusharhero/aitelegrambot&quot;&gt;AI Telegram Bot&lt;/a&gt; (Telegram bot using Ollama in backend)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yaroslavyaroslav/OpenAI-sublime-text&quot;&gt;AI ST Completion&lt;/a&gt; (Sublime Text 4 AI assistant plugin with Ollama support)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kevinthedang/discord-ollama&quot;&gt;Discord-Ollama Chat Bot&lt;/a&gt; (Generalized TypeScript Discord Bot w/ Tuning Documentation)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/josStorer/chatGPTBox&quot;&gt;ChatGPTBox: All in one browser extension&lt;/a&gt; with &lt;a href=&quot;https://github.com/josStorer/chatGPTBox/issues/616#issuecomment-1975186467&quot;&gt;Integrating Tutorial&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rapmd73/Companion&quot;&gt;Discord AI chat/moderation bot&lt;/a&gt; Chat/moderation bot written in python. Uses Ollama to create personalities.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nischalj10/headless-ollama&quot;&gt;Headless Ollama&lt;/a&gt; (Scripts to automatically install ollama client &amp;amp; models on any OS for apps that depend on ollama server)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xuyangbocn/terraform-aws-self-host-llm&quot;&gt;Terraform AWS Ollama &amp;amp; Open WebUI&lt;/a&gt; (A Terraform module to deploy on AWS a ready-to-use Ollama service, together with its front-end Open WebUI service.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jakubburkiewicz/node-red-contrib-ollama&quot;&gt;node-red-contrib-ollama&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ivostoykov/localAI&quot;&gt;Local AI Helper&lt;/a&gt; (Chrome and Firefox extensions that enable interactions with the active tab and customisable API endpoints. Includes secure storage for user prompts.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jake83741/vnc-lm&quot;&gt;vnc-lm&lt;/a&gt; (Discord bot for messaging with LLMs through Ollama and LiteLLM. Seamlessly move between local and flagship models.)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SilasMarvin/lsp-ai&quot;&gt;LSP-AI&lt;/a&gt; (Open-source language server for AI-powered functionality)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Palm1r/QodeAssist&quot;&gt;QodeAssist&lt;/a&gt; (AI-powered coding assistant plugin for Qt Creator)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ECuiDev/obsidian-quiz-generator&quot;&gt;Obsidian Quiz Generator plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/philffm/ai-summary-helper&quot;&gt;AI Summmary Helper plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/suncloudsmoon/TextCraft&quot;&gt;TextCraft&lt;/a&gt; (Copilot in Word alternative using Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zeitlings/alfred-ollama&quot;&gt;Alfred Ollama&lt;/a&gt; (Alfred Workflow)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/adarshM84/TextLLaMA&quot;&gt;TextLLaMA&lt;/a&gt; A Chrome Extension that helps you write emails, correct grammar, and translate into any language&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zyphixor/simple-discord-ai&quot;&gt;Simple-Discord-AI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/innightwolfsleep/llm_telegram_bot&quot;&gt;LLM Telegram Bot&lt;/a&gt; (telegram bot, primary for RP. Oobabooga-like buttons, &lt;a href=&quot;https://github.com/AUTOMATIC1111/stable-diffusion-webui&quot;&gt;A1111&lt;/a&gt; API integration e.t.c)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sammcj/mcp-llm&quot;&gt;mcp-llm&lt;/a&gt; (MCP Server to allow LLMs to call other LLMs)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/HardCodeDev777/SimpleOllamaUnity&quot;&gt;SimpleOllamaUnity&lt;/a&gt; (Unity Engine extension for communicating with Ollama in a few lines of code. Also works at runtime)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/HardCodeDev777/UnityCodeLama&quot;&gt;UnityCodeLama&lt;/a&gt; (Unity Edtior tool to analyze scripts via Ollama)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/NativeMindBrowser/NativeMindExtension&quot;&gt;NativeMind&lt;/a&gt; (Private, on-device AI Assistant, no cloud dependencies)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gmai.premex.se/&quot;&gt;GMAI - Gradle Managed AI&lt;/a&gt; (Gradle plugin for automated Ollama lifecycle management during build phases)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Supported backends&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp&quot;&gt;llama.cpp&lt;/a&gt; project founded by Georgi Gerganov.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Observability&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.comet.com/docs/opik/cookbook/ollama&quot;&gt;Opik&lt;/a&gt; is an open-source platform to debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards. Opik supports native intergration to Ollama.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://lunary.ai/docs/integrations/ollama&quot;&gt;Lunary&lt;/a&gt; is the leading open-source LLM observability platform. It provides a variety of enterprise-grade features such as real-time analytics, prompt templates management, PII masking, and comprehensive agent tracing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/openlit/openlit&quot;&gt;OpenLIT&lt;/a&gt; is an OpenTelemetry-native tool for monitoring Ollama Applications &amp;amp; GPUs using traces and metrics.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.honeyhive.ai/integrations/ollama&quot;&gt;HoneyHive&lt;/a&gt; is an AI observability and evaluation platform for AI agents. Use HoneyHive to evaluate agent performance, interrogate failures, and monitor quality in production.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langfuse.com/docs/integrations/ollama&quot;&gt;Langfuse&lt;/a&gt; is an open source LLM observability platform that enables teams to collaboratively monitor, evaluate and debug AI applications.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mlflow.org/docs/latest/llms/tracing/index.html#automatic-tracing&quot;&gt;MLflow Tracing&lt;/a&gt; is an open source LLM observability tool with a convenient API to log and visualize traces, making it easy to debug and evaluate GenAI applications.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-go</title>
      <link>https://github.com/openai/openai-go</link>
      <description>&lt;p&gt;The official Go library for the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Go API Library&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://pkg.go.dev/github.com/openai/openai-go/v2&quot;&gt;&lt;img src=&quot;https://pkg.go.dev/badge/github.com/openai/openai-go.svg?sanitize=true&quot; alt=&quot;Go Reference&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The OpenAI Go library provides convenient access to the &lt;a href=&quot;https://platform.openai.com/docs&quot;&gt;OpenAI REST API&lt;/a&gt; from applications written in Go.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING] The latest version of this package has small and limited breaking changes. See the &lt;a href=&quot;https://raw.githubusercontent.com/openai/openai-go/main/CHANGELOG.md&quot;&gt;changelog&lt;/a&gt; for details.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;!-- x-release-please-start-version --&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;import (
	&quot;github.com/openai/openai-go/v2&quot; // imported as openai
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- x-release-please-end --&gt; 
&lt;p&gt;Or to pin the version:&lt;/p&gt; 
&lt;!-- x-release-please-start-version --&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;go get -u &#39;github.com/openai/openai-go@v2.0.2&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- x-release-please-end --&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;This library requires Go 1.21+.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;The full API of this library can be found in &lt;a href=&quot;https://raw.githubusercontent.com/openai/openai-go/main/api.md&quot;&gt;api.md&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
	&quot;context&quot;
	&quot;fmt&quot;

	&quot;github.com/openai/openai-go/v2&quot;
	&quot;github.com/openai/openai-go/v2/option&quot;
	&quot;github.com/openai/openai-go/v2/shared&quot;
)

func main() {
	client := openai.NewClient(
		option.WithAPIKey(&quot;My API Key&quot;), // defaults to os.LookupEnv(&quot;OPENAI_API_KEY&quot;)
	)
	chatCompletion, err := client.Chat.Completions.New(context.TODO(), openai.ChatCompletionNewParams{
		Messages: []openai.ChatCompletionMessageParamUnion{
			openai.UserMessage(&quot;Say this is a test&quot;),
		},
		Model: openai.ChatModelGPT4o,
	})
	if err != nil {
		panic(err.Error())
	}
	println(chatCompletion.Choices[0].Message.Content)
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;Conversations&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;param := openai.ChatCompletionNewParams{
	Messages: []openai.ChatCompletionMessageParamUnion{
		openai.UserMessage(&quot;What kind of houseplant is easy to take care of?&quot;),
	},
	Seed:     openai.Int(1),
	Model:    openai.ChatModelGPT4o,
}

completion, err := client.Chat.Completions.New(ctx, param)

param.Messages = append(param.Messages, completion.Choices[0].Message.ToParam())
param.Messages = append(param.Messages, openai.UserMessage(&quot;How big are those?&quot;))

// continue the conversation
completion, err = client.Chat.Completions.New(ctx, param)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Streaming responses&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;question := &quot;Write an epic&quot;

stream := client.Chat.Completions.NewStreaming(ctx, openai.ChatCompletionNewParams{
	Messages: []openai.ChatCompletionMessageParamUnion{
		openai.UserMessage(question),
	},
	Seed:  openai.Int(0),
	Model: openai.ChatModelGPT4o,
})

// optionally, an accumulator helper can be used
acc := openai.ChatCompletionAccumulator{}

for stream.Next() {
	chunk := stream.Current()
	acc.AddChunk(chunk)

	if content, ok := acc.JustFinishedContent(); ok {
		println(&quot;Content stream finished:&quot;, content)
	}

	// if using tool calls
	if tool, ok := acc.JustFinishedToolCall(); ok {
		println(&quot;Tool call stream finished:&quot;, tool.Index, tool.Name, tool.Arguments)
	}

	if refusal, ok := acc.JustFinishedRefusal(); ok {
		println(&quot;Refusal stream finished:&quot;, refusal)
	}

	// it&#39;s best to use chunks after handling JustFinished events
	if len(chunk.Choices) &amp;gt; 0 {
		println(chunk.Choices[0].Delta.Content)
	}
}

if stream.Err() != nil {
	panic(stream.Err())
}

// After the stream is finished, acc can be used like a ChatCompletion
_ = acc.Choices[0].Message.Content
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;See the &lt;a href=&quot;https://raw.githubusercontent.com/openai/openai-go/main/examples/chat-completion-accumulating/main.go&quot;&gt;full streaming and accumulation example&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Tool calling&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;import (
	&quot;encoding/json&quot;
	// ...
)

// ...

question := &quot;What is the weather in New York City?&quot;

params := openai.ChatCompletionNewParams{
	Messages: []openai.ChatCompletionMessageParamUnion{
		openai.UserMessage(question),
	},
	Tools: []openai.ChatCompletionToolParam{
		{
			Function: openai.FunctionDefinitionParam{
				Name:        &quot;get_weather&quot;,
				Description: openai.String(&quot;Get weather at the given location&quot;),
				Parameters: openai.FunctionParameters{
					&quot;type&quot;: &quot;object&quot;,
					&quot;properties&quot;: map[string]interface{}{
						&quot;location&quot;: map[string]string{
							&quot;type&quot;: &quot;string&quot;,
						},
					},
					&quot;required&quot;: []string{&quot;location&quot;},
				},
			},
		},
	},
	Model: openai.ChatModelGPT4o,
}

// If there is a was a function call, continue the conversation
params.Messages = append(params.Messages, completion.Choices[0].Message.ToParam())
for _, toolCall := range toolCalls {
	if toolCall.Function.Name == &quot;get_weather&quot; {
		// Extract the location from the function call arguments
		var args map[string]interface{}
		err := json.Unmarshal([]byte(toolCall.Function.Arguments), &amp;amp;args)
		if err != nil {
			panic(err)
		}
		location := args[&quot;location&quot;].(string)

		// Simulate getting weather data
		weatherData := getWeather(location)

		// Print the weather data
		fmt.Printf(&quot;Weather in %s: %s\n&quot;, location, weatherData)

		params.Messages = append(params.Messages, openai.ToolMessage(weatherData, toolCall.ID))
	}
}

// ... continue the conversation with the information provided by the tool
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;See the &lt;a href=&quot;https://raw.githubusercontent.com/openai/openai-go/main/examples/chat-completion-tool-calling/main.go&quot;&gt;full tool calling example&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Structured outputs&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;import (
	&quot;encoding/json&quot;
	&quot;github.com/invopop/jsonschema&quot;
	// ...
)

// A struct that will be converted to a Structured Outputs response schema
type HistoricalComputer struct {
	Origin       Origin   `json:&quot;origin&quot; jsonschema_description:&quot;The origin of the computer&quot;`
	Name         string   `json:&quot;full_name&quot; jsonschema_description:&quot;The name of the device model&quot;`
	Legacy       string   `json:&quot;legacy&quot; jsonschema:&quot;enum=positive,enum=neutral,enum=negative&quot; jsonschema_description:&quot;Its influence on the field of computing&quot;`
	NotableFacts []string `json:&quot;notable_facts&quot; jsonschema_description:&quot;A few key facts about the computer&quot;`
}

type Origin struct {
	YearBuilt    int64  `json:&quot;year_of_construction&quot; jsonschema_description:&quot;The year it was made&quot;`
	Organization string `json:&quot;organization&quot; jsonschema_description:&quot;The organization that was in charge of its development&quot;`
}

func GenerateSchema[T any]() interface{} {
	// Structured Outputs uses a subset of JSON schema
	// These flags are necessary to comply with the subset
	reflector := jsonschema.Reflector{
		AllowAdditionalProperties: false,
		DoNotReference:            true,
	}
	var v T
	schema := reflector.Reflect(v)
	return schema
}

// Generate the JSON schema at initialization time
var HistoricalComputerResponseSchema = GenerateSchema[HistoricalComputer]()

func main() {

	// ...

	question := &quot;What computer ran the first neural network?&quot;

	schemaParam := openai.ResponseFormatJSONSchemaJSONSchemaParam{
		Name:        &quot;historical_computer&quot;,
		Description: openai.String(&quot;Notable information about a computer&quot;),
		Schema:      HistoricalComputerResponseSchema,
		Strict:      openai.Bool(true),
	}

	chat, _ := client.Chat.Completions.New(ctx, openai.ChatCompletionNewParams{
		// ...
		ResponseFormat: openai.ChatCompletionNewParamsResponseFormatUnion{
			OfJSONSchema: &amp;amp;openai.ResponseFormatJSONSchemaParam{
				JSONSchema: schemaParam,
			},
		},
		// only certain models can perform structured outputs
		Model: openai.ChatModelGPT4o2024_08_06,
	})

	// extract into a well-typed struct
	var historicalComputer HistoricalComputer
	_ = json.Unmarshal([]byte(chat.Choices[0].Message.Content), &amp;amp;historicalComputer)

	historicalComputer.Name
	historicalComputer.Origin.YearBuilt
	historicalComputer.Origin.Organization
	for i, fact := range historicalComputer.NotableFacts {
		// ...
	}
}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;See the &lt;a href=&quot;https://raw.githubusercontent.com/openai/openai-go/main/examples/structured-outputs/main.go&quot;&gt;full structured outputs example&lt;/a&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h3&gt;Request fields&lt;/h3&gt; 
&lt;p&gt;The openai library uses the &lt;a href=&quot;https://tip.golang.org/doc/go1.24#encodingjsonpkgencodingjson&quot;&gt;&lt;code&gt;omitzero&lt;/code&gt;&lt;/a&gt; semantics from the Go 1.24+ &lt;code&gt;encoding/json&lt;/code&gt; release for request fields.&lt;/p&gt; 
&lt;p&gt;Required primitive fields (&lt;code&gt;int64&lt;/code&gt;, &lt;code&gt;string&lt;/code&gt;, etc.) feature the tag &lt;code&gt;`json:&quot;...,required&quot;`&lt;/code&gt;. These fields are always serialized, even their zero values.&lt;/p&gt; 
&lt;p&gt;Optional primitive types are wrapped in a &lt;code&gt;param.Opt[T]&lt;/code&gt;. These fields can be set with the provided constructors, &lt;code&gt;openai.String(string)&lt;/code&gt;, &lt;code&gt;openai.Int(int64)&lt;/code&gt;, etc.&lt;/p&gt; 
&lt;p&gt;Any &lt;code&gt;param.Opt[T]&lt;/code&gt;, map, slice, struct or string enum uses the tag &lt;code&gt;`json:&quot;...,omitzero&quot;`&lt;/code&gt;. Its zero value is considered omitted.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;param.IsOmitted(any)&lt;/code&gt; function can confirm the presence of any &lt;code&gt;omitzero&lt;/code&gt; field.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;p := openai.ExampleParams{
	ID:   &quot;id_xxx&quot;,             // required property
	Name: openai.String(&quot;...&quot;), // optional property

	Point: openai.Point{
		X: 0,             // required field will serialize as 0
		Y: openai.Int(1), // optional field will serialize as 1
		// ... omitted non-required fields will not be serialized
	},

	Origin: openai.Origin{}, // the zero value of [Origin] is considered omitted
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To send &lt;code&gt;null&lt;/code&gt; instead of a &lt;code&gt;param.Opt[T]&lt;/code&gt;, use &lt;code&gt;param.Null[T]()&lt;/code&gt;. To send &lt;code&gt;null&lt;/code&gt; instead of a struct &lt;code&gt;T&lt;/code&gt;, use &lt;code&gt;param.NullStruct[T]()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;p.Name = param.Null[string]()       // &#39;null&#39; instead of string
p.Point = param.NullStruct[Point]() // &#39;null&#39; instead of struct

param.IsNull(p.Name)  // true
param.IsNull(p.Point) // true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Request structs contain a &lt;code&gt;.SetExtraFields(map[string]any)&lt;/code&gt; method which can send non-conforming fields in the request body. Extra fields overwrite any struct fields with a matching key. For security reasons, only use &lt;code&gt;SetExtraFields&lt;/code&gt; with trusted data.&lt;/p&gt; 
&lt;p&gt;To send a custom value instead of a struct, use &lt;code&gt;param.Override[T](value)&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;// In cases where the API specifies a given type,
// but you want to send something else, use [SetExtraFields]:
p.SetExtraFields(map[string]any{
	&quot;x&quot;: 0.01, // send &quot;x&quot; as a float instead of int
})

// Send a number instead of an object
custom := param.Override[openai.FooParams](12)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Request unions&lt;/h3&gt; 
&lt;p&gt;Unions are represented as a struct with fields prefixed by &quot;Of&quot; for each of it&#39;s variants, only one field can be non-zero. The non-zero field will be serialized.&lt;/p&gt; 
&lt;p&gt;Sub-properties of the union can be accessed via methods on the union struct. These methods return a mutable pointer to the underlying data, if present.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;// Only one field can be non-zero, use param.IsOmitted() to check if a field is set
type AnimalUnionParam struct {
	OfCat *Cat `json:&quot;,omitzero,inline`
	OfDog *Dog `json:&quot;,omitzero,inline`
}

animal := AnimalUnionParam{
	OfCat: &amp;amp;Cat{
		Name: &quot;Whiskers&quot;,
		Owner: PersonParam{
			Address: AddressParam{Street: &quot;3333 Coyote Hill Rd&quot;, Zip: 0},
		},
	},
}

// Mutating a field
if address := animal.GetOwner().GetAddress(); address != nil {
	address.ZipCode = 94304
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Response objects&lt;/h3&gt; 
&lt;p&gt;All fields in response structs are ordinary value types (not pointers or wrappers). Response structs also include a special &lt;code&gt;JSON&lt;/code&gt; field containing metadata about each property.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;type Animal struct {
	Name   string `json:&quot;name,nullable&quot;`
	Owners int    `json:&quot;owners&quot;`
	Age    int    `json:&quot;age&quot;`
	JSON   struct {
		Name        respjson.Field
		Owner       respjson.Field
		Age         respjson.Field
		ExtraFields map[string]respjson.Field
	} `json:&quot;-&quot;`
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To handle optional data, use the &lt;code&gt;.Valid()&lt;/code&gt; method on the JSON field. &lt;code&gt;.Valid()&lt;/code&gt; returns true if a field is not &lt;code&gt;null&lt;/code&gt;, not present, or couldn&#39;t be marshaled.&lt;/p&gt; 
&lt;p&gt;If &lt;code&gt;.Valid()&lt;/code&gt; is false, the corresponding field will simply be its zero value.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;raw := `{&quot;owners&quot;: 1, &quot;name&quot;: null}`

var res Animal
json.Unmarshal([]byte(raw), &amp;amp;res)

// Accessing regular fields

res.Owners // 1
res.Name   // &quot;&quot;
res.Age    // 0

// Optional field checks

res.JSON.Owners.Valid() // true
res.JSON.Name.Valid()   // false
res.JSON.Age.Valid()    // false

// Raw JSON values

res.JSON.Owners.Raw()                  // &quot;1&quot;
res.JSON.Name.Raw() == &quot;null&quot;          // true
res.JSON.Name.Raw() == respjson.Null   // true
res.JSON.Age.Raw() == &quot;&quot;               // true
res.JSON.Age.Raw() == respjson.Omitted // true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;These &lt;code&gt;.JSON&lt;/code&gt; structs also include an &lt;code&gt;ExtraFields&lt;/code&gt; map containing any properties in the json response that were not specified in the struct. This can be useful for API features not yet present in the SDK.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;body := res.JSON.ExtraFields[&quot;my_unexpected_field&quot;].Raw()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Response Unions&lt;/h3&gt; 
&lt;p&gt;In responses, unions are represented by a flattened struct containing all possible fields from each of the object variants. To convert it to a variant use the &lt;code&gt;.AsFooVariant()&lt;/code&gt; method or the &lt;code&gt;.AsAny()&lt;/code&gt; method if present.&lt;/p&gt; 
&lt;p&gt;If a response value union contains primitive values, primitive fields will be alongside the properties but prefixed with &lt;code&gt;Of&lt;/code&gt; and feature the tag &lt;code&gt;json:&quot;...,inline&quot;&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;type AnimalUnion struct {
	// From variants [Dog], [Cat]
	Owner Person `json:&quot;owner&quot;`
	// From variant [Dog]
	DogBreed string `json:&quot;dog_breed&quot;`
	// From variant [Cat]
	CatBreed string `json:&quot;cat_breed&quot;`
	// ...

	JSON struct {
		Owner respjson.Field
		// ...
	} `json:&quot;-&quot;`
}

// If animal variant
if animal.Owner.Address.ZipCode == &quot;&quot; {
	panic(&quot;missing zip code&quot;)
}

// Switch on the variant
switch variant := animal.AsAny().(type) {
case Dog:
case Cat:
default:
	panic(&quot;unexpected type&quot;)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;RequestOptions&lt;/h3&gt; 
&lt;p&gt;This library uses the functional options pattern. Functions defined in the &lt;code&gt;option&lt;/code&gt; package return a &lt;code&gt;RequestOption&lt;/code&gt;, which is a closure that mutates a &lt;code&gt;RequestConfig&lt;/code&gt;. These options can be supplied to the client or at individual requests. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;client := openai.NewClient(
	// Adds a header to every request made by the client
	option.WithHeader(&quot;X-Some-Header&quot;, &quot;custom_header_info&quot;),
)

client.Chat.Completions.New(context.TODO(), ...,
	// Override the header
	option.WithHeader(&quot;X-Some-Header&quot;, &quot;some_other_custom_header_info&quot;),
	// Add an undocumented field to the request body, using sjson syntax
	option.WithJSONSet(&quot;some.json.path&quot;, map[string]string{&quot;my&quot;: &quot;object&quot;}),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The request option &lt;code&gt;option.WithDebugLog(nil)&lt;/code&gt; may be helpful while debugging.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://pkg.go.dev/github.com/openai/openai-go/option&quot;&gt;full list of request options&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Pagination&lt;/h3&gt; 
&lt;p&gt;This library provides some conveniences for working with paginated list endpoints.&lt;/p&gt; 
&lt;p&gt;You can use &lt;code&gt;.ListAutoPaging()&lt;/code&gt; methods to iterate through items across all pages:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;iter := client.FineTuning.Jobs.ListAutoPaging(context.TODO(), openai.FineTuningJobListParams{
	Limit: openai.Int(20),
})
// Automatically fetches more pages as needed.
for iter.Next() {
	fineTuningJob := iter.Current()
	fmt.Printf(&quot;%+v\n&quot;, fineTuningJob)
}
if err := iter.Err(); err != nil {
	panic(err.Error())
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or you can use simple &lt;code&gt;.List()&lt;/code&gt; methods to fetch a single page and receive a standard response object with additional helper methods like &lt;code&gt;.GetNextPage()&lt;/code&gt;, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;page, err := client.FineTuning.Jobs.List(context.TODO(), openai.FineTuningJobListParams{
	Limit: openai.Int(20),
})
for page != nil {
	for _, job := range page.Data {
		fmt.Printf(&quot;%+v\n&quot;, job)
	}
	page, err = page.GetNextPage()
}
if err != nil {
	panic(err.Error())
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Errors&lt;/h3&gt; 
&lt;p&gt;When the API returns a non-success status code, we return an error with type &lt;code&gt;*openai.Error&lt;/code&gt;. This contains the &lt;code&gt;StatusCode&lt;/code&gt;, &lt;code&gt;*http.Request&lt;/code&gt;, and &lt;code&gt;*http.Response&lt;/code&gt; values of the request, as well as the JSON of the error body (much like other response objects in the SDK).&lt;/p&gt; 
&lt;p&gt;To handle errors, we recommend that you use the &lt;code&gt;errors.As&lt;/code&gt; pattern:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;_, err := client.FineTuning.Jobs.New(context.TODO(), openai.FineTuningJobNewParams{
	Model:        openai.FineTuningJobNewParamsModelBabbage002,
	TrainingFile: &quot;file-abc123&quot;,
})
if err != nil {
	var apierr *openai.Error
	if errors.As(err, &amp;amp;apierr) {
		println(string(apierr.DumpRequest(true)))  // Prints the serialized HTTP request
		println(string(apierr.DumpResponse(true))) // Prints the serialized HTTP response
	}
	panic(err.Error()) // GET &quot;/fine_tuning/jobs&quot;: 400 Bad Request { ... }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When other errors occur, they are returned unwrapped; for example, if HTTP transport fails, you might receive &lt;code&gt;*url.Error&lt;/code&gt; wrapping &lt;code&gt;*net.OpError&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Timeouts&lt;/h3&gt; 
&lt;p&gt;Requests do not time out by default; use context to configure a timeout for a request lifecycle.&lt;/p&gt; 
&lt;p&gt;Note that if a request is &lt;a href=&quot;https://raw.githubusercontent.com/openai/openai-go/main/#retries&quot;&gt;retried&lt;/a&gt;, the context timeout does not start over. To set a per-retry timeout, use &lt;code&gt;option.WithRequestTimeout()&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;// This sets the timeout for the request, including all the retries.
ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)
defer cancel()
client.Chat.Completions.New(
	ctx,
	openai.ChatCompletionNewParams{
		Messages: []openai.ChatCompletionMessageParamUnion{{
			OfUser: &amp;amp;openai.ChatCompletionUserMessageParam{
				Content: openai.ChatCompletionUserMessageParamContentUnion{
					OfString: openai.String(&quot;How can I list all files in a directory using Python?&quot;),
				},
			},
		}},
		Model: shared.ChatModelGPT5,
	},
	// This sets the per-retry timeout
	option.WithRequestTimeout(20*time.Second),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;File uploads&lt;/h3&gt; 
&lt;p&gt;Request parameters that correspond to file uploads in multipart requests are typed as &lt;code&gt;io.Reader&lt;/code&gt;. The contents of the &lt;code&gt;io.Reader&lt;/code&gt; will by default be sent as a multipart form part with the file name of &quot;anonymous_file&quot; and content-type of &quot;application/octet-stream&quot;.&lt;/p&gt; 
&lt;p&gt;The file name and content-type can be customized by implementing &lt;code&gt;Name() string&lt;/code&gt; or &lt;code&gt;ContentType() string&lt;/code&gt; on the run-time type of &lt;code&gt;io.Reader&lt;/code&gt;. Note that &lt;code&gt;os.File&lt;/code&gt; implements &lt;code&gt;Name() string&lt;/code&gt;, so a file returned by &lt;code&gt;os.Open&lt;/code&gt; will be sent with the file name on disk.&lt;/p&gt; 
&lt;p&gt;We also provide a helper &lt;code&gt;openai.File(reader io.Reader, filename string, contentType string)&lt;/code&gt; which can be used to wrap any &lt;code&gt;io.Reader&lt;/code&gt; with the appropriate file name and content type.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;// A file from the file system
file, err := os.Open(&quot;input.jsonl&quot;)
openai.FileNewParams{
	File:    file,
	Purpose: openai.FilePurposeFineTune,
}

// A file from a string
openai.FileNewParams{
	File:    strings.NewReader(&quot;my file contents&quot;),
	Purpose: openai.FilePurposeFineTune,
}

// With a custom filename and contentType
openai.FileNewParams{
	File:    openai.File(strings.NewReader(`{&quot;hello&quot;: &quot;foo&quot;}`), &quot;file.go&quot;, &quot;application/json&quot;),
	Purpose: openai.FilePurposeFineTune,
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Webhook Verification&lt;/h2&gt; 
&lt;p&gt;Verifying webhook signatures is &lt;em&gt;optional but encouraged&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;For more information about webhooks, see &lt;a href=&quot;https://platform.openai.com/docs/guides/webhooks&quot;&gt;the API docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Parsing webhook payloads&lt;/h3&gt; 
&lt;p&gt;For most use cases, you will likely want to verify the webhook and parse the payload at the same time. To achieve this, we provide the method &lt;code&gt;client.Webhooks.Unwrap()&lt;/code&gt;, which parses a webhook request and verifies that it was sent by OpenAI. This method will return an error if the signature is invalid.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter should be the raw JSON bytes sent from the server (do not parse it first). The &lt;code&gt;Unwrap()&lt;/code&gt; method will parse this JSON for you into an event object after verifying the webhook was sent from OpenAI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
	&quot;io&quot;
	&quot;log&quot;
	&quot;net/http&quot;
	&quot;os&quot;

	&quot;github.com/gin-gonic/gin&quot;
	&quot;github.com/openai/openai-go/v2&quot;
	&quot;github.com/openai/openai-go/v2/option&quot;
	&quot;github.com/openai/openai-go/v2/webhooks&quot;
)

func main() {
	client := openai.NewClient(
		option.WithWebhookSecret(os.Getenv(&quot;OPENAI_WEBHOOK_SECRET&quot;)), // env var used by default; explicit here.
	)

	r := gin.Default()

	r.POST(&quot;/webhook&quot;, func(c *gin.Context) {
		body, err := io.ReadAll(c.Request.Body)
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{&quot;error&quot;: &quot;Error reading request body&quot;})
			return
		}
		defer c.Request.Body.Close()

		webhookEvent, err := client.Webhooks.Unwrap(body, c.Request.Header)
		if err != nil {
			log.Printf(&quot;Invalid webhook signature: %v&quot;, err)
			c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: &quot;invalid signature&quot;})
			return
		}

		switch event := webhookEvent.AsAny().(type) {
		case webhooks.ResponseCompletedWebhookEvent:
			log.Printf(&quot;Response completed: %+v&quot;, event.Data)
		case webhooks.ResponseFailedWebhookEvent:
			log.Printf(&quot;Response failed: %+v&quot;, event.Data)
		default:
			log.Printf(&quot;Unhandled event type: %T&quot;, event)
		}

		c.JSON(http.StatusOK, gin.H{&quot;message&quot;: &quot;ok&quot;})
	})

	r.Run(&quot;:8000&quot;)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Verifying webhook payloads directly&lt;/h3&gt; 
&lt;p&gt;In some cases, you may want to verify the webhook separately from parsing the payload. If you prefer to handle these steps separately, we provide the method &lt;code&gt;client.Webhooks.VerifySignature()&lt;/code&gt; to &lt;em&gt;only verify&lt;/em&gt; the signature of a webhook request. Like &lt;code&gt;Unwrap()&lt;/code&gt;, this method will return an error if the signature is invalid.&lt;/p&gt; 
&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter should be the raw JSON bytes sent from the server (do not parse it first). You will then need to parse the body after verifying the signature.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
	&quot;encoding/json&quot;
	&quot;io&quot;
	&quot;log&quot;
	&quot;net/http&quot;
	&quot;os&quot;

	&quot;github.com/gin-gonic/gin&quot;
	&quot;github.com/openai/openai-go/v2&quot;
	&quot;github.com/openai/openai-go/v2/option&quot;
)

func main() {
	client := openai.NewClient(
		option.WithWebhookSecret(os.Getenv(&quot;OPENAI_WEBHOOK_SECRET&quot;)), // env var used by default; explicit here.
	)

	r := gin.Default()

	r.POST(&quot;/webhook&quot;, func(c *gin.Context) {
		body, err := io.ReadAll(c.Request.Body)
		if err != nil {
			c.JSON(http.StatusInternalServerError, gin.H{&quot;error&quot;: &quot;Error reading request body&quot;})
			return
		}
		defer c.Request.Body.Close()

		err = client.Webhooks.VerifySignature(body, c.Request.Header)
		if err != nil {
			log.Printf(&quot;Invalid webhook signature: %v&quot;, err)
			c.JSON(http.StatusBadRequest, gin.H{&quot;error&quot;: &quot;invalid signature&quot;})
			return
		}

		c.JSON(http.StatusOK, gin.H{&quot;message&quot;: &quot;ok&quot;})
	})

	r.Run(&quot;:8000&quot;)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Retries&lt;/h3&gt; 
&lt;p&gt;Certain errors will be automatically retried 2 times by default, with a short exponential backoff. We retry by default all connection errors, 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors.&lt;/p&gt; 
&lt;p&gt;You can use the &lt;code&gt;WithMaxRetries&lt;/code&gt; option to configure or disable this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;// Configure the default for all requests:
client := openai.NewClient(
	option.WithMaxRetries(0), // default is 2
)

// Override per-request:
client.Chat.Completions.New(
	context.TODO(),
	openai.ChatCompletionNewParams{
		Messages: []openai.ChatCompletionMessageParamUnion{{
			OfUser: &amp;amp;openai.ChatCompletionUserMessageParam{
				Content: openai.ChatCompletionUserMessageParamContentUnion{
					OfString: openai.String(&quot;How can I get the name of the current day in JavaScript?&quot;),
				},
			},
		}},
		Model: shared.ChatModelGPT5,
	},
	option.WithMaxRetries(5),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Accessing raw response data (e.g. response headers)&lt;/h3&gt; 
&lt;p&gt;You can access the raw HTTP response data by using the &lt;code&gt;option.WithResponseInto()&lt;/code&gt; request option. This is useful when you need to examine response headers, status codes, or other details.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;// Create a variable to store the HTTP response
var response *http.Response
chatCompletion, err := client.Chat.Completions.New(
	context.TODO(),
	openai.ChatCompletionNewParams{
		Messages: []openai.ChatCompletionMessageParamUnion{{
			OfUser: &amp;amp;openai.ChatCompletionUserMessageParam{
				Content: openai.ChatCompletionUserMessageParamContentUnion{
					OfString: openai.String(&quot;Say this is a test&quot;),
				},
			},
		}},
		Model: shared.ChatModelGPT5,
	},
	option.WithResponseInto(&amp;amp;response),
)
if err != nil {
	// handle error
}
fmt.Printf(&quot;%+v\n&quot;, chatCompletion)

fmt.Printf(&quot;Status Code: %d\n&quot;, response.StatusCode)
fmt.Printf(&quot;Headers: %+#v\n&quot;, response.Header)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Making custom/undocumented requests&lt;/h3&gt; 
&lt;p&gt;This library is typed for convenient access to the documented API. If you need to access undocumented endpoints, params, or response properties, the library can still be used.&lt;/p&gt; 
&lt;h4&gt;Undocumented endpoints&lt;/h4&gt; 
&lt;p&gt;To make requests to undocumented endpoints, you can use &lt;code&gt;client.Get&lt;/code&gt;, &lt;code&gt;client.Post&lt;/code&gt;, and other HTTP verbs. &lt;code&gt;RequestOptions&lt;/code&gt; on the client, such as retries, will be respected when making these requests.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;var (
    // params can be an io.Reader, a []byte, an encoding/json serializable object,
    // or a &quot;‚Ä¶Params&quot; struct defined in this library.
    params map[string]any

    // result can be an []byte, *http.Response, a encoding/json deserializable object,
    // or a model defined in this library.
    result *http.Response
)
err := client.Post(context.Background(), &quot;/unspecified&quot;, params, &amp;amp;result)
if err != nil {
    ‚Ä¶
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Undocumented request params&lt;/h4&gt; 
&lt;p&gt;To make requests using undocumented parameters, you may use either the &lt;code&gt;option.WithQuerySet()&lt;/code&gt; or the &lt;code&gt;option.WithJSONSet()&lt;/code&gt; methods.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;params := FooNewParams{
    ID:   &quot;id_xxxx&quot;,
    Data: FooNewParamsData{
        FirstName: openai.String(&quot;John&quot;),
    },
}
client.Foo.New(context.Background(), params, option.WithJSONSet(&quot;data.last_name&quot;, &quot;Doe&quot;))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Undocumented response properties&lt;/h4&gt; 
&lt;p&gt;To access undocumented response properties, you may either access the raw JSON of the response as a string with &lt;code&gt;result.JSON.RawJSON()&lt;/code&gt;, or get the raw JSON of a particular field on the result with &lt;code&gt;result.JSON.Foo.Raw()&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Any fields that are not present on the response struct will be saved and can be accessed by &lt;code&gt;result.JSON.ExtraFields()&lt;/code&gt; which returns the extra fields as a &lt;code&gt;map[string]Field&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Middleware&lt;/h3&gt; 
&lt;p&gt;We provide &lt;code&gt;option.WithMiddleware&lt;/code&gt; which applies the given middleware to requests.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;func Logger(req *http.Request, next option.MiddlewareNext) (res *http.Response, err error) {
	// Before the request
	start := time.Now()
	LogReq(req)

	// Forward the request to the next handler
	res, err = next(req)

	// Handle stuff after the request
	end := time.Now()
	LogRes(res, err, start - end)

    return res, err
}

client := openai.NewClient(
	option.WithMiddleware(Logger),
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When multiple middlewares are provided as variadic arguments, the middlewares are applied left to right. If &lt;code&gt;option.WithMiddleware&lt;/code&gt; is given multiple times, for example first in the client then the method, the middleware in the client will run first and the middleware given in the method will run next.&lt;/p&gt; 
&lt;p&gt;You may also replace the default &lt;code&gt;http.Client&lt;/code&gt; with &lt;code&gt;option.WithHTTPClient(client)&lt;/code&gt;. Only one http client is accepted (this overwrites any previous client) and receives requests after any middleware has been applied.&lt;/p&gt; 
&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; 
&lt;p&gt;To use this library with [Azure OpenAI]&lt;a href=&quot;https://learn.microsoft.com/azure/ai-services/openai/overview&quot;&gt;https://learn.microsoft.com/azure/ai-services/openai/overview&lt;/a&gt;), use the option.RequestOption functions in the &lt;code&gt;azure&lt;/code&gt; package.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
	&quot;github.com/Azure/azure-sdk-for-go/sdk/azidentity&quot;
	&quot;github.com/openai/openai-go/v2&quot;
	&quot;github.com/openai/openai-go/v2/azure&quot;
)

func main() {
	const azureOpenAIEndpoint = &quot;https://&amp;lt;azure-openai-resource&amp;gt;.openai.azure.com&quot;

	// The latest API versions, including previews, can be found here:
	// ttps://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versionng
	const azureOpenAIAPIVersion = &quot;2024-06-01&quot;

	tokenCredential, err := azidentity.NewDefaultAzureCredential(nil)

	if err != nil {
		fmt.Printf(&quot;Failed to create the DefaultAzureCredential: %s&quot;, err)
		os.Exit(1)
	}

	client := openai.NewClient(
		azure.WithEndpoint(azureOpenAIEndpoint, azureOpenAIAPIVersion),

		// Choose between authenticating using a TokenCredential or an API Key
		azure.WithTokenCredential(tokenCredential),
		// or azure.WithAPIKey(azureOpenAIAPIKey),
	)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Semantic versioning&lt;/h2&gt; 
&lt;p&gt;This package generally follows &lt;a href=&quot;https://semver.org/spec/v2.0.0.html&quot;&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals.)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; 
&lt;p&gt;We are keen for your feedback; please open an &lt;a href=&quot;https://www.github.com/openai/openai-go/issues&quot;&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/openai/openai-go/main/CONTRIBUTING.md&quot;&gt;the contributing documentation&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>simstudioai/sim</title>
      <link>https://github.com/simstudioai/sim</link>
      <description>&lt;p&gt;Sim is an open-source AI agent workflow builder. Sim Studio&#39;s interface is a lightweight, intuitive way to quickly build and deploy LLMs that connect with your favorite tools.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/simstudioai/sim/main/apps/sim/public/static/sim.png&quot; alt=&quot;Sim Logo&quot; width=&quot;500&quot; /&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&quot; alt=&quot;License: Apache-2.0&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/Hr4UWYEcTT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Server-7289DA?logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://x.com/simdotai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/simstudioai?style=social&quot; alt=&quot;Twitter&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/simstudioai/sim/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true&quot; alt=&quot;PRs welcome&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://docs.sim.ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Docs-visit%20documentation-blue.svg?sanitize=true&quot; alt=&quot;Documentation&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;strong&gt;Sim&lt;/strong&gt; is a lightweight, user-friendly platform for building AI agent workflows. &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/simstudioai/sim/main/apps/sim/public/static/demo.gif&quot; alt=&quot;Sim Demo&quot; width=&quot;800&quot; /&gt; &lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Use our &lt;a href=&quot;https://sim.ai&quot;&gt;cloud-hosted version&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Self-host using one of the methods below&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Self-Hosting Options&lt;/h2&gt; 
&lt;h3&gt;Option 1: NPM Package (Simplest)&lt;/h3&gt; 
&lt;p&gt;The easiest way to run Sim locally is using our &lt;a href=&quot;https://www.npmjs.com/package/simstudio?activeTab=readme&quot;&gt;NPM package&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;npx simstudio
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running these commands, open &lt;a href=&quot;http://localhost:3000/&quot;&gt;http://localhost:3000/&lt;/a&gt; in your browser.&lt;/p&gt; 
&lt;h4&gt;Options&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;-p, --port &amp;lt;port&amp;gt;&lt;/code&gt;: Specify the port to run Sim on (default: 3000)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;--no-pull&lt;/code&gt;: Skip pulling the latest Docker images&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Requirements&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker must be installed and running on your machine&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Option 2: Docker Compose&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Clone the repository
git clone https://github.com/simstudioai/sim.git

# Navigate to the project directory
cd sim

# Start Sim
docker compose -f docker-compose.prod.yml up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Access the application at &lt;a href=&quot;http://localhost:3000/&quot;&gt;http://localhost:3000/&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;Using Local Models with Ollama&lt;/h4&gt; 
&lt;p&gt;Run Sim with local AI models using &lt;a href=&quot;https://ollama.ai&quot;&gt;Ollama&lt;/a&gt; - no external APIs required:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Start with GPU support (automatically downloads gemma3:4b model)
docker compose -f docker-compose.ollama.yml --profile setup up -d

# For CPU-only systems:
docker compose -f docker-compose.ollama.yml --profile cpu --profile setup up -d
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Wait for the model to download, then visit &lt;a href=&quot;http://localhost:3000&quot;&gt;http://localhost:3000&lt;/a&gt;. Add more models with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker compose -f docker-compose.ollama.yml exec ollama ollama pull llama3.1:8b
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option 3: Dev Containers&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open VS Code with the &lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers&quot;&gt;Remote - Containers extension&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Open the project and click &quot;Reopen in Container&quot; when prompted&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;bun run dev:full&lt;/code&gt; in the terminal or use the &lt;code&gt;sim-start&lt;/code&gt; alias 
  &lt;ul&gt; 
   &lt;li&gt;This starts both the main application and the realtime socket server&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Option 4: Manual Setup&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://bun.sh/&quot;&gt;Bun&lt;/a&gt; runtime&lt;/li&gt; 
 &lt;li&gt;PostgreSQL 12+ with &lt;a href=&quot;https://github.com/pgvector/pgvector&quot;&gt;pgvector extension&lt;/a&gt; (required for AI embeddings)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Sim uses vector embeddings for AI features like knowledge bases and semantic search, which requires the &lt;code&gt;pgvector&lt;/code&gt; PostgreSQL extension.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone and install dependencies:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/simstudioai/sim.git
cd sim
bun install
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Set up PostgreSQL with pgvector:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You need PostgreSQL with the &lt;code&gt;vector&lt;/code&gt; extension for embedding support. Choose one option:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Option A: Using Docker (Recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Start PostgreSQL with pgvector extension
docker run --name simstudio-db \
  -e POSTGRES_PASSWORD=your_password \
  -e POSTGRES_DB=simstudio \
  -p 5432:5432 -d \
  pgvector/pgvector:pg17
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Option B: Manual Installation&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install PostgreSQL 12+ and the pgvector extension&lt;/li&gt; 
 &lt;li&gt;See &lt;a href=&quot;https://github.com/pgvector/pgvector#installation&quot;&gt;pgvector installation guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Set up environment:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd apps/sim
cp .env.example .env  # Configure with required variables (DATABASE_URL, BETTER_AUTH_SECRET, BETTER_AUTH_URL)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Update your &lt;code&gt;.env&lt;/code&gt; file with the database URL:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;DATABASE_URL=&quot;postgresql://postgres:your_password@localhost:5432/simstudio&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;4&quot;&gt; 
 &lt;li&gt;Set up the database:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bunx drizzle-kit migrate 
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;5&quot;&gt; 
 &lt;li&gt;Start the development servers:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Recommended approach - run both servers together (from project root):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bun run dev:full
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This starts both the main Next.js application and the realtime socket server required for full functionality.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Alternative - run servers separately:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Next.js app (from project root):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bun run dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Realtime socket server (from &lt;code&gt;apps/sim&lt;/code&gt; directory in a separate terminal):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd apps/sim
bun run dev:sockets
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Tech Stack&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Framework&lt;/strong&gt;: &lt;a href=&quot;https://nextjs.org/&quot;&gt;Next.js&lt;/a&gt; (App Router)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Runtime&lt;/strong&gt;: &lt;a href=&quot;https://bun.sh/&quot;&gt;Bun&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: PostgreSQL with &lt;a href=&quot;https://orm.drizzle.team&quot;&gt;Drizzle ORM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Authentication&lt;/strong&gt;: &lt;a href=&quot;https://better-auth.com&quot;&gt;Better Auth&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UI&lt;/strong&gt;: &lt;a href=&quot;https://ui.shadcn.com/&quot;&gt;Shadcn&lt;/a&gt;, &lt;a href=&quot;https://tailwindcss.com&quot;&gt;Tailwind CSS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;State Management&lt;/strong&gt;: &lt;a href=&quot;https://zustand-demo.pmnd.rs/&quot;&gt;Zustand&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flow Editor&lt;/strong&gt;: &lt;a href=&quot;https://reactflow.dev/&quot;&gt;ReactFlow&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href=&quot;https://fumadocs.vercel.app/&quot;&gt;Fumadocs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Monorepo&lt;/strong&gt;: &lt;a href=&quot;https://turborepo.org/&quot;&gt;Turborepo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Realtime&lt;/strong&gt;: &lt;a href=&quot;https://socket.io/&quot;&gt;Socket.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Background Jobs&lt;/strong&gt;: &lt;a href=&quot;https://trigger.dev/&quot;&gt;Trigger.dev&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions! Please see our &lt;a href=&quot;https://raw.githubusercontent.com/simstudioai/sim/main/.github/CONTRIBUTING.md&quot;&gt;Contributing Guide&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the Apache License 2.0 - see the &lt;a href=&quot;https://raw.githubusercontent.com/simstudioai/sim/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;Made with ‚ù§Ô∏è by the Sim Team&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>patchy631/ai-engineering-hub</title>
      <link>https://github.com/patchy631/ai-engineering-hub</link>
      <description>&lt;p&gt;In-depth tutorials on LLMs, RAGs and real-world AI agent applications.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/12800&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/assets/TRENDING-BADGE.png&quot; alt=&quot;Trending Badge&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/assets/ai-eng-hub.gif&quot; alt=&quot;AI Engineering Hub Banner&quot; /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;AI Engineering Hub üöÄ&lt;/h1&gt; 
&lt;p&gt;Welcome to the &lt;strong&gt;AI Engineering Hub&lt;/strong&gt;!&lt;/p&gt; 
&lt;h2&gt;üåü Why This Repo?&lt;/h2&gt; 
&lt;p&gt;AI Engineering is advancing rapidly, and staying at the forefront requires both deep understanding and hands-on experience. Here, you will find:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In-depth tutorials on &lt;strong&gt;LLMs and RAGs&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Real-world &lt;strong&gt;AI agent&lt;/strong&gt; applications&lt;/li&gt; 
 &lt;li&gt;Examples to implement, adapt, and scale in your projects&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Whether you‚Äôre a beginner, practitioner, or researcher, this repo provides resources for all skill levels to experiment and succeed in AI engineering.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üì¨ Stay Updated with Our Newsletter!&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Get a FREE Data Science eBook&lt;/strong&gt; üìñ with 150+ essential lessons in Data Science when you subscribe to our newsletter! Stay in the loop with the latest tutorials, insights, and exclusive resources. &lt;a href=&quot;https://join.dailydoseofds.com&quot;&gt;Subscribe now!&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://join.dailydoseofds.com&quot;&gt;&lt;img src=&quot;https://github.com/patchy631/ai-engineering/raw/main/resources/join_ddods.png&quot; alt=&quot;Daily Dose of Data Science Newsletter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üì¢ Contribute to the AI Engineering Hub!&lt;/h2&gt; 
&lt;p&gt;We welcome contributors! Whether you want to add new tutorials, improve existing code, or report issues, your contributions make this community thrive. Here‚Äôs how to get involved:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Fork&lt;/strong&gt; the repository.&lt;/li&gt; 
 &lt;li&gt;Create a new branch for your contribution.&lt;/li&gt; 
 &lt;li&gt;Submit a &lt;strong&gt;Pull Request&lt;/strong&gt; and describe the improvements.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìú License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the MIT License - see the &lt;a href=&quot;https://raw.githubusercontent.com/patchy631/ai-engineering-hub/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;h2&gt;üí¨ Connect&lt;/h2&gt; 
&lt;p&gt;For discussions, suggestions, and more, feel free to &lt;a href=&quot;https://github.com/patchy631/ai-engineering/issues&quot;&gt;create an issue&lt;/a&gt; or reach out directly!&lt;/p&gt; 
&lt;p&gt;Happy Coding! üéâ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>nautechsystems/nautilus_trader</title>
      <link>https://github.com/nautechsystems/nautilus_trader</link>
      <description>&lt;p&gt;A high-performance algorithmic trading platform and event-driven backtester&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader-logo.png&quot; width=&quot;500&quot; /&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://codecov.io/gh/nautechsystems/nautilus_trader&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/nautechsystems/nautilus_trader/branch/master/graph/badge.svg?token=DXO9QQI40H&quot; alt=&quot;codecov&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codspeed.io/nautechsystems/nautilus_trader&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://codspeed.io/badge.json&quot; alt=&quot;codspeed&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/pypi/pyversions/nautilus_trader&quot; alt=&quot;pythons&quot; /&gt; &lt;img src=&quot;https://img.shields.io/pypi/v/nautilus_trader&quot; alt=&quot;pypi-version&quot; /&gt; &lt;img src=&quot;https://img.shields.io/pypi/format/nautilus_trader?color=blue&quot; alt=&quot;pypi-format&quot; /&gt; &lt;a href=&quot;https://pepy.tech/project/nautilus-trader&quot;&gt;&lt;img src=&quot;https://pepy.tech/badge/nautilus-trader&quot; alt=&quot;Downloads&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/NautilusTrader&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;left&quot;&gt;Branch&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;Version&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;master&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://packages.nautechsystems.io/simple/nautilus-trader/index.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fmaster%2Fversion.json&quot; alt=&quot;version&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml&quot;&gt;&lt;img src=&quot;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly&quot; alt=&quot;build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://packages.nautechsystems.io/simple/nautilus-trader/index.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fnightly%2Fversion.json&quot; alt=&quot;version&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml&quot;&gt;&lt;img src=&quot;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly&quot; alt=&quot;build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;develop&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://packages.nautechsystems.io/simple/nautilus-trader/index.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fdevelop%2Fversion.json&quot; alt=&quot;version&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml&quot;&gt;&lt;img src=&quot;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=develop&quot; alt=&quot;build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;left&quot;&gt;Platform&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;Rust&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;Python&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;1.89.0&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;1.89.0&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;1.89.0&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;3.11-3.13&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;1.89.0&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;3.11-3.13*&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;* Windows builds are currently pinned to CPython 3.13.2, see &lt;a href=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/docs/getting_started/installation.md&quot;&gt;installation guide&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href=&quot;https://nautilustrader.io/docs/&quot;&gt;https://nautilustrader.io/docs/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Website&lt;/strong&gt;: &lt;a href=&quot;https://nautilustrader.io&quot;&gt;https://nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Support&lt;/strong&gt;: &lt;a href=&quot;mailto:support@nautilustrader.io&quot;&gt;support@nautilustrader.io&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform, providing quantitative traders with the ability to backtest portfolios of automated trading strategies on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.&lt;/p&gt; 
&lt;p&gt;The platform is &lt;em&gt;AI-first&lt;/em&gt;, designed to develop and deploy algorithmic trading strategies within a highly performant and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest environment consistent with the production live trading environment.&lt;/p&gt; 
&lt;p&gt;NautilusTrader&#39;s design, architecture, and implementation philosophy prioritizes software correctness and safety at the highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting and live deployment workloads.&lt;/p&gt; 
&lt;p&gt;The platform is also universal, and asset-class-agnostic ‚Äî with any REST API or WebSocket feed able to be integrated via modular adapters. It supports high-frequency trading across a wide range of asset classes and instrument types including FX, Equities, Futures, Options, Crypto, DeFi, and Betting, enabling seamless operations across multiple venues simultaneously.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-trader.png&quot; alt=&quot;nautilus-trader&quot; title=&quot;nautilus-trader&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Core is written in Rust with asynchronous networking using &lt;a href=&quot;https://crates.io/crates/tokio&quot;&gt;tokio&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reliable&lt;/strong&gt;: Rust-powered type- and thread-safety, with optional Redis-backed state persistence.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Portable&lt;/strong&gt;: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Modular adapters mean any REST API or WebSocket feed can be integrated.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: Time in force &lt;code&gt;IOC&lt;/code&gt;, &lt;code&gt;FOK&lt;/code&gt;, &lt;code&gt;GTC&lt;/code&gt;, &lt;code&gt;GTD&lt;/code&gt;, &lt;code&gt;DAY&lt;/code&gt;, &lt;code&gt;AT_THE_OPEN&lt;/code&gt;, &lt;code&gt;AT_THE_CLOSE&lt;/code&gt;, advanced order types and conditional triggers. Execution instructions &lt;code&gt;post-only&lt;/code&gt;, &lt;code&gt;reduce-only&lt;/code&gt;, and icebergs. Contingency orders including &lt;code&gt;OCO&lt;/code&gt;, &lt;code&gt;OUO&lt;/code&gt;, &lt;code&gt;OTO&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Add user-defined custom components, or assemble entire systems from scratch leveraging the &lt;a href=&quot;https://nautilustrader.io/docs/latest/concepts/cache&quot;&gt;cache&lt;/a&gt; and &lt;a href=&quot;https://nautilustrader.io/docs/latest/concepts/message_bus&quot;&gt;message bus&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backtesting&lt;/strong&gt;: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Live&lt;/strong&gt;: Use identical strategy implementations between backtesting and live deployments.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Multi-venue&lt;/strong&gt;: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;AI Training&lt;/strong&gt;: Backtest engine fast enough to be used to train AI trading agents (RL/ES).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/nautilus-art.png&quot; alt=&quot;Alt text&quot; title=&quot;nautilus&quot; /&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;nautilus - from ancient Greek &#39;sailor&#39; and naus &#39;ship&#39;.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral. The idea is that this can be translated to the aesthetics of design and architecture.&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Why NautilusTrader?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Highly performant event-driven Python&lt;/strong&gt;: Native binary core components.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Parity between backtesting and live trading&lt;/strong&gt;: Identical strategy code.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reduced operational risk&lt;/strong&gt;: Enhanced risk management functionality, logical accuracy, and type safety.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Highly extendable&lt;/strong&gt;: Message bus, custom components and actors, custom data, custom adapters.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Traditionally, trading strategy research and backtesting might be conducted in Python using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot express the granular time and event dependent complexity of real-time trading, where compiled languages have proven to be more suitable due to their inherently higher performance, and type safety.&lt;/p&gt; 
&lt;p&gt;One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform have all been written entirely in &lt;a href=&quot;https://www.rust-lang.org/&quot;&gt;Rust&lt;/a&gt; or &lt;a href=&quot;https://cython.org/&quot;&gt;Cython&lt;/a&gt;. This means we&#39;re using the right tools for the job, where systems programming languages compile performant binaries, with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.&lt;/p&gt; 
&lt;h2&gt;Why Python?&lt;/h2&gt; 
&lt;p&gt;Python was originally created decades ago as a simple scripting language with a clean straightforward syntax. It has since evolved into a fully fledged general purpose object-oriented programming language. Based on the TIOBE index, Python is currently the most popular programming language in the world. Not only that, Python has become the &lt;em&gt;de facto lingua franca&lt;/em&gt; of data science, machine learning, and artificial intelligence.&lt;/p&gt; 
&lt;p&gt;developer/user communities. However, Python has performance and typing limitations for large-scale, latency-sensitive systems. Cython addresses many of these issues by introducing static typing into Python&#39;s rich ecosystem of libraries and communities.&lt;/p&gt; 
&lt;h2&gt;Why Rust?&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.rust-lang.org/&quot;&gt;Rust&lt;/a&gt; is a multi-paradigm programming language designed for performance and safety, especially safe concurrency. Rust is &quot;blazingly fast&quot; and memory-efficient (comparable to C and C++) with no garbage collector. It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.&lt;/p&gt; 
&lt;p&gt;Rust‚Äôs rich type system and ownership model guarantees memory-safety and thread-safety deterministically ‚Äî eliminating many classes of bugs at compile-time.&lt;/p&gt; 
&lt;p&gt;The project increasingly utilizes Rust for core performance-critical components. Python bindings are implemented via Cython and &lt;a href=&quot;https://pyo3.rs&quot;&gt;PyO3&lt;/a&gt;‚Äîno Rust toolchain is required at install time.&lt;/p&gt; 
&lt;p&gt;This project makes the &lt;a href=&quot;https://raphlinus.github.io/rust/2020/01/18/soundness-pledge.html&quot;&gt;Soundness Pledge&lt;/a&gt;:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄúThe intent of this project is to be free of soundness bugs. The developers will do their best to avoid them, and welcome help in analyzing and fixing them.‚Äù&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;MSRV:&lt;/strong&gt; NautilusTrader relies heavily on improvements in the Rust language and compiler. As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Integrations&lt;/h2&gt; 
&lt;p&gt;NautilusTrader is modularly designed to work with &lt;em&gt;adapters&lt;/em&gt;, enabling connectivity to trading venues and data providers by translating their raw APIs into a unified interface and normalized domain model.&lt;/p&gt; 
&lt;p&gt;The following integrations are currently supported; see &lt;a href=&quot;https://nautilustrader.io/docs/latest/integrations/&quot;&gt;docs/integrations/&lt;/a&gt; for details:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;left&quot;&gt;Name&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;ID&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;Type&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;Status&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;Docs&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://betfair.com&quot;&gt;Betfair&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;BETFAIR&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Sports Betting Exchange&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stable-green&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/betfair.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://binance.com&quot;&gt;Binance&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stable-green&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://binance.us&quot;&gt;Binance US&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stable-green&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.binance.com/en/futures&quot;&gt;Binance Futures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stable-green&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/binance.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.bitmex.com&quot;&gt;BitMEX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;BITMEX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/building-orange&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bitmex.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.bybit.com&quot;&gt;Bybit&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;BYBIT&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stable-green&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/bybit.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.coinbase.com/en/international-exchange&quot;&gt;Coinbase International&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;COINBASE_INTX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stable-green&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/coinbase_intx.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://databento.com&quot;&gt;Databento&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;DATABENTO&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Data Provider&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stable-green&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/databento.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://dydx.exchange/&quot;&gt;dYdX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;DYDX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stable-green&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/dydx.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://hyperliquid.xyz&quot;&gt;Hyperliquid&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;HYPERLIQUID&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Crypto Exchange (DEX)&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/building-orange&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/hyperliquid.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://www.interactivebrokers.com&quot;&gt;Interactive Brokers&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;INTERACTIVE_BROKERS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Brokerage (multi-venue)&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stable-green&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/ib.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://okx.com&quot;&gt;OKX&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;OKX&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Crypto Exchange (CEX)&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/beta-yellow&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/okx.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://polymarket.com&quot;&gt;Polymarket&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;POLYMARKET&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Prediction Market (DEX)&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stable-green&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/polymarket.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://tardis.dev&quot;&gt;Tardis&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;TARDIS&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;Crypto Data Provider&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/stable-green&quot; alt=&quot;status&quot; /&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/docs/integrations/tardis.md&quot;&gt;Guide&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;ID&lt;/strong&gt;: The default client ID for the integrations adapter clients.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: The type of integration (often the venue type).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Status&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;building&lt;/code&gt;: Under construction and likely not in a usable state.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: Completed to a minimally working state and in a beta testing phase.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://nautilustrader.io/docs/latest/integrations/index.html&quot;&gt;Integrations&lt;/a&gt; documentation for further details.&lt;/p&gt; 
&lt;h2&gt;Versioning and releases&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;NautilusTrader is still under active development&lt;/strong&gt;. Some features may be incomplete, and while the API is becoming more stable, breaking changes can occur between releases. We strive to document these changes in the release notes on a &lt;strong&gt;best-effort basis&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;We aim to follow a &lt;strong&gt;bi-weekly release schedule&lt;/strong&gt;, though experimental or larger features may cause delays.&lt;/p&gt; 
&lt;h3&gt;Branches&lt;/h3&gt; 
&lt;p&gt;We aim to maintain a stable, passing build across all branches.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;master&lt;/code&gt;: Reflects the source code for the latest released version; recommended for production use.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt;: Daily snapshots of the &lt;code&gt;develop&lt;/code&gt; branch for early testing; merged at &lt;strong&gt;14:00 UTC&lt;/strong&gt; or on demand.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt;: Active development branch for contributors and feature work.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Our &lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md&quot;&gt;roadmap&lt;/a&gt; aims to achieve a &lt;strong&gt;stable API for version 2.x&lt;/strong&gt; (likely after the Rust port). Once this milestone is reached, we plan to implement a formal deprecation process for any API changes. This approach allows us to maintain a rapid development pace for now.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Precision mode&lt;/h2&gt; 
&lt;p&gt;NautilusTrader supports two precision modes for its core value types (&lt;code&gt;Price&lt;/code&gt;, &lt;code&gt;Quantity&lt;/code&gt;, &lt;code&gt;Money&lt;/code&gt;), which differ in their internal bit-width and maximum decimal precision.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;High-precision&lt;/strong&gt;: 128-bit integers with up to 16 decimals of precision, and a larger value range.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Standard-precision&lt;/strong&gt;: 64-bit integers with up to 9 decimals of precision, and a smaller value range.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;By default, the official Python wheels &lt;strong&gt;ship&lt;/strong&gt; in high-precision (128-bit) mode on Linux and macOS. On Windows, only standard-precision (64-bit) is available due to the lack of native 128-bit integer support. For the Rust crates, the default is standard-precision unless you explicitly enable the &lt;code&gt;high-precision&lt;/code&gt; feature flag.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://nautilustrader.io/docs/latest/getting_started/installation&quot;&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Rust feature flag&lt;/strong&gt;: To enable high-precision mode in Rust, add the &lt;code&gt;high-precision&lt;/code&gt; feature to your Cargo.toml:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;[dependencies]
nautilus_model = { version = &quot;*&quot;, features = [&quot;high-precision&quot;] }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;We recommend using the latest supported version of Python and installing &lt;a href=&quot;https://pypi.org/project/nautilus_trader/&quot;&gt;nautilus_trader&lt;/a&gt; inside a virtual environment to isolate dependencies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;There are two supported ways to install&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Pre-built binary wheel from PyPI &lt;em&gt;or&lt;/em&gt; the Nautech Systems package index.&lt;/li&gt; 
 &lt;li&gt;Build from source.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;We highly recommend installing using the &lt;a href=&quot;https://docs.astral.sh/uv&quot;&gt;uv&lt;/a&gt; package manager with a &quot;vanilla&quot; CPython.&lt;/p&gt; 
 &lt;p&gt;Conda and other Python distributions &lt;em&gt;may&lt;/em&gt; work but aren‚Äôt officially supported.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;From PyPI&lt;/h3&gt; 
&lt;p&gt;To install the latest binary wheel (or sdist package) from PyPI using Python&#39;s pip package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U nautilus_trader
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;From the Nautech Systems package index&lt;/h3&gt; 
&lt;p&gt;The Nautech Systems package index (&lt;code&gt;packages.nautechsystems.io&lt;/code&gt;) complies with &lt;a href=&quot;https://peps.python.org/pep-0503/&quot;&gt;PEP-503&lt;/a&gt; and hosts both stable and development binary wheels for &lt;code&gt;nautilus_trader&lt;/code&gt;. This enables users to install either the latest stable release or pre-release versions for testing.&lt;/p&gt; 
&lt;h4&gt;Stable wheels&lt;/h4&gt; 
&lt;p&gt;Stable wheels correspond to official releases of &lt;code&gt;nautilus_trader&lt;/code&gt; on PyPI, and use standard versioning.&lt;/p&gt; 
&lt;p&gt;To install the latest stable release:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Development wheels&lt;/h4&gt; 
&lt;p&gt;Development wheels are published from both the &lt;code&gt;nightly&lt;/code&gt; and &lt;code&gt;develop&lt;/code&gt; branches, allowing users to test features and fixes ahead of stable releases.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Wheels from the &lt;code&gt;develop&lt;/code&gt; branch are only built for the Linux x86_64 platform to save time and compute resources, while &lt;code&gt;nightly&lt;/code&gt; wheels support additional platforms as shown below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;left&quot;&gt;Platform&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;Nightly&lt;/th&gt; 
   &lt;th align=&quot;left&quot;&gt;Develop&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;‚úì&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;‚úì&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;Linux (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;‚úì&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;macOS (ARM64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;‚úì&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;left&quot;&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;‚úì&lt;/td&gt; 
   &lt;td align=&quot;left&quot;&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;This process also helps preserve compute resources and ensures easy access to the exact binaries tested in CI pipelines, while adhering to &lt;a href=&quot;https://peps.python.org/pep-0440/&quot;&gt;PEP-440&lt;/a&gt; versioning standards:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; wheels use the version format &lt;code&gt;dev{date}+{build_number}&lt;/code&gt; (e.g., &lt;code&gt;1.208.0.dev20241212+7001&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; wheels use the version format &lt;code&gt;a{date}&lt;/code&gt; (alpha) (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;We do not recommend using development wheels in production environments, such as live trading controlling real capital.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Installation commands&lt;/h4&gt; 
&lt;p&gt;By default, pip will install the latest stable release. Adding the &lt;code&gt;--pre&lt;/code&gt; flag ensures that pre-release versions, including development wheels, are considered.&lt;/p&gt; 
&lt;p&gt;To install the latest available pre-release (including development wheels):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To install a specific development wheel (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt; for December 12, 2024):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install nautilus_trader==1.208.0a20241212 --index-url=https://packages.nautechsystems.io/simple
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Available versions&lt;/h4&gt; 
&lt;p&gt;You can view all available versions of &lt;code&gt;nautilus_trader&lt;/code&gt; on the &lt;a href=&quot;https://packages.nautechsystems.io/simple/nautilus-trader/index.html&quot;&gt;package index&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To programmatically fetch and list available versions:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -s https://packages.nautechsystems.io/simple/nautilus-trader/index.html | grep -oP &#39;(?&amp;lt;=&amp;lt;a href=&quot;)[^&quot;]+(?=&quot;)&#39; | awk -F&#39;#&#39; &#39;{print $1}&#39; | sort
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Branch updates&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): Build and publish continuously with every merged commit.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): Build and publish daily when we automatically merge the &lt;code&gt;develop&lt;/code&gt; branch at &lt;strong&gt;14:00 UTC&lt;/strong&gt; (if there are changes).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Retention policies&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): We retain only the most recent wheel build.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): We retain only the 10 most recent wheel builds.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;From Source&lt;/h3&gt; 
&lt;p&gt;It&#39;s possible to install from source using pip if you first install the build dependencies as specified in the &lt;code&gt;pyproject.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href=&quot;https://rustup.rs/&quot;&gt;rustup&lt;/a&gt; (the Rust toolchain installer):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl https://sh.rustup.rs -sSf | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Download and install &lt;a href=&quot;https://win.rustup.rs/x86_64&quot;&gt;&lt;code&gt;rustup-init.exe&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;Install &quot;Desktop development with C++&quot; with &lt;a href=&quot;https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16&quot;&gt;Build Tools for Visual Studio 2019&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;rustc --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;cargo&lt;/code&gt; in the current shell:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux and macOS:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;source $HOME/.cargo/env
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Start a new PowerShell&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install &lt;a href=&quot;https://clang.llvm.org/&quot;&gt;clang&lt;/a&gt; (a C language frontend for LLVM):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Linux:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt-get install clang
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows:&lt;/p&gt; 
    &lt;ol&gt; 
     &lt;li&gt; &lt;p&gt;Add Clang to your &lt;a href=&quot;https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16&quot;&gt;Build Tools for Visual Studio 2019&lt;/a&gt;:&lt;/p&gt; 
      &lt;ul&gt; 
       &lt;li&gt;Start | Visual Studio Installer | Modify | C++ Clang tools for Windows (12.0.0 - x64‚Ä¶) = checked | Modify&lt;/li&gt; 
      &lt;/ul&gt; &lt;/li&gt; 
     &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;clang&lt;/code&gt; in the current shell:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-powershell&quot;&gt;[System.Environment]::SetEnvironmentVariable(&#39;path&#39;, &quot;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\Llvm\x64\bin\;&quot; + $env:Path,&quot;User&quot;)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
    &lt;/ol&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Verify (any system): from a terminal session run: &lt;code&gt;clang --version&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install uv (see the &lt;a href=&quot;https://docs.astral.sh/uv/getting-started/installation&quot;&gt;uv installation guide&lt;/a&gt; for more details):&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the source with &lt;code&gt;git&lt;/code&gt;, and install from the project&#39;s root directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone --branch develop --depth 1 https://github.com/nautechsystems/nautilus_trader
cd nautilus_trader
uv sync --all-extras
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;The &lt;code&gt;--depth 1&lt;/code&gt; flag fetches just the latest commit for a faster, lightweight clone.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ol start=&quot;6&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Set environment variables for PyO3 compilation (Linux and macOS only):&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Set the library path for the Python interpreter (in this case Python 3.13.4)
export LD_LIBRARY_PATH=&quot;$HOME/.local/share/uv/python/cpython-3.13.4-linux-x86_64-gnu/lib:$LD_LIBRARY_PATH&quot;

# Set the Python executable path for PyO3
export PYO3_PYTHON=$(pwd)/.venv/bin/python
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Adjust the Python version and architecture in the &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to match your system. Use &lt;code&gt;uv python list&lt;/code&gt; to find the exact path for your Python installation.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://nautilustrader.io/docs/latest/getting_started/installation&quot;&gt;Installation Guide&lt;/a&gt; for other options and further details.&lt;/p&gt; 
&lt;h2&gt;Redis&lt;/h2&gt; 
&lt;p&gt;Using &lt;a href=&quot;https://redis.io&quot;&gt;Redis&lt;/a&gt; with NautilusTrader is &lt;strong&gt;optional&lt;/strong&gt; and only required if configured as the backend for a &lt;a href=&quot;https://nautilustrader.io/docs/latest/concepts/cache&quot;&gt;cache&lt;/a&gt; database or &lt;a href=&quot;https://nautilustrader.io/docs/latest/concepts/message_bus&quot;&gt;message bus&lt;/a&gt;. See the &lt;strong&gt;Redis&lt;/strong&gt; section of the &lt;a href=&quot;https://nautilustrader.io/docs/latest/getting_started/installation#redis&quot;&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; 
&lt;h2&gt;Makefile&lt;/h2&gt; 
&lt;p&gt;A &lt;code&gt;Makefile&lt;/code&gt; is provided to automate most installation and build tasks for development. Some of the targets include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;make install&lt;/code&gt;: Installs in &lt;code&gt;release&lt;/code&gt; build mode with all dependency groups and extras.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-debug&lt;/code&gt;: Same as &lt;code&gt;make install&lt;/code&gt; but with &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make install-just-deps&lt;/code&gt;: Installs just the &lt;code&gt;main&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; dependencies (does not install package).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build&lt;/code&gt;: Runs the build script in &lt;code&gt;release&lt;/code&gt; build mode (default).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-debug&lt;/code&gt;: Runs the build script in &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;release&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make build-wheel-debug&lt;/code&gt;: Runs uv build with a wheel format in &lt;code&gt;debug&lt;/code&gt; mode.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make cargo-test&lt;/code&gt;: Runs all Rust crate tests using &lt;code&gt;cargo-nextest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;: Deletes all build results, such as &lt;code&gt;.so&lt;/code&gt; or &lt;code&gt;.dll&lt;/code&gt; files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make distclean&lt;/code&gt;: &lt;strong&gt;CAUTION&lt;/strong&gt; Removes all artifacts not in the git index from the repository. This includes source files which have not been &lt;code&gt;git add&lt;/code&gt;ed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make docs&lt;/code&gt;: Builds the documentation HTML using Sphinx.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pre-commit&lt;/code&gt;: Runs the pre-commit checks over all files.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make ruff&lt;/code&gt;: Runs ruff over all files using the &lt;code&gt;pyproject.toml&lt;/code&gt; config (with autofix).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make pytest&lt;/code&gt;: Runs all tests with &lt;code&gt;pytest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;make test-performance&lt;/code&gt;: Runs performance tests with &lt;a href=&quot;https://codspeed.io&quot;&gt;codspeed&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make help&lt;/code&gt; for documentation on all available make targets.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;See the &lt;a href=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/crates/infrastructure/TESTS.md&quot;&gt;crates/infrastructure/TESTS.md&lt;/a&gt; file for running the infrastructure integration tests.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Indicators and strategies can be developed in both Python and Cython. For performance and latency-sensitive applications, we recommend using Cython. Below are some examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/indicators/ema_python.py&quot;&gt;indicator&lt;/a&gt; example written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/indicators/&quot;&gt;indicator&lt;/a&gt; examples written in Cython.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/strategies/&quot;&gt;strategy&lt;/a&gt; examples written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/examples/backtest/&quot;&gt;backtest&lt;/a&gt; examples using a &lt;code&gt;BacktestEngine&lt;/code&gt; directly.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Docker&lt;/h2&gt; 
&lt;p&gt;Docker containers are built using the base image &lt;code&gt;python:3.12-slim&lt;/code&gt; with the following variant tags:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:latest&lt;/code&gt; has the latest release version installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;nautilus_trader:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:latest&lt;/code&gt; has the latest release version installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jupyterlab:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can pull the container images as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker pull ghcr.io/nautechsystems/&amp;lt;image_variant_tag&amp;gt; --platform linux/amd64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can launch the backtest example container by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker pull ghcr.io/nautechsystems/jupyterlab:nightly --platform linux/amd64
docker run -p 8888:8888 ghcr.io/nautechsystems/jupyterlab:nightly
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then open your browser at the following address:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;http://127.0.0.1:8888/lab
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader currently exceeds the rate limit for Jupyter notebook logging (stdout output). Therefore, we set the &lt;code&gt;log_level&lt;/code&gt; to &lt;code&gt;ERROR&lt;/code&gt; in the examples. Lowering this level to see more logging will cause the notebook to hang during cell execution. We are investigating a fix that may involve either raising the configured rate limits for Jupyter or throttling the log flushing from Nautilus.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/jupyterlab/jupyterlab/issues/12845&quot;&gt;https://github.com/jupyterlab/jupyterlab/issues/12845&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/deshaw/jupyterlab-limit-output&quot;&gt;https://github.com/deshaw/jupyterlab-limit-output&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;We aim to provide the most pleasant developer experience possible for this hybrid codebase of Python, Cython and Rust. See the &lt;a href=&quot;https://nautilustrader.io/docs/latest/developer_guide/index.html&quot;&gt;Developer Guide&lt;/a&gt; for helpful information.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run &lt;code&gt;make build-debug&lt;/code&gt; to compile after changes to Rust or Cython code for the most efficient development workflow.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Testing with Rust&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://nexte.st&quot;&gt;cargo-nextest&lt;/a&gt; is the standard Rust test runner for NautilusTrader. Its key benefit is isolating each test in its own process, ensuring test reliability by avoiding interference.&lt;/p&gt; 
&lt;p&gt;You can install cargo-nextest by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cargo install cargo-nextest
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;Run Rust tests with &lt;code&gt;make cargo-test&lt;/code&gt;, which uses &lt;strong&gt;cargo-nextest&lt;/strong&gt; with an efficient profile.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Thank you for considering contributing to NautilusTrader! We welcome any and all help to improve the project. If you have an idea for an enhancement or a bug fix, the first step is to open an &lt;a href=&quot;https://github.com/nautechsystems/nautilus_trader/issues&quot;&gt;issue&lt;/a&gt; on GitHub to discuss it with the team. This helps to ensure that your contribution will be well-aligned with the goals of the project and avoids duplication of effort.&lt;/p&gt; 
&lt;p&gt;Before getting started, be sure to review the &lt;a href=&quot;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md#open-source-scope&quot;&gt;open-source scope&lt;/a&gt; outlined in the project‚Äôs roadmap to understand what‚Äôs in and out of scope.&lt;/p&gt; 
&lt;p&gt;Once you&#39;re ready to start working on your contribution, make sure to follow the guidelines outlined in the &lt;a href=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; file. This includes signing a Contributor License Agreement (CLA) to ensure that your contributions can be included in the project.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;Pull requests should target the &lt;code&gt;develop&lt;/code&gt; branch (the default branch). This is where new features and improvements are integrated before release.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Thank you again for your interest in NautilusTrader! We look forward to reviewing your contributions and working with you to improve the project.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Join our community of users and contributors on &lt;a href=&quot;https://discord.gg/NautilusTrader&quot;&gt;Discord&lt;/a&gt; to chat and stay up-to-date with the latest announcements and features of NautilusTrader. Whether you&#39;re a developer looking to contribute or just want to learn more about the platform, all are welcome on our Discord server.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;NautilusTrader does not issue, promote, or endorse any cryptocurrency tokens. Any claims or communications suggesting otherwise are unauthorized and false.&lt;/p&gt; 
 &lt;p&gt;All official updates and communications from NautilusTrader will be shared exclusively through &lt;a href=&quot;https://nautilustrader.io&quot;&gt;https://nautilustrader.io&lt;/a&gt;, our &lt;a href=&quot;https://discord.gg/NautilusTrader&quot;&gt;Discord server&lt;/a&gt;, or our X (Twitter) account: &lt;a href=&quot;https://x.com/NautilusTrader&quot;&gt;@NautilusTrader&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;If you encounter any suspicious activity, please report it to the appropriate platform and contact us at &lt;a href=&quot;mailto:info@nautechsystems.io&quot;&gt;info@nautechsystems.io&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The source code for NautilusTrader is available on GitHub under the &lt;a href=&quot;https://www.gnu.org/licenses/lgpl-3.0.en.html&quot;&gt;GNU Lesser General Public License v3.0&lt;/a&gt;. Contributions to the project are welcome and require the completion of a standard &lt;a href=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/CLA.md&quot;&gt;Contributor License Agreement (CLA)&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;NautilusTrader‚Ñ¢ is developed and maintained by Nautech Systems, a technology company specializing in the development of high-performance trading systems. For more information, visit &lt;a href=&quot;https://nautilustrader.io&quot;&gt;https://nautilustrader.io&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;¬© 2015-2025 Nautech Systems Pty Ltd. All rights reserved.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ns-logo.png&quot; alt=&quot;nautechsystems&quot; title=&quot;nautechsystems&quot; /&gt; &lt;img src=&quot;https://github.com/nautechsystems/nautilus_trader/raw/develop/assets/ferris.png&quot; width=&quot;128&quot; /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>actualbudget/actual</title>
      <link>https://github.com/actualbudget/actual</link>
      <description>&lt;p&gt;A local-first personal finance app&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/actualbudget/actual/master/demo.png&quot; alt=&quot;Actualbudget&quot; /&gt; &lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Actual is a local-first personal finance tool. It is 100% free and open-source, written in NodeJS, it has a synchronization element so that all your changes can move between devices without any heavy lifting.&lt;/p&gt; 
&lt;p&gt;If you are interested in contributing, or want to know how development works, see our &lt;a href=&quot;https://actualbudget.org/docs/contributing/&quot;&gt;contributing&lt;/a&gt; document we would love to have you.&lt;/p&gt; 
&lt;p&gt;Want to say thanks? Click the ‚≠ê at the top of the page.&lt;/p&gt; 
&lt;h2&gt;Key Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Actual &lt;a href=&quot;https://discord.gg/pRYNYr4W5A&quot;&gt;discord&lt;/a&gt; community.&lt;/li&gt; 
 &lt;li&gt;Actual &lt;a href=&quot;https://actualbudget.org/docs&quot;&gt;Community Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://actualbudget.org/docs/faq&quot;&gt;Frequently asked questions&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;There are four ways to deploy Actual:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;One-click deployment &lt;a href=&quot;https://www.pikapods.com/pods?run=actual&quot;&gt;via PikaPods&lt;/a&gt; (~1.40 $/month) - recommended for non-technical users&lt;/li&gt; 
 &lt;li&gt;Managed hosting &lt;a href=&quot;https://actualbudget.org/docs/install/fly&quot;&gt;via Fly.io&lt;/a&gt; (~1.50 $/month)&lt;/li&gt; 
 &lt;li&gt;Self-hosted by using &lt;a href=&quot;https://actualbudget.org/docs/install/docker&quot;&gt;a Docker image&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Local-only apps - &lt;a href=&quot;https://actualbudget.org/download/&quot;&gt;downloadable Windows, Mac and Linux apps&lt;/a&gt; you can run on your device&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Learn more in the &lt;a href=&quot;https://actualbudget.org/docs/install/&quot;&gt;installation instructions docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Ready to Start Budgeting?&lt;/h2&gt; 
&lt;p&gt;Read about &lt;a href=&quot;https://actualbudget.org/docs/getting-started/envelope-budgeting&quot;&gt;Envelope budgeting&lt;/a&gt; to know more about the idea behind Actual Budget.&lt;/p&gt; 
&lt;h3&gt;Are you new to budgeting or want to start fresh?&lt;/h3&gt; 
&lt;p&gt;Check out the community&#39;s &lt;a href=&quot;https://actualbudget.org/docs/getting-started/starting-fresh&quot;&gt;Starting Fresh&lt;/a&gt; guide so you can quickly get up and running!&lt;/p&gt; 
&lt;h3&gt;Are you migrating from other budgeting apps?&lt;/h3&gt; 
&lt;p&gt;Check out the community&#39;s &lt;a href=&quot;https://actualbudget.org/docs/migration/&quot;&gt;Migration&lt;/a&gt; guide to start jumping on the Actual Budget train!&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;We have a wide range of documentation on how to use Actual, this is all available in our &lt;a href=&quot;https://actualbudget.org/docs&quot;&gt;Community Documentation&lt;/a&gt;, this includes topics on Budgeting, Account Management, Tips &amp;amp; Tricks and some documentation for developers.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Actual is a community driven product. Learn more about &lt;a href=&quot;https://actualbudget.org/docs/contributing/&quot;&gt;contributing to Actual&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Code structure&lt;/h3&gt; 
&lt;p&gt;The Actual app is split up into a few packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;loot-core - The core application that runs on any platform&lt;/li&gt; 
 &lt;li&gt;desktop-client - The desktop UI&lt;/li&gt; 
 &lt;li&gt;desktop-electron - The desktop app&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;More information on the project structure is available in our &lt;a href=&quot;https://actualbudget.org/docs/contributing/project-details&quot;&gt;community documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Feature Requests&lt;/h3&gt; 
&lt;p&gt;Current feature requests can be seen &lt;a href=&quot;https://github.com/actualbudget/actual/issues?q=is%3Aissue+label%3A%22needs+votes%22+sort%3Areactions-%2B1-desc&quot;&gt;here&lt;/a&gt;. Vote for your favorite requests by reacting &lt;span&gt;üëç&lt;/span&gt; to the top comment of the request.&lt;/p&gt; 
&lt;p&gt;To add new feature requests, open a new Issue of the &quot;Feature Request&quot; type.&lt;/p&gt; 
&lt;h3&gt;Translation&lt;/h3&gt; 
&lt;p&gt;Make Actual Budget accessible to more people by helping with the &lt;a href=&quot;https://actualbudget.org/docs/contributing/i18n/&quot;&gt;Internationalization&lt;/a&gt; of Actual. We are using a crowd sourcing tool to manage the translations, see our &lt;a href=&quot;https://hosted.weblate.org/projects/actualbudget/&quot;&gt;Weblate Project&lt;/a&gt;. Weblate proudly supports open-source software projects through their &lt;a href=&quot;https://weblate.org/en/hosting/#libre&quot;&gt;Libre plan&lt;/a&gt;.&lt;/p&gt; 
&lt;a href=&quot;https://hosted.weblate.org/engage/actualbudget/&quot;&gt; &lt;img src=&quot;https://hosted.weblate.org/widget/actualbudget/actual/287x66-grey.png&quot; alt=&quot;Translation status&quot; /&gt; &lt;/a&gt; 
&lt;h2&gt;Repo Activity&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://repobeats.axiom.co/api/embed/e20537dd8b74956f86736726ccfbc6f0565bec22.svg?sanitize=true&quot; alt=&quot;Alt&quot; title=&quot;Repobeats analytics image&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Thanks to our wonderful sponsors who make Actual Budget possible!&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.netlify.com&quot;&gt; &lt;img src=&quot;https://www.netlify.com/v3/img/components/netlify-color-accent.svg?sanitize=true&quot; alt=&quot;Deploys by Netlify&quot; /&gt; &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/codex</title>
      <link>https://github.com/openai/codex</link>
      <description>&lt;p&gt;Lightweight coding agent that runs in your terminal&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&quot;center&quot;&gt;OpenAI Codex CLI&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install codex&lt;/code&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer.&lt;br /&gt;If you are looking for the &lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, &lt;strong&gt;Codex Web&lt;/strong&gt;, see &lt;a href=&quot;https://chatgpt.com/codex&quot;&gt;chatgpt.com/codex&lt;/a&gt;.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-splash.png&quot; alt=&quot;Codex CLI splash&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Table of contents&lt;/strong&gt;&lt;/summary&gt; 
 &lt;!-- Begin ToC --&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#quickstart&quot;&gt;Quickstart&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#installing-and-running-codex-cli&quot;&gt;Installing and running Codex CLI&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#using-codex-with-your-chatgpt-plan&quot;&gt;Using Codex with your ChatGPT plan&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#connecting-on-a-headless-machine&quot;&gt;Connecting on a &quot;Headless&quot; Machine&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#authenticate-locally-and-copy-your-credentials-to-the-headless-machine&quot;&gt;Authenticate locally and copy your credentials to the &quot;headless&quot; machine&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#connecting-through-vps-or-remote&quot;&gt;Connecting through VPS or remote&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#usage-based-billing-alternative-use-an-openai-api-key&quot;&gt;Usage-based billing alternative: Use an OpenAI API key&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#choosing-codexs-level-of-autonomy&quot;&gt;Choosing Codex&#39;s level of autonomy&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#1-readwrite&quot;&gt;&lt;strong&gt;1. Read/write&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#2-read-only&quot;&gt;&lt;strong&gt;2. Read-only&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#3-advanced-configuration&quot;&gt;&lt;strong&gt;3. Advanced configuration&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#can-i-run-without-any-approvals&quot;&gt;Can I run without ANY approvals?&lt;/a&gt;&lt;/li&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#fine-tuning-in-configtoml&quot;&gt;Fine-tuning in &lt;code&gt;config.toml&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#example-prompts&quot;&gt;Example prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#running-with-a-prompt-as-input&quot;&gt;Running with a prompt as input&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#using-open-source-models&quot;&gt;Using Open Source Models&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#platform-sandboxing-details&quot;&gt;Platform sandboxing details&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#experimental-technology-disclaimer&quot;&gt;Experimental technology disclaimer&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#system-requirements&quot;&gt;System requirements&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#cli-reference&quot;&gt;CLI reference&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#memory--project-docs&quot;&gt;Memory &amp;amp; project docs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#non-interactive--ci-mode&quot;&gt;Non-interactive / CI mode&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#model-context-protocol-mcp&quot;&gt;Model Context Protocol (MCP)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#tracing--verbose-logging&quot;&gt;Tracing / verbose logging&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#dotslash&quot;&gt;DotSlash&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#configuration&quot;&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#faq&quot;&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#zero-data-retention-zdr-usage&quot;&gt;Zero data retention (ZDR) usage&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#codex-open-source-fund&quot;&gt;Codex open source fund&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#contributing&quot;&gt;Contributing&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#development-workflow&quot;&gt;Development workflow&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#writing-high-impact-code-changes&quot;&gt;Writing high-impact code changes&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#opening-a-pull-request&quot;&gt;Opening a pull request&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#review-process&quot;&gt;Review process&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#community-values&quot;&gt;Community values&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#getting-help&quot;&gt;Getting help&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#contributor-license-agreement-cla&quot;&gt;Contributor license agreement (CLA)&lt;/a&gt; 
     &lt;ul&gt; 
      &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#quick-fixes&quot;&gt;Quick fixes&lt;/a&gt;&lt;/li&gt; 
     &lt;/ul&gt; &lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#releasing-codex&quot;&gt;Releasing &lt;code&gt;codex&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#security--responsible-ai&quot;&gt;Security &amp;amp; responsible AI&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;!-- End ToC --&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Installing and running Codex CLI&lt;/h3&gt; 
&lt;p&gt;Install globally with your preferred package manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;npm install -g @openai/codex  # Alternatively: `brew install codex`
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run &lt;code&gt;codex&lt;/code&gt; to get started:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can also go to the &lt;a href=&quot;https://github.com/openai/codex/releases/latest&quot;&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt; 
 &lt;p&gt;Each GitHub Release contains many executables, but in practice, you likely want one of these:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS 
   &lt;ul&gt; 
    &lt;li&gt;Apple Silicon/arm64: &lt;code&gt;codex-aarch64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;x86_64 (older Mac hardware): &lt;code&gt;codex-x86_64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Linux 
   &lt;ul&gt; 
    &lt;li&gt;x86_64: &lt;code&gt;codex-x86_64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;arm64: &lt;code&gt;codex-aarch64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Each archive contains a single entry with the platform baked into the name (e.g., &lt;code&gt;codex-x86_64-unknown-linux-musl&lt;/code&gt;), so you likely want to rename it to &lt;code&gt;codex&lt;/code&gt; after extracting it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Using Codex with your ChatGPT plan&lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-login.png&quot; alt=&quot;Codex CLI login&quot; width=&quot;50%&quot; /&gt; &lt;/p&gt; 
&lt;p&gt;Run &lt;code&gt;codex&lt;/code&gt; and select &lt;strong&gt;Sign in with ChatGPT&lt;/strong&gt;. You&#39;ll need a Plus, Pro, or Team ChatGPT account, and will get access to our latest models, including &lt;code&gt;gpt-5&lt;/code&gt;, at no extra cost to your plan. (Enterprise is coming soon.)&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Important: If you&#39;ve used the Codex CLI before, follow these steps to migrate from usage-based billing with your API key:&lt;/p&gt; 
 &lt;ol&gt; 
  &lt;li&gt;Update the CLI and ensure &lt;code&gt;codex --version&lt;/code&gt; is &lt;code&gt;0.20.0&lt;/code&gt; or later&lt;/li&gt; 
  &lt;li&gt;Delete &lt;code&gt;~/.codex/auth.json&lt;/code&gt; (this should be &lt;code&gt;C:\Users\USERNAME\.codex\auth.json&lt;/code&gt; on Windows)&lt;/li&gt; 
  &lt;li&gt;Run &lt;code&gt;codex login&lt;/code&gt; again&lt;/li&gt; 
 &lt;/ol&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;If you encounter problems with the login flow, please comment on &lt;a href=&quot;https://github.com/openai/codex/issues/1243&quot;&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Connecting on a &quot;Headless&quot; Machine&lt;/h3&gt; 
&lt;p&gt;Today, the login process entails running a server on &lt;code&gt;localhost:1455&lt;/code&gt;. If you are on a &quot;headless&quot; server, such as a Docker container or are &lt;code&gt;ssh&lt;/code&gt;&#39;d into a remote machine, loading &lt;code&gt;localhost:1455&lt;/code&gt; in the browser on your local machine will not automatically connect to the webserver running on the &lt;em&gt;headless&lt;/em&gt; machine, so you must use one of the following workarounds:&lt;/p&gt; 
&lt;h4&gt;Authenticate locally and copy your credentials to the &quot;headless&quot; machine&lt;/h4&gt; 
&lt;p&gt;The easiest solution is likely to run through the &lt;code&gt;codex login&lt;/code&gt; process on your local machine such that &lt;code&gt;localhost:1455&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; accessible in your web browser. When you complete the authentication process, an &lt;code&gt;auth.json&lt;/code&gt; file should be available at &lt;code&gt;$CODEX_HOME/auth.json&lt;/code&gt; (on Mac/Linux, &lt;code&gt;$CODEX_HOME&lt;/code&gt; defaults to &lt;code&gt;~/.codex&lt;/code&gt; whereas on Windows, it defaults to &lt;code&gt;%USERPROFILE%\.codex&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Because the &lt;code&gt;auth.json&lt;/code&gt; file is not tied to a specific host, once you complete the authentication flow locally, you can copy the &lt;code&gt;$CODEX_HOME/auth.json&lt;/code&gt; file to the headless machine and then &lt;code&gt;codex&lt;/code&gt; should &quot;just work&quot; on that machine. Note to copy a file to a Docker container, you can do:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# substitute MY_CONTAINER with the name or id of your Docker container:
CONTAINER_HOME=$(docker exec MY_CONTAINER printenv HOME)
docker exec MY_CONTAINER mkdir -p &quot;$CONTAINER_HOME/.codex&quot;
docker cp auth.json MY_CONTAINER:&quot;$CONTAINER_HOME/.codex/auth.json&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;whereas if you are &lt;code&gt;ssh&lt;/code&gt;&#39;d into a remote machine, you likely want to use &lt;a href=&quot;https://en.wikipedia.org/wiki/Secure_copy_protocol&quot;&gt;&lt;code&gt;scp&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ssh user@remote &#39;mkdir -p ~/.codex&#39;
scp ~/.codex/auth.json user@remote:~/.codex/auth.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or try this one-liner:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;ssh user@remote &#39;mkdir -p ~/.codex &amp;amp;&amp;amp; cat &amp;gt; ~/.codex/auth.json&#39; &amp;lt; ~/.codex/auth.json
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Connecting through VPS or remote&lt;/h4&gt; 
&lt;p&gt;If you run Codex on a remote machine (VPS/server) without a local browser, the login helper starts a server on &lt;code&gt;localhost:1455&lt;/code&gt; on the remote host. To complete login in your local browser, forward that port to your machine before starting the login flow:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# From your local machine
ssh -L 1455:localhost:1455 &amp;lt;user&amp;gt;@&amp;lt;remote-host&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, in that SSH session, run &lt;code&gt;codex&lt;/code&gt; and select &quot;Sign in with ChatGPT&quot;. When prompted, open the printed URL (it will be &lt;code&gt;http://localhost:1455/...&lt;/code&gt;) in your local browser. The traffic will be tunneled to the remote server.&lt;/p&gt; 
&lt;h3&gt;Usage-based billing alternative: Use an OpenAI API key&lt;/h3&gt; 
&lt;p&gt;If you prefer to pay-as-you-go, you can still authenticate with your OpenAI API key by setting it as an environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;export OPENAI_API_KEY=&quot;your-api-key-here&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Notes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;This command only sets the key for your current terminal session, which we recommend. To set it for all future sessions, you can also add the &lt;code&gt;export&lt;/code&gt; line to your shell&#39;s configuration file (e.g., &lt;code&gt;~/.zshrc&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;If you have signed in with ChatGPT, Codex will default to using your ChatGPT credits. If you wish to use your API key, use the &lt;code&gt;/logout&lt;/code&gt; command to clear your ChatGPT authentication.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Choosing Codex&#39;s level of autonomy&lt;/h3&gt; 
&lt;p&gt;We always recommend running Codex in its default sandbox that gives you strong guardrails around what the agent can do. The default sandbox prevents it from editing files outside its workspace, or from accessing the network.&lt;/p&gt; 
&lt;p&gt;When you launch Codex in a new folder, it detects whether the folder is version controlled and recommends one of two levels of autonomy:&lt;/p&gt; 
&lt;h4&gt;&lt;strong&gt;1. Read/write&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Codex can run commands and write files in the workspace without approval.&lt;/li&gt; 
 &lt;li&gt;To write files in other folders, access network, update git or perform other actions protected by the sandbox, Codex will need your permission.&lt;/li&gt; 
 &lt;li&gt;By default, the workspace includes the current directory, as well as temporary directories like &lt;code&gt;/tmp&lt;/code&gt;. You can see what directories are in the workspace with the &lt;code&gt;/status&lt;/code&gt; command. See the docs for how to customize this behavior.&lt;/li&gt; 
 &lt;li&gt;Advanced: You can manually specify this configuration by running &lt;code&gt;codex --sandbox workspace-write --ask-for-approval on-request&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;This is the recommended default for version-controlled folders.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;2. Read-only&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Codex can run read-only commands without approval.&lt;/li&gt; 
 &lt;li&gt;To edit files, access network, or perform other actions protected by the sandbox, Codex will need your permission.&lt;/li&gt; 
 &lt;li&gt;Advanced: You can manually specify this configuration by running &lt;code&gt;codex --sandbox read-only --ask-for-approval on-request&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;This is the recommended default non-version-controlled folders.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;&lt;strong&gt;3. Advanced configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Codex gives you fine-grained control over the sandbox with the &lt;code&gt;--sandbox&lt;/code&gt; option, and over when it requests approval with the &lt;code&gt;--ask-for-approval&lt;/code&gt; option. Run &lt;code&gt;codex help&lt;/code&gt; for more on these options.&lt;/p&gt; 
&lt;h4&gt;Can I run without ANY approvals?&lt;/h4&gt; 
&lt;p&gt;Yes, run codex non-interactively with &lt;code&gt;--ask-for-approval never&lt;/code&gt;. This option works with all &lt;code&gt;--sandbox&lt;/code&gt; options, so you still have full control over Codex&#39;s level of autonomy. It will make its best attempt with whatever contrainsts you provide. For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox read-only&lt;/code&gt; when you are running many agents to answer questions in parallel in the same workspace.&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox workspace-write&lt;/code&gt; when you want the agent to non-interactively take time to produce the best outcome, with strong guardrails around its behavior.&lt;/li&gt; 
 &lt;li&gt;Use &lt;code&gt;codex --ask-for-approval never --sandbox danger-full-access&lt;/code&gt; to dangerously give the agent full autonomy. Because this disables important safety mechanisms, we recommend against using this unless running Codex in an isolated environment.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Fine-tuning in &lt;code&gt;config.toml&lt;/code&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;# approval mode
approval_policy = &quot;untrusted&quot;
sandbox_mode    = &quot;read-only&quot;

# full-auto mode
approval_policy = &quot;on-request&quot;
sandbox_mode    = &quot;workspace-write&quot;

# Optional: allow network in workspace-write mode
[sandbox_workspace_write]
network_access = true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also save presets as &lt;strong&gt;profiles&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;[profiles.full_auto]
approval_policy = &quot;on-request&quot;
sandbox_mode    = &quot;workspace-write&quot;

[profiles.readonly_quiet]
approval_policy = &quot;never&quot;
sandbox_mode    = &quot;read-only&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Example prompts&lt;/h3&gt; 
&lt;p&gt;Below are a few bite-size examples you can copy-paste. Replace the text in quotes with your own task. See the &lt;a href=&quot;https://github.com/openai/codex/raw/main/codex-cli/examples/prompting_guide.md&quot;&gt;prompting guide&lt;/a&gt; for more tips and usage patterns.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;‚ú®&lt;/th&gt; 
   &lt;th&gt;What you type&lt;/th&gt; 
   &lt;th&gt;What happens&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;1&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex &quot;Refactor the Dashboard component to React Hooks&quot;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Codex rewrites the class component, runs &lt;code&gt;npm test&lt;/code&gt;, and shows the diff.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;2&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex &quot;Generate SQL migrations for adding a users table&quot;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Infers your ORM, creates migration files, and runs them in a sandboxed DB.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;3&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex &quot;Write unit tests for utils/date.ts&quot;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Generates tests, executes them, and iterates until they pass.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;4&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex &quot;Bulk-rename *.jpeg -&amp;gt; *.jpg with git mv&quot;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Safely renames files and updates imports/usages.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;5&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex &quot;Explain what this regex does: ^(?=.*[A-Z]).{8,}$&quot;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Outputs a step-by-step human explanation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;6&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex &quot;Carefully review this repo, and propose 3 high impact well-scoped PRs&quot;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Suggests impactful PRs in the current codebase.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;7&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex &quot;Look for vulnerabilities and create a security review report&quot;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Finds and explains security bugs.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Running with a prompt as input&lt;/h2&gt; 
&lt;p&gt;You can also run Codex CLI with a prompt as input:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;codex &quot;explain this codebase to me&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;codex --full-auto &quot;create the fanciest todo-list app&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;That&#39;s it - Codex will scaffold a file, run it inside a sandbox, install any missing dependencies, and show you the live result. Approve the changes and they&#39;ll be committed to your working directory.&lt;/p&gt; 
&lt;h2&gt;Using Open Source Models&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Use &lt;code&gt;--profile&lt;/code&gt; to use other models&lt;/strong&gt;&lt;/summary&gt; 
 &lt;p&gt;Codex also allows you to use other providers that support the OpenAI Chat Completions (or Responses) API.&lt;/p&gt; 
 &lt;p&gt;To do so, you must first define custom &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/config.md#model_providers&quot;&gt;providers&lt;/a&gt; in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. For example, the provider for a standard Ollama setup would be defined as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;[model_providers.ollama]
name = &quot;Ollama&quot;
base_url = &quot;http://localhost:11434/v1&quot;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;The &lt;code&gt;base_url&lt;/code&gt; will have &lt;code&gt;/chat/completions&lt;/code&gt; appended to it to build the full URL for the request.&lt;/p&gt; 
 &lt;p&gt;For providers that also require an &lt;code&gt;Authorization&lt;/code&gt; header of the form &lt;code&gt;Bearer: SECRET&lt;/code&gt;, an &lt;code&gt;env_key&lt;/code&gt; can be specified, which indicates the environment variable to read to use as the value of &lt;code&gt;SECRET&lt;/code&gt; when making a request:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;[model_providers.openrouter]
name = &quot;OpenRouter&quot;
base_url = &quot;https://openrouter.ai/api/v1&quot;
env_key = &quot;OPENROUTER_API_KEY&quot;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Providers that speak the Responses API are also supported by adding &lt;code&gt;wire_api = &quot;responses&quot;&lt;/code&gt; as part of the definition. Accessing OpenAI models via Azure is an example of such a provider, though it also requires specifying additional &lt;code&gt;query_params&lt;/code&gt; that need to be appended to the request URL:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;[model_providers.azure]
name = &quot;Azure&quot;
# Make sure you set the appropriate subdomain for this URL.
base_url = &quot;https://YOUR_PROJECT_NAME.openai.azure.com/openai&quot;
env_key = &quot;AZURE_OPENAI_API_KEY&quot;  # Or &quot;OPENAI_API_KEY&quot;, whichever you use.
# Newer versions appear to support the responses API, see https://github.com/openai/codex/pull/1321
query_params = { api-version = &quot;2025-04-01-preview&quot; }
wire_api = &quot;responses&quot;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Once you have defined a provider you wish to use, you can configure it as your default provider as follows:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;model_provider = &quot;azure&quot;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;[!TIP] If you find yourself experimenting with a variety of models and providers, then you likely want to invest in defining a &lt;em&gt;profile&lt;/em&gt; for each configuration like so:&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;[profiles.o3]
model_provider = &quot;azure&quot;
model = &quot;o3&quot;

[profiles.mistral]
model_provider = &quot;ollama&quot;
model = &quot;mistral&quot;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This way, you can specify one command-line argument (.e.g., &lt;code&gt;--profile o3&lt;/code&gt;, &lt;code&gt;--profile mistral&lt;/code&gt;) to override multiple settings together.&lt;/p&gt; 
&lt;/details&gt; 
&lt;p&gt;Codex can run fully locally against an OpenAI-compatible OSS host (like Ollama) using the &lt;code&gt;--oss&lt;/code&gt; flag:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Interactive UI: 
  &lt;ul&gt; 
   &lt;li&gt;codex --oss&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Non-interactive (programmatic) mode: 
  &lt;ul&gt; 
   &lt;li&gt;echo &quot;Refactor utils&quot; | codex exec --oss&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Model selection when using &lt;code&gt;--oss&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you omit &lt;code&gt;-m/--model&lt;/code&gt;, Codex defaults to -m gpt-oss:20b and will verify it exists locally (downloading if needed).&lt;/li&gt; 
 &lt;li&gt;To pick a different size, pass one of: 
  &lt;ul&gt; 
   &lt;li&gt;-m &quot;gpt-oss:20b&quot;&lt;/li&gt; 
   &lt;li&gt;-m &quot;gpt-oss:120b&quot;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Point Codex at your own OSS host:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;By default, &lt;code&gt;--oss&lt;/code&gt; talks to &lt;a href=&quot;http://localhost:11434/v1&quot;&gt;http://localhost:11434/v1&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;To use a different host, set one of these environment variables before running Codex: 
  &lt;ul&gt; 
   &lt;li&gt;CODEX_OSS_BASE_URL, for example: 
    &lt;ul&gt; 
     &lt;li&gt;CODEX_OSS_BASE_URL=&quot;&lt;a href=&quot;http://my-ollama.example.com:11434/v1&quot;&gt;http://my-ollama.example.com:11434/v1&lt;/a&gt;&quot; codex --oss -m gpt-oss:20b&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;or CODEX_OSS_PORT (when the host is localhost): 
    &lt;ul&gt; 
     &lt;li&gt;CODEX_OSS_PORT=11434 codex --oss&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Advanced: you can persist this in your config instead of environment variables by overriding the built-in &lt;code&gt;oss&lt;/code&gt; provider in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;[model_providers.oss]
name = &quot;Open Source&quot;
base_url = &quot;http://my-ollama.example.com:11434/v1&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Platform sandboxing details&lt;/h3&gt; 
&lt;p&gt;The mechanism Codex uses to implement the sandbox policy depends on your OS:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;macOS 12+&lt;/strong&gt; uses &lt;strong&gt;Apple Seatbelt&lt;/strong&gt; and runs commands using &lt;code&gt;sandbox-exec&lt;/code&gt; with a profile (&lt;code&gt;-p&lt;/code&gt;) that corresponds to the &lt;code&gt;--sandbox&lt;/code&gt; that was specified.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Linux&lt;/strong&gt; uses a combination of Landlock/seccomp APIs to enforce the &lt;code&gt;sandbox&lt;/code&gt; configuration.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that when running Linux in a containerized environment such as Docker, sandboxing may not work if the host/container configuration does not support the necessary Landlock/seccomp APIs. In such cases, we recommend configuring your Docker container so that it provides the sandbox guarantees you are looking for and then running &lt;code&gt;codex&lt;/code&gt; with &lt;code&gt;--sandbox danger-full-access&lt;/code&gt; (or, more simply, the &lt;code&gt;--dangerously-bypass-approvals-and-sandbox&lt;/code&gt; flag) within your container.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Experimental technology disclaimer&lt;/h2&gt; 
&lt;p&gt;Codex CLI is an experimental project under active development. It is not yet stable, may contain bugs, incomplete features, or undergo breaking changes. We&#39;re building it in the open with the community and welcome:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Bug reports&lt;/li&gt; 
 &lt;li&gt;Feature requests&lt;/li&gt; 
 &lt;li&gt;Pull requests&lt;/li&gt; 
 &lt;li&gt;Good vibes&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Help us improve by filing issues or submitting PRs (see the section below for how to contribute)!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;System requirements&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Requirement&lt;/th&gt; 
   &lt;th&gt;Details&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Operating systems&lt;/td&gt; 
   &lt;td&gt;macOS 12+, Ubuntu 20.04+/Debian 10+, or Windows 11 &lt;strong&gt;via WSL2&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Git (optional, recommended)&lt;/td&gt; 
   &lt;td&gt;2.23+ for built-in PR helpers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;RAM&lt;/td&gt; 
   &lt;td&gt;4-GB minimum (8-GB recommended)&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;CLI reference&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
   &lt;th&gt;Purpose&lt;/th&gt; 
   &lt;th&gt;Example&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Interactive TUI&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex &quot;...&quot;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for interactive TUI&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex &quot;fix lint errors&quot;&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;codex exec &quot;...&quot;&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Non-interactive &quot;automation mode&quot;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;codex exec &quot;explain utils.ts&quot;&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Key flags: &lt;code&gt;--model/-m&lt;/code&gt;, &lt;code&gt;--ask-for-approval/-a&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Memory &amp;amp; project docs&lt;/h2&gt; 
&lt;p&gt;You can give Codex extra instructions and guidance using &lt;code&gt;AGENTS.md&lt;/code&gt; files. Codex looks for &lt;code&gt;AGENTS.md&lt;/code&gt; files in the following places, and merges them top-down:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;~/.codex/AGENTS.md&lt;/code&gt; - personal global guidance&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AGENTS.md&lt;/code&gt; at repo root - shared project notes&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;AGENTS.md&lt;/code&gt; in the current working directory - sub-folder/feature specifics&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Non-interactive / CI mode&lt;/h2&gt; 
&lt;p&gt;Run Codex head-less in pipelines. Example GitHub Action step:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;- name: Update changelog via Codex
  run: |
    npm install -g @openai/codex
    export OPENAI_API_KEY=&quot;${{ secrets.OPENAI_KEY }}&quot;
    codex exec --full-auto &quot;update CHANGELOG for next release&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model Context Protocol (MCP)&lt;/h2&gt; 
&lt;p&gt;The Codex CLI can be configured to leverage MCP servers by defining an &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md#mcp_servers&quot;&gt;&lt;code&gt;mcp_servers&lt;/code&gt;&lt;/a&gt; section in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. It is intended to mirror how tools such as Claude and Cursor define &lt;code&gt;mcpServers&lt;/code&gt; in their respective JSON config files, though the Codex format is slightly different since it uses TOML rather than JSON, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;# IMPORTANT: the top-level key is `mcp_servers` rather than `mcpServers`.
[mcp_servers.server-name]
command = &quot;npx&quot;
args = [&quot;-y&quot;, &quot;mcp-server&quot;]
env = { &quot;API_KEY&quot; = &quot;value&quot; }
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] It is somewhat experimental, but the Codex CLI can also be run as an MCP &lt;em&gt;server&lt;/em&gt; via &lt;code&gt;codex mcp&lt;/code&gt;. If you launch it with an MCP client such as &lt;code&gt;npx @modelcontextprotocol/inspector codex mcp&lt;/code&gt; and send it a &lt;code&gt;tools/list&lt;/code&gt; request, you will see that there is only one tool, &lt;code&gt;codex&lt;/code&gt;, that accepts a grab-bag of inputs, including a catch-all &lt;code&gt;config&lt;/code&gt; map for anything you might want to override. Feel free to play around with it and provide feedback via GitHub issues.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Tracing / verbose logging&lt;/h2&gt; 
&lt;p&gt;Because Codex is written in Rust, it honors the &lt;code&gt;RUST_LOG&lt;/code&gt; environment variable to configure its logging behavior.&lt;/p&gt; 
&lt;p&gt;The TUI defaults to &lt;code&gt;RUST_LOG=codex_core=info,codex_tui=info&lt;/code&gt; and log messages are written to &lt;code&gt;~/.codex/log/codex-tui.log&lt;/code&gt;, so you can leave the following running in a separate terminal to monitor log messages as they are written:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;tail -F ~/.codex/log/codex-tui.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By comparison, the non-interactive mode (&lt;code&gt;codex exec&lt;/code&gt;) defaults to &lt;code&gt;RUST_LOG=error&lt;/code&gt;, but messages are printed inline, so there is no need to monitor a separate file.&lt;/p&gt; 
&lt;p&gt;See the Rust documentation on &lt;a href=&quot;https://docs.rs/env_logger/latest/env_logger/#enabling-logging&quot;&gt;&lt;code&gt;RUST_LOG&lt;/code&gt;&lt;/a&gt; for more information on the configuration options.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;DotSlash&lt;/h3&gt; 
&lt;p&gt;The GitHub Release also contains a &lt;a href=&quot;https://dotslash-cli.com/&quot;&gt;DotSlash&lt;/a&gt; file for the Codex CLI named &lt;code&gt;codex&lt;/code&gt;. Using a DotSlash file makes it possible to make a lightweight commit to source control to ensure all contributors use the same version of an executable, regardless of what platform they use for development.&lt;/p&gt;  
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;Build from source&lt;/strong&gt;&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Clone the repository and navigate to the root of the Cargo workspace.
git clone https://github.com/openai/codex.git
cd codex/codex-rs

# Install the Rust toolchain, if necessary.
curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
source &quot;$HOME/.cargo/env&quot;
rustup component add rustfmt
rustup component add clippy

# Build Codex.
cargo build

# Launch the TUI with a sample prompt.
cargo run --bin codex -- &quot;explain this codebase to me&quot;

# After making changes, ensure the code is clean.
cargo fmt -- --config imports_granularity=Item
cargo clippy --tests

# Run the tests.
cargo test
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;Codex supports a rich set of configuration options documented in &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md&quot;&gt;&lt;code&gt;codex-rs/config.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;By default, Codex loads its configuration from &lt;code&gt;~/.codex/config.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Though &lt;code&gt;--config&lt;/code&gt; can be used to set/override ad-hoc config values for individual invocations of &lt;code&gt;codex&lt;/code&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;details&gt; 
 &lt;summary&gt;OpenAI released a model called Codex in 2021 - is this related?&lt;/summary&gt; 
 &lt;p&gt;In 2021, OpenAI released Codex, an AI system designed to generate code from natural language prompts. That original Codex model was deprecated as of March 2023 and is separate from the CLI tool.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Which models are supported?&lt;/summary&gt; 
 &lt;p&gt;Any model available with &lt;a href=&quot;https://platform.openai.com/docs/api-reference/responses&quot;&gt;Responses API&lt;/a&gt;. The default is &lt;code&gt;o4-mini&lt;/code&gt;, but pass &lt;code&gt;--model gpt-4.1&lt;/code&gt; or set &lt;code&gt;model: gpt-4.1&lt;/code&gt; in your config file to override.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Why does &lt;code&gt;o3&lt;/code&gt; or &lt;code&gt;o4-mini&lt;/code&gt; not work for me?&lt;/summary&gt; 
 &lt;p&gt;It&#39;s possible that your &lt;a href=&quot;https://help.openai.com/en/articles/10910291-api-organization-verification&quot;&gt;API account needs to be verified&lt;/a&gt; in order to start streaming responses and seeing chain of thought summaries from the API. If you&#39;re still running into issues, please let us know!&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;How do I stop Codex from editing my files?&lt;/summary&gt; 
 &lt;p&gt;Codex runs model-generated commands in a sandbox. If a proposed command or file change doesn&#39;t look right, you can simply type &lt;strong&gt;n&lt;/strong&gt; to deny the command or give the model feedback.&lt;/p&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Does it work on Windows?&lt;/summary&gt; 
 &lt;p&gt;Not directly. It requires &lt;a href=&quot;https://learn.microsoft.com/en-us/windows/wsl/install&quot;&gt;Windows Subsystem for Linux (WSL2)&lt;/a&gt; - Codex has been tested on macOS and Linux with Node 22.&lt;/p&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Zero data retention (ZDR) usage&lt;/h2&gt; 
&lt;p&gt;Codex CLI &lt;strong&gt;does&lt;/strong&gt; support OpenAI organizations with &lt;a href=&quot;https://platform.openai.com/docs/guides/your-data#zero-data-retention&quot;&gt;Zero Data Retention (ZDR)&lt;/a&gt; enabled. If your OpenAI organization has Zero Data Retention enabled and you still encounter errors such as:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;OpenAI rejected the request. Error details: Status: 400, Code: unsupported_parameter, Type: invalid_request_error, Message: 400 Previous response cannot be used for this organization due to Zero Data Retention.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ensure you are running &lt;code&gt;codex&lt;/code&gt; with &lt;code&gt;--config disable_response_storage=true&lt;/code&gt; or add this line to &lt;code&gt;~/.codex/config.toml&lt;/code&gt; to avoid specifying the command line option each time:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-toml&quot;&gt;disable_response_storage = true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/codex-rs/config.md#disable_response_storage&quot;&gt;the configuration documentation on &lt;code&gt;disable_response_storage&lt;/code&gt;&lt;/a&gt; for details.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Codex open source fund&lt;/h2&gt; 
&lt;p&gt;We&#39;re excited to launch a &lt;strong&gt;$1 million initiative&lt;/strong&gt; supporting open source projects that use Codex CLI and other OpenAI models.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Grants are awarded up to &lt;strong&gt;$25,000&lt;/strong&gt; API credits.&lt;/li&gt; 
 &lt;li&gt;Applications are reviewed &lt;strong&gt;on a rolling basis&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Interested? &lt;a href=&quot;https://openai.com/form/codex-open-source-fund/&quot;&gt;Apply here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project is under active development and the code will likely change pretty significantly. We&#39;ll update this message once that&#39;s complete!&lt;/p&gt; 
&lt;p&gt;More broadly we welcome contributions - whether you are opening your very first pull request or you&#39;re a seasoned maintainer. At the same time we care about reliability and long-term maintainability, so the bar for merging code is intentionally &lt;strong&gt;high&lt;/strong&gt;. The guidelines below spell out what &quot;high-quality&quot; means in practice and should make the whole process transparent and friendly.&lt;/p&gt; 
&lt;h3&gt;Development workflow&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create a &lt;em&gt;topic branch&lt;/em&gt; from &lt;code&gt;main&lt;/code&gt; - e.g. &lt;code&gt;feat/interactive-prompt&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Keep your changes focused. Multiple unrelated fixes should be opened as separate PRs.&lt;/li&gt; 
 &lt;li&gt;Following the &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/#development-workflow&quot;&gt;development setup&lt;/a&gt; instructions above, ensure your change is free of lint warnings and test failures.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Writing high-impact code changes&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Start with an issue.&lt;/strong&gt; Open a new one or comment on an existing discussion so we can agree on the solution before code is written.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Add or update tests.&lt;/strong&gt; Every new feature or bug-fix should come with test coverage that fails before your change and passes afterwards. 100% coverage is not required, but aim for meaningful assertions.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Document behaviour.&lt;/strong&gt; If your change affects user-facing behaviour, update the README, inline help (&lt;code&gt;codex --help&lt;/code&gt;), or relevant example projects.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Keep commits atomic.&lt;/strong&gt; Each commit should compile and the tests should pass. This makes reviews and potential rollbacks easier.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Opening a pull request&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fill in the PR template (or include similar information) - &lt;strong&gt;What? Why? How?&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Run &lt;strong&gt;all&lt;/strong&gt; checks locally (&lt;code&gt;cargo test &amp;amp;&amp;amp; cargo clippy --tests &amp;amp;&amp;amp; cargo fmt -- --config imports_granularity=Item&lt;/code&gt;). CI failures that could have been caught locally slow down the process.&lt;/li&gt; 
 &lt;li&gt;Make sure your branch is up-to-date with &lt;code&gt;main&lt;/code&gt; and that you have resolved merge conflicts.&lt;/li&gt; 
 &lt;li&gt;Mark the PR as &lt;strong&gt;Ready for review&lt;/strong&gt; only when you believe it is in a merge-able state.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Review process&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;One maintainer will be assigned as a primary reviewer.&lt;/li&gt; 
 &lt;li&gt;We may ask for changes - please do not take this personally. We value the work, we just also value consistency and long-term maintainability.&lt;/li&gt; 
 &lt;li&gt;When there is consensus that the PR meets the bar, a maintainer will squash-and-merge.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Community values&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Be kind and inclusive.&lt;/strong&gt; Treat others with respect; we follow the &lt;a href=&quot;https://www.contributor-covenant.org/&quot;&gt;Contributor Covenant&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Assume good intent.&lt;/strong&gt; Written communication is hard - err on the side of generosity.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Teach &amp;amp; learn.&lt;/strong&gt; If you spot something confusing, open an issue or PR with improvements.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting help&lt;/h3&gt; 
&lt;p&gt;If you run into problems setting up the project, would like feedback on an idea, or just want to say &lt;em&gt;hi&lt;/em&gt; - please open a Discussion or jump into the relevant issue. We are happy to help.&lt;/p&gt; 
&lt;p&gt;Together we can make Codex CLI an incredible tool. &lt;strong&gt;Happy hacking!&lt;/strong&gt; &lt;span&gt;üöÄ&lt;/span&gt;&lt;/p&gt; 
&lt;h3&gt;Contributor license agreement (CLA)&lt;/h3&gt; 
&lt;p&gt;All contributors &lt;strong&gt;must&lt;/strong&gt; accept the CLA. The process is lightweight:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Open your pull request.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Paste the following comment (or reply &lt;code&gt;recheck&lt;/code&gt; if you&#39;ve signed before):&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;I have read the CLA Document and I hereby sign the CLA
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The CLA-Assistant bot records your signature in the repo and marks the status check as passed.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;No special Git commands, email attachments, or commit footers required.&lt;/p&gt; 
&lt;h4&gt;Quick fixes&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Scenario&lt;/th&gt; 
   &lt;th&gt;Command&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Amend last commit&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git commit --amend -s --no-edit &amp;amp;&amp;amp; git push -f&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The &lt;strong&gt;DCO check&lt;/strong&gt; blocks merges until every commit in the PR carries the footer (with squash this is just the one).&lt;/p&gt; 
&lt;h3&gt;Releasing &lt;code&gt;codex&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;For admins only.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Make sure you are on &lt;code&gt;main&lt;/code&gt; and have no local changes. Then run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;VERSION=0.2.0  # Can also be 0.2.0-alpha.1 or any valid Rust version.
./codex-rs/scripts/create_github_release.sh &quot;$VERSION&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will make a local commit on top of &lt;code&gt;main&lt;/code&gt; with &lt;code&gt;version&lt;/code&gt; set to &lt;code&gt;$VERSION&lt;/code&gt; in &lt;code&gt;codex-rs/Cargo.toml&lt;/code&gt; (note that on &lt;code&gt;main&lt;/code&gt;, we leave the version as &lt;code&gt;version = &quot;0.0.0&quot;&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;This will push the commit using the tag &lt;code&gt;rust-v${VERSION}&lt;/code&gt;, which in turn kicks off &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/.github/workflows/rust-release.yml&quot;&gt;the release workflow&lt;/a&gt;. This will create a new GitHub Release named &lt;code&gt;$VERSION&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If everything looks good in the generated GitHub Release, uncheck the &lt;strong&gt;pre-release&lt;/strong&gt; box so it is the latest release.&lt;/p&gt; 
&lt;p&gt;Create a PR to update &lt;a href=&quot;https://github.com/Homebrew/homebrew-core/raw/main/Formula/c/codex.rb&quot;&gt;&lt;code&gt;Formula/c/codex.rb&lt;/code&gt;&lt;/a&gt; on Homebrew.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Security &amp;amp; responsible AI&lt;/h2&gt; 
&lt;p&gt;Have you discovered a vulnerability or have concerns about model output? Please e-mail &lt;strong&gt;&lt;a href=&quot;mailto:security@openai.com&quot;&gt;security@openai.com&lt;/a&gt;&lt;/strong&gt; and we will respond promptly.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/LICENSE&quot;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>browserbase/stagehand</title>
      <link>https://github.com/browserbase/stagehand</link>
      <description>&lt;p&gt;The AI Browser Automation Framework&lt;/p&gt;&lt;hr&gt;&lt;div id=&quot;toc&quot; align=&quot;center&quot; style=&quot;margin-bottom: 0;&quot;&gt; 
 &lt;ul style=&quot;list-style: none; margin: 0; padding: 0;&quot;&gt; 
  &lt;a href=&quot;https://stagehand.dev&quot;&gt; 
   &lt;picture&gt; 
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;media/dark_logo.png&quot; /&gt; 
    &lt;img alt=&quot;Stagehand&quot; src=&quot;https://raw.githubusercontent.com/browserbase/stagehand/main/media/light_logo.png&quot; width=&quot;200&quot; style=&quot;margin-right: 30px;&quot; /&gt; 
   &lt;/picture&gt; &lt;/a&gt; 
 &lt;/ul&gt; 
&lt;/div&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;strong&gt;The AI Browser Automation Framework&lt;/strong&gt;&lt;br /&gt; &lt;a href=&quot;https://docs.stagehand.dev&quot;&gt;Read the Docs&lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/browserbase/stagehand/tree/main?tab=MIT-1-ov-file#MIT-1-ov-file&quot;&gt; 
  &lt;picture&gt; 
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;media/dark_license.svg&quot; /&gt; 
   &lt;img alt=&quot;MIT License&quot; src=&quot;https://raw.githubusercontent.com/browserbase/stagehand/main/media/light_license.svg?sanitize=true&quot; /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;a href=&quot;https://join.slack.com/t/stagehand-dev/shared_invite/zt-38khc8iv5-T2acb50_0OILUaX7lxeBOg&quot;&gt; 
  &lt;picture&gt; 
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;media/dark_slack.svg&quot; /&gt; 
   &lt;img alt=&quot;Slack Community&quot; src=&quot;https://raw.githubusercontent.com/browserbase/stagehand/main/media/light_slack.svg?sanitize=true&quot; /&gt; 
  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/12122&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12122&quot; alt=&quot;browserbase%2Fstagehand | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; If you&#39;re looking for the Python implementation, you can find it &lt;a href=&quot;https://github.com/browserbase/stagehand-python&quot;&gt; here&lt;/a&gt; &lt;/p&gt; 
&lt;div align=&quot;center&quot; style=&quot;display: flex; align-items: center; justify-content: center; gap: 4px; margin-bottom: 0;&quot;&gt; 
 &lt;b&gt;Vibe code&lt;/b&gt; 
 &lt;span style=&quot;font-size: 1.05em;&quot;&gt; Stagehand with &lt;/span&gt; 
 &lt;a href=&quot;https://director.ai&quot; style=&quot;display: flex; align-items: center;&quot;&gt; &lt;span&gt;Director&lt;/span&gt; &lt;/a&gt; 
 &lt;span&gt; &lt;/span&gt; 
 &lt;picture&gt; 
  &lt;img alt=&quot;Director&quot; src=&quot;https://raw.githubusercontent.com/browserbase/stagehand/main/media/director_icon.svg?sanitize=true&quot; width=&quot;25&quot; /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;h2&gt;Why Stagehand?&lt;/h2&gt; 
&lt;p&gt;Most existing browser automation tools either require you to write low-level code in a framework like Selenium, Playwright, or Puppeteer, or use high-level agents that can be unpredictable in production. By letting developers choose what to write in code vs. natural language, Stagehand is the natural choice for browser automations in production.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Choose when to write code vs. natural language&lt;/strong&gt;: use AI when you want to navigate unfamiliar pages, and use code (&lt;a href=&quot;https://playwright.dev/&quot;&gt;Playwright&lt;/a&gt;) when you know exactly what you want to do.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Preview and cache actions&lt;/strong&gt;: Stagehand lets you preview AI actions before running them, and also helps you easily cache repeatable actions to save time and tokens.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Computer use models with one line of code&lt;/strong&gt;: Stagehand lets you integrate SOTA computer use models from OpenAI and Anthropic into the browser with one line of code.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Example&lt;/h2&gt; 
&lt;p&gt;Here&#39;s how to build a sample browser automation with Stagehand:&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;div style=&quot;max-width:300px;&quot;&gt; 
  &lt;img src=&quot;https://raw.githubusercontent.com/browserbase/stagehand/main/media/github_demo.gif&quot; alt=&quot;See Stagehand in Action&quot; /&gt; 
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;pre&gt;&lt;code class=&quot;language-typescript&quot;&gt;// Use Playwright functions on the page object
const page = stagehand.page;
await page.goto(&quot;https://github.com/browserbase&quot;);

// Use act() to execute individual actions
await page.act(&quot;click on the stagehand repo&quot;);

// Use Computer Use agents for larger actions
const agent = stagehand.agent({
    provider: &quot;openai&quot;,
    model: &quot;computer-use-preview&quot;,
});
await agent.execute(&quot;Get to the latest PR&quot;);

// Use extract() to read data from the page
const { author, title } = await page.extract({
  instruction: &quot;extract the author and title of the PR&quot;,
  schema: z.object({
    author: z.string().describe(&quot;The username of the PR author&quot;),
    title: z.string().describe(&quot;The title of the PR&quot;),
  }),
});
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href=&quot;https://docs.stagehand.dev&quot;&gt;docs.stagehand.dev&lt;/a&gt; to view the full documentation.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Start with Stagehand with one line of code, or check out our &lt;a href=&quot;https://docs.stagehand.dev/get_started/quickstart&quot;&gt;Quickstart Guide&lt;/a&gt; for more information:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;npx create-browser-app
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://www.loom.com/share/f5107f86d8c94fa0a8b4b1e89740f7a7&quot;&gt; &lt;p&gt;Watch Anirudh demo create-browser-app to create a Stagehand project!&lt;/p&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://www.loom.com/share/f5107f86d8c94fa0a8b4b1e89740f7a7&quot;&gt; &lt;img style=&quot;max-width:300px;&quot; src=&quot;https://cdn.loom.com/sessions/thumbnails/f5107f86d8c94fa0a8b4b1e89740f7a7-ec3f428b6775ceeb-full-play.gif&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Build and Run from Source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/browserbase/stagehand.git
cd stagehand
pnpm install
pnpm playwright install
pnpm run build
pnpm run example # run the blank script at ./examples/example.ts
pnpm run example 2048 # run the 2048 example at ./examples/2048.ts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Stagehand is best when you have an API key for an LLM provider and Browserbase credentials. To add these to your project, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cp .env.example .env
nano .env # Edit the .env file to add API keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;br /&gt; We highly value contributions to Stagehand! For questions or support, please join our &lt;a href=&quot;https://join.slack.com/t/stagehand-dev/shared_invite/zt-38khc8iv5-T2acb50_0OILUaX7lxeBOg&quot;&gt;Slack community&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;At a high level, we&#39;re focused on improving reliability, speed, and cost in that order of priority. If you&#39;re interested in contributing, we strongly recommend reaching out to &lt;a href=&quot;https://x.com/miguel_gonzf&quot;&gt;Miguel Gonzalez&lt;/a&gt; or &lt;a href=&quot;https://x.com/pk_iv&quot;&gt;Paul Klein&lt;/a&gt; in our &lt;a href=&quot;https://join.slack.com/t/stagehand-dev/shared_invite/zt-38khc8iv5-T2acb50_0OILUaX7lxeBOg&quot;&gt;Slack community&lt;/a&gt; before starting to ensure that your contribution aligns with our goals.&lt;/p&gt; 
&lt;p&gt;For more information, please see our &lt;a href=&quot;https://docs.stagehand.dev/examples/contributing&quot;&gt;Contributing Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This project heavily relies on &lt;a href=&quot;https://playwright.dev/&quot;&gt;Playwright&lt;/a&gt; as a resilient backbone to automate the web. It also would not be possible without the awesome techniques and discoveries made by &lt;a href=&quot;https://github.com/reworkd/tarsier&quot;&gt;tarsier&lt;/a&gt;, &lt;a href=&quot;https://github.com/jbeoris/gemini-zod&quot;&gt;gemini-zod&lt;/a&gt;, and &lt;a href=&quot;https://github.com/normal-computing/fuji-web&quot;&gt;fuji-web&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We&#39;d like to thank the following people for their major contributions to Stagehand:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pkiv&quot;&gt;Paul Klein&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kamath&quot;&gt;Anirudh Kamath&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/seanmcguire12&quot;&gt;Sean McGuire&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/miguelg719&quot;&gt;Miguel Gonzalez&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sameelarif&quot;&gt;Sameel Arif&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/filip-michalsky&quot;&gt;Filip Michalsky&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://x.com/jeremypress&quot;&gt;Jeremy Press&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/navidpour&quot;&gt;Navid Pour&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under the MIT License.&lt;/p&gt; 
&lt;p&gt;Copyright 2025 Browserbase, Inc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vllm-project/vllm</title>
      <link>https://github.com/vllm-project/vllm</link>
      <description>&lt;p&gt;A high-throughput and memory-efficient inference and serving engine for LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png&quot; /&gt; 
  &lt;img alt=&quot;vLLM&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png&quot; width=&quot;55%&quot; /&gt; 
 &lt;/picture&gt; &lt;/p&gt; 
&lt;h3 align=&quot;center&quot;&gt; Easy, fast, and cheap LLM serving for everyone &lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt; | &lt;a href=&quot;https://docs.vllm.ai&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://blog.vllm.ai/&quot;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://x.com/vllm_project&quot;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discuss.vllm.ai&quot;&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt; üî•&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[2025/08] We hosted &lt;a href=&quot;https://mp.weixin.qq.com/s/dgkWg1WFpWGO2jCdTqQHxA&quot;&gt;vLLM Beijing Meetup&lt;/a&gt; focusing on large-scale LLM deployment! Please find the meetup slides &lt;a href=&quot;https://drive.google.com/drive/folders/1Pid6NSFLU43DZRi0EaTcPgXsAzDvbBqF&quot;&gt;here&lt;/a&gt; and the recording &lt;a href=&quot;https://www.chaspark.com/#/live/1166916873711665152&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/05] We hosted &lt;a href=&quot;https://lu.ma/c1rqyf1f&quot;&gt;NYC vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement &lt;a href=&quot;https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;[2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post &lt;a href=&quot;https://blog.vllm.ai/2025/01/27/v1-alpha-release.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;details&gt; 
 &lt;summary&gt;Previous News&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;[2025/04] We hosted &lt;a href=&quot;https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day&quot;&gt;Asia Developer Day&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href=&quot;https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href=&quot;https://lu.ma/vllm-ollama&quot;&gt;vLLM x Ollama Inference Night&lt;/a&gt;! Please find the meetup slides from the vLLM team &lt;a href=&quot;https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href=&quot;https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg&quot;&gt;the first vLLM China Meetup&lt;/a&gt;! Please find the meetup slides from vLLM team &lt;a href=&quot;https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/03] We hosted &lt;a href=&quot;https://lu.ma/7mu4k4xx&quot;&gt;the East Coast vLLM Meetup&lt;/a&gt;! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2025/02] We hosted &lt;a href=&quot;https://lu.ma/h7g3kuj9&quot;&gt;the ninth vLLM meetup&lt;/a&gt; with Meta! Please find the meetup slides from vLLM team &lt;a href=&quot;https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing&quot;&gt;here&lt;/a&gt; and AMD &lt;a href=&quot;https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing&quot;&gt;here&lt;/a&gt;. The slides from Meta will not be posted.&lt;/li&gt; 
  &lt;li&gt;[2025/01] We hosted &lt;a href=&quot;https://lu.ma/zep56hui&quot;&gt;the eighth vLLM meetup&lt;/a&gt; with Google Cloud! Please find the meetup slides from vLLM team &lt;a href=&quot;https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing&quot;&gt;here&lt;/a&gt;, and Google Cloud team &lt;a href=&quot;https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/12] vLLM joins &lt;a href=&quot;https://pytorch.org/blog/vllm-joins-pytorch&quot;&gt;pytorch ecosystem&lt;/a&gt;! Easy, Fast, and Cheap LLM Serving for Everyone!&lt;/li&gt; 
  &lt;li&gt;[2024/11] We hosted &lt;a href=&quot;https://lu.ma/h0qvrajz&quot;&gt;the seventh vLLM meetup&lt;/a&gt; with Snowflake! Please find the meetup slides from vLLM team &lt;a href=&quot;https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing&quot;&gt;here&lt;/a&gt;, and Snowflake team &lt;a href=&quot;https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/10] We have just created a developer slack (&lt;a href=&quot;https://slack.vllm.ai&quot;&gt;slack.vllm.ai&lt;/a&gt;) focusing on coordinating contributions and discussing features. Please feel free to join us there!&lt;/li&gt; 
  &lt;li&gt;[2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team &lt;a href=&quot;https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing&quot;&gt;here&lt;/a&gt;. Learn more from the &lt;a href=&quot;https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR&quot;&gt;talks&lt;/a&gt; from other vLLM contributors and users!&lt;/li&gt; 
  &lt;li&gt;[2024/09] We hosted &lt;a href=&quot;https://lu.ma/87q3nvnh&quot;&gt;the sixth vLLM meetup&lt;/a&gt; with NVIDIA! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] We hosted &lt;a href=&quot;https://lu.ma/lp0gyjqr&quot;&gt;the fifth vLLM meetup&lt;/a&gt; with AWS! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post &lt;a href=&quot;https://blog.vllm.ai/2024/07/23/llama31.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/06] We hosted &lt;a href=&quot;https://lu.ma/agivllm&quot;&gt;the fourth vLLM meetup&lt;/a&gt; with Cloudflare and BentoML! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/04] We hosted &lt;a href=&quot;https://robloxandvllmmeetup2024.splashthat.com/&quot;&gt;the third vLLM meetup&lt;/a&gt; with Roblox! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2024/01] We hosted &lt;a href=&quot;https://lu.ma/ygxbpzhl&quot;&gt;the second vLLM meetup&lt;/a&gt; with IBM! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/10] We hosted &lt;a href=&quot;https://lu.ma/first-vllm-meetup&quot;&gt;the first vLLM meetup&lt;/a&gt; with a16z! Please find the meetup slides &lt;a href=&quot;https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
  &lt;li&gt;[2023/08] We would like to express our sincere gratitude to &lt;a href=&quot;https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/&quot;&gt;Andreessen Horowitz&lt;/a&gt; (a16z) for providing a generous grant to support the open-source development and research of vLLM.&lt;/li&gt; 
  &lt;li&gt;[2023/06] We officially released vLLM! FastChat-vLLM integration has powered &lt;a href=&quot;https://chat.lmsys.org&quot;&gt;LMSYS Vicuna and Chatbot Arena&lt;/a&gt; since mid-April. Check out our &lt;a href=&quot;https://vllm.ai&quot;&gt;blog post&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;hr /&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;vLLM is a fast and easy-to-use library for LLM inference and serving.&lt;/p&gt; 
&lt;p&gt;Originally developed in the &lt;a href=&quot;https://sky.cs.berkeley.edu&quot;&gt;Sky Computing Lab&lt;/a&gt; at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.&lt;/p&gt; 
&lt;p&gt;vLLM is fast with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;State-of-the-art serving throughput&lt;/li&gt; 
 &lt;li&gt;Efficient management of attention key and value memory with &lt;a href=&quot;https://blog.vllm.ai/2023/06/20/vllm.html&quot;&gt;&lt;strong&gt;PagedAttention&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Continuous batching of incoming requests&lt;/li&gt; 
 &lt;li&gt;Fast model execution with CUDA/HIP graph&lt;/li&gt; 
 &lt;li&gt;Quantizations: &lt;a href=&quot;https://arxiv.org/abs/2210.17323&quot;&gt;GPTQ&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2306.00978&quot;&gt;AWQ&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2309.05516&quot;&gt;AutoRound&lt;/a&gt;, INT4, INT8, and FP8&lt;/li&gt; 
 &lt;li&gt;Optimized CUDA kernels, including integration with FlashAttention and FlashInfer&lt;/li&gt; 
 &lt;li&gt;Speculative decoding&lt;/li&gt; 
 &lt;li&gt;Chunked prefill&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM is flexible and easy to use with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Seamless integration with popular Hugging Face models&lt;/li&gt; 
 &lt;li&gt;High-throughput serving with various decoding algorithms, including &lt;em&gt;parallel sampling&lt;/em&gt;, &lt;em&gt;beam search&lt;/em&gt;, and more&lt;/li&gt; 
 &lt;li&gt;Tensor, pipeline, data and expert parallelism support for distributed inference&lt;/li&gt; 
 &lt;li&gt;Streaming outputs&lt;/li&gt; 
 &lt;li&gt;OpenAI-compatible API server&lt;/li&gt; 
 &lt;li&gt;Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron&lt;/li&gt; 
 &lt;li&gt;Prefix caching support&lt;/li&gt; 
 &lt;li&gt;Multi-LoRA support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;vLLM seamlessly supports most popular open-source models on HuggingFace, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transformer-like LLMs (e.g., Llama)&lt;/li&gt; 
 &lt;li&gt;Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)&lt;/li&gt; 
 &lt;li&gt;Embedding Models (e.g., E5-Mistral)&lt;/li&gt; 
 &lt;li&gt;Multi-modal LLMs (e.g., LLaVA)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Find the full list of supported models &lt;a href=&quot;https://docs.vllm.ai/en/latest/models/supported_models.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;Install vLLM with &lt;code&gt;pip&lt;/code&gt; or &lt;a href=&quot;https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source&quot;&gt;from source&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install vllm
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Visit our &lt;a href=&quot;https://docs.vllm.ai/en/latest/&quot;&gt;documentation&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.vllm.ai/en/latest/getting_started/installation.html&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.vllm.ai/en/latest/getting_started/quickstart.html&quot;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.vllm.ai/en/latest/models/supported_models.html&quot;&gt;List of Supported Models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome and value any contributions and collaborations. Please check out &lt;a href=&quot;https://docs.vllm.ai/en/latest/contributing/index.html&quot;&gt;Contributing to vLLM&lt;/a&gt; for how to get involved.&lt;/p&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!&lt;/p&gt; 
&lt;!-- Note: Please sort them in alphabetical order. --&gt; 
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt; 
&lt;p&gt;Cash Donations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a16z&lt;/li&gt; 
 &lt;li&gt;Dropbox&lt;/li&gt; 
 &lt;li&gt;Sequoia Capital&lt;/li&gt; 
 &lt;li&gt;Skywork AI&lt;/li&gt; 
 &lt;li&gt;ZhenFund&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Compute Resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Alibaba Cloud&lt;/li&gt; 
 &lt;li&gt;AMD&lt;/li&gt; 
 &lt;li&gt;Anyscale&lt;/li&gt; 
 &lt;li&gt;AWS&lt;/li&gt; 
 &lt;li&gt;Crusoe Cloud&lt;/li&gt; 
 &lt;li&gt;Databricks&lt;/li&gt; 
 &lt;li&gt;DeepInfra&lt;/li&gt; 
 &lt;li&gt;Google Cloud&lt;/li&gt; 
 &lt;li&gt;Intel&lt;/li&gt; 
 &lt;li&gt;Lambda Lab&lt;/li&gt; 
 &lt;li&gt;Nebius&lt;/li&gt; 
 &lt;li&gt;Novita AI&lt;/li&gt; 
 &lt;li&gt;NVIDIA&lt;/li&gt; 
 &lt;li&gt;Replicate&lt;/li&gt; 
 &lt;li&gt;Roblox&lt;/li&gt; 
 &lt;li&gt;RunPod&lt;/li&gt; 
 &lt;li&gt;Trainy&lt;/li&gt; 
 &lt;li&gt;UC Berkeley&lt;/li&gt; 
 &lt;li&gt;UC San Diego&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Slack Sponsor: Anyscale&lt;/p&gt; 
&lt;p&gt;We also have an official fundraising venue through &lt;a href=&quot;https://opencollective.com/vllm&quot;&gt;OpenCollective&lt;/a&gt;. We plan to use the fund to support the development, maintenance, and adoption of vLLM.&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you use vLLM for your research, please cite our &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;paper&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Contact Us&lt;/h2&gt; 
&lt;!-- --8&lt;-- [start:contact-us] --&gt; 
&lt;ul&gt; 
 &lt;li&gt;For technical questions and feature requests, please use GitHub &lt;a href=&quot;https://github.com/vllm-project/vllm/issues&quot;&gt;Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For discussing with fellow users, please use the &lt;a href=&quot;https://discuss.vllm.ai&quot;&gt;vLLM Forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For coordinating contributions and development, please use &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;Slack&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;For security disclosures, please use GitHub&#39;s &lt;a href=&quot;https://github.com/vllm-project/vllm/security/advisories&quot;&gt;Security Advisories&lt;/a&gt; feature&lt;/li&gt; 
 &lt;li&gt;For collaborations and partnerships, please contact us at &lt;a href=&quot;mailto:vllm-questions@lists.berkeley.edu&quot;&gt;vllm-questions@lists.berkeley.edu&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- --8&lt;-- [end:contact-us] --&gt; 
&lt;h2&gt;Media Kit&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you wish to use vLLM&#39;s logo, please refer to &lt;a href=&quot;https://github.com/vllm-project/media-kit&quot;&gt;our media kit repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
